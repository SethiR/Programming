{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This site contains programming files and documentation.","title":"Home"},{"location":"Data/Hadoop/","text":"Hadoop Installing Hadoop Installing Java Installing Java1.8 sudo apt install openjdk-8-jdk Check java version java -version Add JAVA_HOME to /etc/environment vim /etc/environment JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\" or echo \"JAVA_HOME= $( which java ) \" | sudo tee -a /etc/environment Source the envirounment file again source /etc/environment make sure java home details are returned. echo $JAVA_HOME If you have multiple java versions installed then you can refer to this link to set default. Installing Hadoop You can check out the video which I referred while installing hadoop on ubuntu here Create a user hadoop adduser hadoop download hadoop You can use wget and download the tarball hadoop file on /usr/local or /opt. extract the tarball file tar -xvf <filename> create symlink For easier access create a symlink of hadoop to actual hadoop extracted folder. (This is nice to have as its easier to refer to /hadoop vs /hadoop-2.9.2 etc..) sudo ln -s ./hadoop-2.9.2 hadoo bashrc add env variables Configure the java home and hadoop env variables in your bashrc or /etc/envirounment file for system wide. For all (this and below mentioned config files) check out the sub dir in this folder. Create hdfs dirs data directory Namenode directory Datanode directory when I create these directories these are blank and do not contain anything (just blank of level folders, when hadoop uses them then it adds information to this directory.) <<<<<< TODO - https://www.youtube.com/watch?v=l2n124ioO1I >>>>> Technology Stack Ambari : making hadoop management simpler HIVE - Execute SQL queries Spark Pig Hbase Yarn Zookeeper Flume Scoop Concept Mapreduce is a good fit for problems that need to analyze the whole dataset in batch fashion. An RDBMS is good for point queries or udpates. Mapreduce shines where data is written once but read many times. It applies schema on read. Hadoop tries to locate the data closer to the compute node for fast access. Mapreduce focuses on shared nothing architecture thus the nodes have no dependency on each other. MapReduce Hadoop can run MapReduce programs written in various languages. MapReduce are inherently parallel. Basic File concept HDFS (Hadoop distributed file system) is designed for storing very large files (petabytes) with streaming data access patterns(write once, read multiple times, not 100% sure but streaming may refer to the IO stream which you can setup on that petabyte text file), running on clusters of commodity hardware. Nodes - HDFS cluster has namenode and datanode concept i.e. Master and Workers. Name Node - NameNode is the centerpiece of HDFS. NameNode is also known as the Master. NameNode only stores the metadata of HDFS \u2013 the directory tree of all files in the file system, and tracks the files across the cluster. NameNode does not store the actual data or the dataset. The data itself is actually stored in the DataNodes. NameNode knows the list of the blocks and its location for any given file in HDFS. With this information NameNode knows how to construct the file from blocks. NameNode is usually configured with a lot of memory (RAM). Because the namenode holds filesystem metadata in memory, the limit of the number of files is goverened by the amount of memory on the namenode. (As a thumb rule each directory, block, file takes around 150 bytes) Writing data to file - Writers can write data to a file (1 at a time) and append only. Blocks - Usually 128MB in size. The large file is broken into blocks and distributed across data nodes. If file is less than 128MB it takes less than 128MB space. HDFS Directory Structure There is no 1 size fix all for directory structure and even after looking at various posts I did not find consistancy. Below is excerpt from this link . Another link to go though. Hadoop\u2019s Schema-on-Read approach does not put any restrictions regarding how data is ingested into HDFS, BUT having some structure/order/standards around stored data gave us many benefits. Standard structure made it easier to share data between teams working on same data sets. Standard HDFS structure promoted reusability of data processing code. Standard HDFS data organization allowed us to enforce multi-user access and quota controls (support for multi-tenancy). We set up organization wide standards to stage data from external sources (B2B applications) before moving it to data lake. This prevented partially loaded data from being used for processing pipelines. We also used staging area to act as a silo to vet external data (for correctness). Implemented cluster security using Kerberos and encryption (to prevent access to sensitive data). HDFS \u2013 Directory structure and organizing file(s): Defining standard practices around how data is organized on HDFS and enforcing them made it easier to find, use and share data. Based on our experience and mistakes we made, I suggest the following HDFS directory structure (at a high level): /user/{individual_user_sub_directory}: place to hold user specific files (xml, configuration, jar, sample data files) that are not part of business process and primarily used for ad-hoc experiments. /applications: location containing 3rd party jars, custom uber-jars, oozie workflow definitions, pig scripts etc that are required to run applications on Hadoop. /dsc: top level folder containing data in various stages of extract, transform and load (ETL) workflow. There will be sub-folders under root folder for various departments/groups OR applications that own the ETL process. Within each department or application specific sub-folder, have a directory for each ETL process or workflow. Within each workflow/process folder, have a sub-folders for following stages: input, output, errors, intermediate-data. /data-sets: folders containing data-sets that are shared across the organization. This included raw data-sets as well as data created via transformation, aggregation in various steps of dsc. There should be strict controls around which user(s) can read and write its data with only ETL process(es) having write access. Since this folder acts as root folder for shared data-sets, there will be sub-folders for each data set. /tmp: short term storage for temporary data generated by tools or users. In addition to the above directory structure, I suggest that one should make good use of techniques like Partitioning (reduces I/O required during data processing) and Bucketing (breaking large sets into smaller, more managable sub-sets) for organizing data. File System Operations Usually Copying a file from local filesystem to HDFS. Source is left and right is destination. (By default it will be copied over to the users dir on HDFS) [ maria_dev@sandbox-hdp ~ ] $ hadoop fs -copyFromLocal a.txt a.txt Similarly you can copy from hdfs to your local dir hadoop fs -copyToLocal a.txt a.txt The ls command shows similar information to unix system, below mentioned cols provide details. Col1 - File mode (read write access etc...) Col2 - Replication factorn (configured in hdfs-site.xml file) of file. (Replication factor dictates how many copies of a block should be kept in your cluster. By default its 3, however here its 1. Empty (-) for dir because this does not apply to them. ) Col3 - file owner (maria_dev) Col4 - group (hdfs) Col5 - size in bytes Col6 - last modified date Col7 - last modified time Col8 - name -rw-r--r-- 1 maria_dev hdfs 4 2020 -05-25 20 :18 a.txt drwxr-xr-x - maria_dev hdfs 0 2020 -05-26 01 :00 mydir Curated Links - hadoop vs hdfs commands Was looking at running my first hadoop program using java. https://examples.javacodegeeks.com/enterprise-java/apache-hadoop/hadoop-hello-world-example/ Parquet and Avro Hadoop file formats. Avro is a row based storage format. Parquet is a col based storage format. They are self describing formats. They use compression Parquet is geared towards analytical query. e.g. Write once and read many times. Magnitude slower than Avro on the write aspects. Avro is good for write operations (faster than Parquet), querying are slower. Optimized for write operations. Avro has been around longer than Parquet. Avro and parquet support idea of schema evolution. (Lets say you have a order system, today you capture 100 fields for each order. You write that order to hadoop but tomorrow you add 10 more fields which means now for all new orders you will have 100 fields which means schema has evolved). Parquet does not fully support all types of schema evoloution, only supports append (This video was 2017 so may have changed). i.e. you can add cols, but cannot change cols. Where as Avro supports much feature rich schema evolution. General guideline is if you have a workload for analytics you should consider parquet. Where as for ETL workloads Avro is much preferred. (As an example if you have a analyst who does SELECT A, B, C from XYZ then these types of queries are better suited for Parquet, however if you have queries which are scanning the entire table lets say reading each row and then transforming it etc... then in those cases Avro is preferred.) Its common in data lake where you do not necessary know the down stream applications which are going to consume your data from your hadoop cluster in those scenarios you can duplicate the data in both formats. Its also common the primary data format is Avro because of schema evolution. You can also create Parquet on demand based on the query engine you are using (e.g. Impala which works with Parquet) Parquet Parquet uses a hybrid partitioning model. Where the data is stored as cols (every n row where n could be lets say 3). Its sort of a hybrid storage model. (row and col combined.) A parquet file is not actually a single file but is a dir (may be with sub dirs) ./examplefile/ /examplefile/part-000-xxx-yyy-zzz.snappy.parquet /examplefile/part-111-xxx-yyy-zzz.snappy.parquet Data organization Row groups - horizontal partitioning (default 128MB) - A single file can have multiple row groups. Column chunks - Columnar storage Actual data in pages (default 1MB) Data page will have metadata as well such as min, max value total # of values etc... Metadata is also stored at row group level which is stored in the footer. Encdoing schemes There are 2 primary encoding schemes (in actual there are more than 2) - Plain encoding - RLE_Dictionary encoding There is only 1 dictionary per col chunk. If your dict gets too big it will default to plain encoding. There is a configurable parameter parquet.dictionary.page.size which you can customize. You can also decrease row group size parquet.block.size You can use parquet.tools to inspect a parquet file. As we stated that each row group has the min/max stats in the footer. If you enable the spark.sql.parkquet.filterPushdown then when do you queries like select * from x where y > 100 it will read the footer and check if that row group (which is 128MB) is worth reading or not. Which can amount to saving significant read time/data. Parquet can also check values in the dicts to see if the row group needs to be read. Use the config parquet.filter.dictionary.enabled for controlling the behaviour. You can also save a parquet file by a particular column e.g. by date, so this column will become a part of your file directory structure speeding up access. df.write.partitionby(\"date\").parquet(...) . If you know the query beforehand then this will help as it will only read the relevant files by the specified date. To have good optimization avoid having too many small files as every file brings overhead. You can all df.repartition(...).write.parquet(...) . The opposite is also true as to avoid few huge files.","title":"Hadoop"},{"location":"Data/Hadoop/#hadoop","text":"","title":"Hadoop"},{"location":"Data/Hadoop/#installing-hadoop","text":"Installing Java Installing Java1.8 sudo apt install openjdk-8-jdk Check java version java -version Add JAVA_HOME to /etc/environment vim /etc/environment JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\" or echo \"JAVA_HOME= $( which java ) \" | sudo tee -a /etc/environment Source the envirounment file again source /etc/environment make sure java home details are returned. echo $JAVA_HOME If you have multiple java versions installed then you can refer to this link to set default. Installing Hadoop You can check out the video which I referred while installing hadoop on ubuntu here Create a user hadoop adduser hadoop download hadoop You can use wget and download the tarball hadoop file on /usr/local or /opt. extract the tarball file tar -xvf <filename> create symlink For easier access create a symlink of hadoop to actual hadoop extracted folder. (This is nice to have as its easier to refer to /hadoop vs /hadoop-2.9.2 etc..) sudo ln -s ./hadoop-2.9.2 hadoo bashrc add env variables Configure the java home and hadoop env variables in your bashrc or /etc/envirounment file for system wide. For all (this and below mentioned config files) check out the sub dir in this folder. Create hdfs dirs data directory Namenode directory Datanode directory when I create these directories these are blank and do not contain anything (just blank of level folders, when hadoop uses them then it adds information to this directory.) <<<<<< TODO - https://www.youtube.com/watch?v=l2n124ioO1I >>>>>","title":"Installing Hadoop"},{"location":"Data/Hadoop/#technology-stack","text":"Ambari : making hadoop management simpler HIVE - Execute SQL queries Spark Pig Hbase Yarn Zookeeper Flume Scoop","title":"Technology Stack"},{"location":"Data/Hadoop/#concept","text":"Mapreduce is a good fit for problems that need to analyze the whole dataset in batch fashion. An RDBMS is good for point queries or udpates. Mapreduce shines where data is written once but read many times. It applies schema on read. Hadoop tries to locate the data closer to the compute node for fast access. Mapreduce focuses on shared nothing architecture thus the nodes have no dependency on each other.","title":"Concept"},{"location":"Data/Hadoop/#mapreduce","text":"Hadoop can run MapReduce programs written in various languages. MapReduce are inherently parallel.","title":"MapReduce"},{"location":"Data/Hadoop/#basic-file-concept","text":"HDFS (Hadoop distributed file system) is designed for storing very large files (petabytes) with streaming data access patterns(write once, read multiple times, not 100% sure but streaming may refer to the IO stream which you can setup on that petabyte text file), running on clusters of commodity hardware. Nodes - HDFS cluster has namenode and datanode concept i.e. Master and Workers. Name Node - NameNode is the centerpiece of HDFS. NameNode is also known as the Master. NameNode only stores the metadata of HDFS \u2013 the directory tree of all files in the file system, and tracks the files across the cluster. NameNode does not store the actual data or the dataset. The data itself is actually stored in the DataNodes. NameNode knows the list of the blocks and its location for any given file in HDFS. With this information NameNode knows how to construct the file from blocks. NameNode is usually configured with a lot of memory (RAM). Because the namenode holds filesystem metadata in memory, the limit of the number of files is goverened by the amount of memory on the namenode. (As a thumb rule each directory, block, file takes around 150 bytes) Writing data to file - Writers can write data to a file (1 at a time) and append only. Blocks - Usually 128MB in size. The large file is broken into blocks and distributed across data nodes. If file is less than 128MB it takes less than 128MB space. HDFS Directory Structure There is no 1 size fix all for directory structure and even after looking at various posts I did not find consistancy. Below is excerpt from this link . Another link to go though. Hadoop\u2019s Schema-on-Read approach does not put any restrictions regarding how data is ingested into HDFS, BUT having some structure/order/standards around stored data gave us many benefits. Standard structure made it easier to share data between teams working on same data sets. Standard HDFS structure promoted reusability of data processing code. Standard HDFS data organization allowed us to enforce multi-user access and quota controls (support for multi-tenancy). We set up organization wide standards to stage data from external sources (B2B applications) before moving it to data lake. This prevented partially loaded data from being used for processing pipelines. We also used staging area to act as a silo to vet external data (for correctness). Implemented cluster security using Kerberos and encryption (to prevent access to sensitive data). HDFS \u2013 Directory structure and organizing file(s): Defining standard practices around how data is organized on HDFS and enforcing them made it easier to find, use and share data. Based on our experience and mistakes we made, I suggest the following HDFS directory structure (at a high level): /user/{individual_user_sub_directory}: place to hold user specific files (xml, configuration, jar, sample data files) that are not part of business process and primarily used for ad-hoc experiments. /applications: location containing 3rd party jars, custom uber-jars, oozie workflow definitions, pig scripts etc that are required to run applications on Hadoop. /dsc: top level folder containing data in various stages of extract, transform and load (ETL) workflow. There will be sub-folders under root folder for various departments/groups OR applications that own the ETL process. Within each department or application specific sub-folder, have a directory for each ETL process or workflow. Within each workflow/process folder, have a sub-folders for following stages: input, output, errors, intermediate-data. /data-sets: folders containing data-sets that are shared across the organization. This included raw data-sets as well as data created via transformation, aggregation in various steps of dsc. There should be strict controls around which user(s) can read and write its data with only ETL process(es) having write access. Since this folder acts as root folder for shared data-sets, there will be sub-folders for each data set. /tmp: short term storage for temporary data generated by tools or users. In addition to the above directory structure, I suggest that one should make good use of techniques like Partitioning (reduces I/O required during data processing) and Bucketing (breaking large sets into smaller, more managable sub-sets) for organizing data. File System Operations Usually Copying a file from local filesystem to HDFS. Source is left and right is destination. (By default it will be copied over to the users dir on HDFS) [ maria_dev@sandbox-hdp ~ ] $ hadoop fs -copyFromLocal a.txt a.txt Similarly you can copy from hdfs to your local dir hadoop fs -copyToLocal a.txt a.txt The ls command shows similar information to unix system, below mentioned cols provide details. Col1 - File mode (read write access etc...) Col2 - Replication factorn (configured in hdfs-site.xml file) of file. (Replication factor dictates how many copies of a block should be kept in your cluster. By default its 3, however here its 1. Empty (-) for dir because this does not apply to them. ) Col3 - file owner (maria_dev) Col4 - group (hdfs) Col5 - size in bytes Col6 - last modified date Col7 - last modified time Col8 - name -rw-r--r-- 1 maria_dev hdfs 4 2020 -05-25 20 :18 a.txt drwxr-xr-x - maria_dev hdfs 0 2020 -05-26 01 :00 mydir Curated Links - hadoop vs hdfs commands Was looking at running my first hadoop program using java. https://examples.javacodegeeks.com/enterprise-java/apache-hadoop/hadoop-hello-world-example/","title":"Basic File concept"},{"location":"Data/Hadoop/#parquet-and-avro","text":"Hadoop file formats. Avro is a row based storage format. Parquet is a col based storage format. They are self describing formats. They use compression Parquet is geared towards analytical query. e.g. Write once and read many times. Magnitude slower than Avro on the write aspects. Avro is good for write operations (faster than Parquet), querying are slower. Optimized for write operations. Avro has been around longer than Parquet. Avro and parquet support idea of schema evolution. (Lets say you have a order system, today you capture 100 fields for each order. You write that order to hadoop but tomorrow you add 10 more fields which means now for all new orders you will have 100 fields which means schema has evolved). Parquet does not fully support all types of schema evoloution, only supports append (This video was 2017 so may have changed). i.e. you can add cols, but cannot change cols. Where as Avro supports much feature rich schema evolution. General guideline is if you have a workload for analytics you should consider parquet. Where as for ETL workloads Avro is much preferred. (As an example if you have a analyst who does SELECT A, B, C from XYZ then these types of queries are better suited for Parquet, however if you have queries which are scanning the entire table lets say reading each row and then transforming it etc... then in those cases Avro is preferred.) Its common in data lake where you do not necessary know the down stream applications which are going to consume your data from your hadoop cluster in those scenarios you can duplicate the data in both formats. Its also common the primary data format is Avro because of schema evolution. You can also create Parquet on demand based on the query engine you are using (e.g. Impala which works with Parquet)","title":"Parquet and Avro"},{"location":"Data/Hadoop/#parquet","text":"Parquet uses a hybrid partitioning model. Where the data is stored as cols (every n row where n could be lets say 3). Its sort of a hybrid storage model. (row and col combined.) A parquet file is not actually a single file but is a dir (may be with sub dirs) ./examplefile/ /examplefile/part-000-xxx-yyy-zzz.snappy.parquet /examplefile/part-111-xxx-yyy-zzz.snappy.parquet Data organization Row groups - horizontal partitioning (default 128MB) - A single file can have multiple row groups. Column chunks - Columnar storage Actual data in pages (default 1MB) Data page will have metadata as well such as min, max value total # of values etc... Metadata is also stored at row group level which is stored in the footer. Encdoing schemes There are 2 primary encoding schemes (in actual there are more than 2) - Plain encoding - RLE_Dictionary encoding There is only 1 dictionary per col chunk. If your dict gets too big it will default to plain encoding. There is a configurable parameter parquet.dictionary.page.size which you can customize. You can also decrease row group size parquet.block.size You can use parquet.tools to inspect a parquet file. As we stated that each row group has the min/max stats in the footer. If you enable the spark.sql.parkquet.filterPushdown then when do you queries like select * from x where y > 100 it will read the footer and check if that row group (which is 128MB) is worth reading or not. Which can amount to saving significant read time/data. Parquet can also check values in the dicts to see if the row group needs to be read. Use the config parquet.filter.dictionary.enabled for controlling the behaviour. You can also save a parquet file by a particular column e.g. by date, so this column will become a part of your file directory structure speeding up access. df.write.partitionby(\"date\").parquet(...) . If you know the query beforehand then this will help as it will only read the relevant files by the specified date. To have good optimization avoid having too many small files as every file brings overhead. You can all df.repartition(...).write.parquet(...) . The opposite is also true as to avoid few huge files.","title":"Parquet"},{"location":"Data/Kafka/","text":"Kafka A simple kafka producer. Run kafka Create topic named \"test\" Run console consumer to confirm msg received. bin/kafka-topics.sh --list --zookeeper localhost:2181 // SampleProducer.java import org.apache.kafka.clients.producer.KafkaProducer ; import org.apache.kafka.clients.producer.Producer ; import org.apache.kafka.clients.producer.ProducerRecord ; import java.util.Properties ; public class SampleProducer { public SampleProducer (){ Properties props = new Properties (); props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); props . put ( \"key.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); props . put ( \"value.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); ProducerRecord producerRecord = new ProducerRecord ( \"test\" , \"name\" , \"sam\" ); KafkaProducer kafkaProducer = new KafkaProducer ( props ); kafkaProducer . send ( producerRecord ); kafkaProducer . close (); } } // Runner.java public class Runner { public static void main ( String [] args ) { SampleProducer sampleProducer = new SampleProducer (); } } <!-- Maven --> <dependencies> <!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients --> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <version> 2.5.0 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-api --> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-api </artifactId> <version> 1.7.30 </version> </dependency> </dependencies> Hortonworks connect to kafka broker from within the same machine CentOS. (Same machine no ssh) from pykafka import KafkaClient client = KafkaClient ( hosts = \"localhost:9092\" , zookeeper_hosts = \"localhost:2181\" ) topic = client . topics [ 'weatherData' ] with topic . get_sync_producer () as producer : for i in range ( 4 ): producer . produce ( b \"test message\" ) You can consume using the console consumer. /usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic weatherData --from-beginning \"\"\" Simple script to upload a json data to Kafka in Python \"\"\" from pykafka import KafkaClient import json client = KafkaClient ( hosts = \"localhost:9092\" , zookeeper_hosts = \"localhost:2181\" ) # print(client.topics) topic = client . topics [ 'weatherData' ] data = { \"name\" : \"Sam\" , \"age\" : 10 , \"hobbies\" : [ \"football\" , \"basketball\" ] } print ( json . dumps ( data )) with topic . get_sync_producer () as producer : producer . produce ( bytes ( json . dumps ( data ), encoding = 'utf-8' )) A simple Java produer -- coded by me for a test check on my laptop. Uses the weather data with header where it forms the csv. /* A simple example where we pick data from file (csv), the file has a header. The file is read and we create a map where key is the header and value is each row in the file, this map can then be converted to JSON easily. */ import com.google.gson.Gson ; import org.apache.kafka.clients.producer.KafkaProducer ; import org.apache.kafka.clients.producer.Producer ; import org.apache.kafka.clients.producer.ProducerRecord ; import java.io.IOException ; import java.nio.file.Files ; import java.nio.file.Paths ; import java.util.HashMap ; import java.util.List ; import java.util.Map ; import java.util.Properties ; class FileProcessor { String fileName ; public FileProcessor ( String fileName ){ this . fileName = fileName ; } static Map < String , String > toMap ( String [] keys , String [] values ){ Map < String , String > map = new HashMap <> (); for ( int i = 0 ; i < keys . length - 1 ; i ++ ){ map . put ( keys [ i ] , values [ i ] ); } return map ; } boolean run () throws InterruptedException { List < String > lines = null ; try { lines = Files . readAllLines ( Paths . get ( fileName )); } catch ( IOException e ) { e . printStackTrace (); } // If no lines in file. if ( lines . size () == 0 ){ return true ; } String [] header = lines . get ( 0 ). split ( \",\" ); lines = lines . subList ( 1 , lines . size ()); for ( String line : lines ){ String [] word = line . split ( \",\" ); Gson gson = new Gson (); Map < String , String > dataMap = toMap ( header , word ); String dataJSON = gson . toJson ( dataMap ); String key = dataMap . get ( \"Site Num\" ) + \"-\" + dataMap . get ( \"\" ); boolean result = postToKafka ( key , dataJSON ); Thread . sleep ( 2000 ); } return true ; } private boolean postToKafka ( String key , String data ){ Properties props = new Properties (); props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); props . put ( \"acks\" , \"all\" ); props . put ( \"key.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); props . put ( \"value.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); Producer < String , String > producer = new KafkaProducer <> ( props ); producer . send ( new ProducerRecord < String , String > ( \"weatherData\" , key , data )); producer . close (); return true ; } } public class Main8 { public static void main ( String [] args ){ String fileName = args [ 0 ] ; FileProcessor fp = new FileProcessor ( fileName ); try { fp . run (); } catch ( InterruptedException e ) { e . printStackTrace (); } } }","title":"Kafka"},{"location":"Data/Kafka/#kafka","text":"A simple kafka producer. Run kafka Create topic named \"test\" Run console consumer to confirm msg received. bin/kafka-topics.sh --list --zookeeper localhost:2181 // SampleProducer.java import org.apache.kafka.clients.producer.KafkaProducer ; import org.apache.kafka.clients.producer.Producer ; import org.apache.kafka.clients.producer.ProducerRecord ; import java.util.Properties ; public class SampleProducer { public SampleProducer (){ Properties props = new Properties (); props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); props . put ( \"key.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); props . put ( \"value.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); ProducerRecord producerRecord = new ProducerRecord ( \"test\" , \"name\" , \"sam\" ); KafkaProducer kafkaProducer = new KafkaProducer ( props ); kafkaProducer . send ( producerRecord ); kafkaProducer . close (); } } // Runner.java public class Runner { public static void main ( String [] args ) { SampleProducer sampleProducer = new SampleProducer (); } } <!-- Maven --> <dependencies> <!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients --> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <version> 2.5.0 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-api --> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-api </artifactId> <version> 1.7.30 </version> </dependency> </dependencies> Hortonworks connect to kafka broker from within the same machine CentOS. (Same machine no ssh) from pykafka import KafkaClient client = KafkaClient ( hosts = \"localhost:9092\" , zookeeper_hosts = \"localhost:2181\" ) topic = client . topics [ 'weatherData' ] with topic . get_sync_producer () as producer : for i in range ( 4 ): producer . produce ( b \"test message\" ) You can consume using the console consumer. /usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic weatherData --from-beginning \"\"\" Simple script to upload a json data to Kafka in Python \"\"\" from pykafka import KafkaClient import json client = KafkaClient ( hosts = \"localhost:9092\" , zookeeper_hosts = \"localhost:2181\" ) # print(client.topics) topic = client . topics [ 'weatherData' ] data = { \"name\" : \"Sam\" , \"age\" : 10 , \"hobbies\" : [ \"football\" , \"basketball\" ] } print ( json . dumps ( data )) with topic . get_sync_producer () as producer : producer . produce ( bytes ( json . dumps ( data ), encoding = 'utf-8' )) A simple Java produer -- coded by me for a test check on my laptop. Uses the weather data with header where it forms the csv. /* A simple example where we pick data from file (csv), the file has a header. The file is read and we create a map where key is the header and value is each row in the file, this map can then be converted to JSON easily. */ import com.google.gson.Gson ; import org.apache.kafka.clients.producer.KafkaProducer ; import org.apache.kafka.clients.producer.Producer ; import org.apache.kafka.clients.producer.ProducerRecord ; import java.io.IOException ; import java.nio.file.Files ; import java.nio.file.Paths ; import java.util.HashMap ; import java.util.List ; import java.util.Map ; import java.util.Properties ; class FileProcessor { String fileName ; public FileProcessor ( String fileName ){ this . fileName = fileName ; } static Map < String , String > toMap ( String [] keys , String [] values ){ Map < String , String > map = new HashMap <> (); for ( int i = 0 ; i < keys . length - 1 ; i ++ ){ map . put ( keys [ i ] , values [ i ] ); } return map ; } boolean run () throws InterruptedException { List < String > lines = null ; try { lines = Files . readAllLines ( Paths . get ( fileName )); } catch ( IOException e ) { e . printStackTrace (); } // If no lines in file. if ( lines . size () == 0 ){ return true ; } String [] header = lines . get ( 0 ). split ( \",\" ); lines = lines . subList ( 1 , lines . size ()); for ( String line : lines ){ String [] word = line . split ( \",\" ); Gson gson = new Gson (); Map < String , String > dataMap = toMap ( header , word ); String dataJSON = gson . toJson ( dataMap ); String key = dataMap . get ( \"Site Num\" ) + \"-\" + dataMap . get ( \"\" ); boolean result = postToKafka ( key , dataJSON ); Thread . sleep ( 2000 ); } return true ; } private boolean postToKafka ( String key , String data ){ Properties props = new Properties (); props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); props . put ( \"acks\" , \"all\" ); props . put ( \"key.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); props . put ( \"value.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); Producer < String , String > producer = new KafkaProducer <> ( props ); producer . send ( new ProducerRecord < String , String > ( \"weatherData\" , key , data )); producer . close (); return true ; } } public class Main8 { public static void main ( String [] args ){ String fileName = args [ 0 ] ; FileProcessor fp = new FileProcessor ( fileName ); try { fp . run (); } catch ( InterruptedException e ) { e . printStackTrace (); } } }","title":"Kafka"},{"location":"Data/SQL/","text":"SQL (Structured Query Lanauage). There are many database which can be used to learn SQL. I am using Postgres. Postgres can be installed on Linux or Windows with ease. In this document I am using Linux (Ubuntu 20.04) to connect to Postgres-12 using terminal. Once Postgres is installed open terminal to connect to postgres and begin. Connect to postgres. sudo -i -u postgres Once you are connected to postgres you can go to psql prompt using psql command. As you see the prompt changes. postgres@X:~$ psql psql ( 12 .2 ( Ubuntu 12 .2-4 )) Type \"help\" for help. postgres = # List all databases. \\l postgres-# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+-------------+-------------+----------------------- postgres | postgres | UTF8 | en_CA.UTF-8 | en_CA.UTF-8 | template0 | postgres | UTF8 | en_CA.UTF-8 | en_CA.UTF-8 | = c/postgres + | | | | | postgres = CTc/postgres template1 | postgres | UTF8 | en_CA.UTF-8 | en_CA.UTF-8 | = c/postgres + | | | | | postgres = CTc/postgres Create a new database. postgres = # CREATE DATABASE test; CREATE DATABASE Drop a database postgres = # DROP DATABASE test; DROP DATABASE Using a particular database such as test postgres = # \\c test You are now connected to database \"test\" as user \"postgres\" . Creating a table test = # CREATE TABLE Customer( test ( # ID INT PRIMARY KEY NOT NULL, test ( # NAME TEXT NOT NULL test ( # ); CREATE TABLE Checking tables in the db est = # \\d List of relations Schema | Name | Type | Owner --------+----------+-------+---------- public | customer | table | postgres Checking the details of a table. test = # \\d customer Table \"public.customer\" Column | Type | Collation | Nullable | Default --------+---------+-----------+----------+--------- id | integer | | not null | name | text | | not null | Indexes: \"customer_pkey\" PRIMARY KEY, btree ( id ) Deleting a database, dropping a database. test = # drop table customer; DROP TABLE test = # \\d Did not find any relations. Creating table and inserting data by hand gets very tedius thus we can download the northwind database from here and load it into postgres. To load do the following - Download the SQL file and keep it in lets say downloads folder. - log into psql - load file using \\i /home/rs/Downlaods/northwind.postgre.sql Now that I know a little bit about Postgres I will return to standard SQL using the northwind database. #+BEGIN~QUOTE~ #+BEGIN~QUOTE~ Change here. Using postgres is getting too difficult because the northwind database has quotes \\\"\\\" in table name which are creating issues with query. Also there are user authorization issues which is creating another issue so switching to SQLite3. #+END~QUOTE~ #+END~QUOTE~ Using SQLite3 to create a .db using .sql file. sqlite3 northwind.db < Northwind.Sqlite3.sql A simple select statement select ContactName , City from Customers ; Select distince gives unique values. select DISTINCT City from Customers ; -- will first get distince cities and then count them, so you will have a total of unique values. select count ( DISTINCT City ) from Customers ; A simple where clause select * from Customers where Country = \"Mexico\" The where clause can use = , > etc... and BETWEEN , LIKE (for a pattern) and IN (To specify multiple possible values for a column) SELECT * FROM Products WHERE Price BETWEEN 50 AND 60 ; SELECT * FROM Customers WHERE City LIKE 's%' ; SELECT * FROM Customers WHERE City LIKE 's%v%' ; SELECT * FROM Customers WHERE City IN ( 'Paris' , 'London' ); SQL supports AND , OR , NOT operations primarly on the where clause. SELECT * FROM Customers WHERE City LIKE 's%' AND Country in ( \"Spain\" , \"Norwary\" ); SELECT * FROM Customers WHERE Country = 'Germany' AND ( City = 'Berlin' OR City = 'M\u00fcnchen' ); The ORDER BY keyword is used to sort the result-set in ascending or descending order. select * from Customers order by city ; If you wish to sort by descending order SELECT * FROM Customers ORDER BY Country DESC ; SELECT * FROM Customers ORDER BY Country ASC , City DESC ; Inserting values into the database. INSERT INTO Customers ( CustomerName , ContactName , Address , City , PostalCode , Country ) VALUES ( 'Cardinal' , 'Tom B. Erichsen' , 'Skagen 21' , 'Stavanger' , '4006' , 'Norway' ); It is not possible to test for NULL values with comparison operators, such as =, \\<, or \\<>. SELECT * FROM Customers where city is null ; You can also use IS NOT NULL for checking non null values. Updating a row in the database. UPDATE Customers SET ContactName = 'Alfred Schmidt' , City = 'Frankfurt' WHERE CustomerID = 1 ; Deleting from database is also pretty easy. DELETE FROM table_name WHERE condition ; DELETE FROM Customers ; -- deletes all records. Limiting the number of rows which get returned. Not all database have the same syntax as below e.g. =LIMIT 10=, some of them have something like SELECT TOP 10 FROM TABLE SELECT column_name ( s ) FROM table_name WHERE condition LIMIT number ; MIN and MAX. You can get min and max values directly from the database. SELECT MIN ( Freight ) from Orders where EmployeeId = 2 ; SQL can do COUNT(), AVG() and SUM() functions. All of them are fairly similar. SELECT AVG ( Freight ) from Orders ; You can tack on where clause if necessary. SQL Wildcards Wildcard characters can be used with LIKE keyword or NOT LIKE keyword. The characters are database specific so check out the particular database manual. SQL aliases are used to give a table, column a temporary name. SELECT City AS cty , CompanyName AS cn FROM Customers ; JOINS SELECT Orders . OrderID , Customers . ContactName , Orders . OrderDate FROM Orders INNER JOIN Customers ON Orders . CustomerID = Customers . CustomerID ; (INNER) JOIN: Returns records that have matching values in both tables LEFT (OUTER) JOIN: Returns all records from the left table, and the matched - records from the right table RIGHT (OUTER) JOIN: Returns all records from the right table, and the matched - records from the left table FULL (OUTER) JOIN: Returns all records when there is a match in either left or right table Select Orders . OrderDate , Orders . ShipCity , Customers . ContactName from Orders Inner join Customers ON orders . CustomerID = Customers . CustomerID We can inner join multiple tables. SELECT Orders . OrderID , Customers . CustomerName , Shippers . ShipperName FROM ( ( Orders INNER JOIN Customers ON Orders . CustomerID = Customers . CustomerID ) INNER JOIN Shippers ON Orders . ShipperID = Shippers . ShipperID ); The UNION operator is used to combine the result-set of two or more SELECT statements. Select * from Customers where CustomerID like \"A%\" union Select * from Customers where CustomerID like \"B%\" The UNION operator selects only distinct values by default. To allow duplicate values, use UNION ALL The GROUP BY statement groups rows that have the same values into summary rows, like \\\"find the number of customers in each country\\\".The GROUP BY statement is often used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns. SELECT COUNT ( CustomerID ), Country FROM Customers GROUP BY Country ; SELECT COUNT ( CustomerID ), Country FROM Customers GROUP BY Country ORDER BY COUNT ( CustomerID ) DESC ; SELECT Shippers . ShipperName , COUNT ( Orders . OrderID ) AS NumberOfOrders FROM Orders LEFT JOIN Shippers ON Orders . ShipperID = Shippers . ShipperID GROUP BY ShipperName ; A stored procedure is a prepared SQL code that you can save, so the code can be reused over and over again. So if you have an SQL query that you write over and over again, save it as a stored procedure, and then just call it to execute it. You can also pass parameters to a stored procedure, so that the stored procedure can act based on the parameter value(s) that is passed. CREATE PROCEDURE SelectAllCustomers AS SELECT * FROM Customers GO ; To execute the procedure. EXEC SelectAllCustomers ; CREATE PROCEDURE SelectAllCustomers @ City nvarchar ( 30 ) AS SELECT * FROM Customers WHERE City = @ City GO ; EXEC SelectAllCustomers @ City = 'London' ; Another example CREATE PROCEDURE SelectAllCustomers @ City nvarchar ( 30 ), @ PostalCode nvarchar ( 10 ) AS SELECT * FROM Customers WHERE City = @ City AND PostalCode = @ PostalCode GO ; EXEC SelectAllCustomers @ City = 'London' , @ PostalCode = 'WA1 1DP' ;","title":"SQL"},{"location":"Data/Spark/","text":"Spark Pyspark Install steps taken Spark Download Trial 1: Pyspark 2.4.5 was only working with python 3.7 and not python 3.8 Ubuntu came by default with python 3.7 so I had to install python 3.7 I built from source in /opt Created a link as /usr/bin/python3.7 pointing to /opt/python3.3.7/python Configured update-alternatives to make python3.7 default. Installed Spark in /opt (nothing more than wget the 2.4.5 spark) Created a system link /opt/spark pointing to /opt/spark2.xxxx As I did not have python on my system and only had python3 I added the following to /spark/conf/spark-env.sh export PYSPARK_PYTHON = python3 Added the following in my .bashrc export SPARK_HOME = /opt/spark export PATH = $SPARK_HOME /bin: $PATH Trial 2: Download spark from Apache spark website, un-tar it in some dir. Create a python venv or download Anaconda distribution, whatever you feel comfortable. Set the PYSPARK_PYTHON variable in conf/spark-env.sh . For example, if Python executable is installed under /opt/anaconda3/bin/python3: PYSPARK_PYTHON='/opt/anaconda3/bin/python3 PIP Pyspark This time I did things a little differently. I created a venv of python3.7 and installed pyspark. The system does not even have apache spark downloaded and insalled. Looks like this is not needed and pyspark (which is of size 225MB) comes with spark built in. So this means we can test spark applications on my laptop with just a pip3 install pyspark and we'll see how to submit them to yarn cluster when the time comes. Spark Overview In Memory Computing Spark shell spark-shell is located in spark/bin To goto spark shell execute the above ./spark-shell In spark-shell you have 2 variables by default sc and spark :help :history RDD RDD is basic unit of data on which all operations are performed. RDD is immutable collection of data (which can be distributed). RDD is split into multiple partitions which are computed on different nodes. RDD can be created on any hadoop input source (that is supported by hadoop). Spark automatically partitions RDDs and distributes the partitions across different nodes. A partition in spark is an atomic chunk of data (logical division of data) stored on a node in the cluster. We can create RDD using - Parallalize method - Takes in a sequence (like list or array) and # of partitions you need the data split on. - CreateDataFrame method parallelize method from pyspark.sql import SparkSession spark = SparkSession.builder.appName ( \"Python Spark create RDD example\" ) .config ( \"local[*]\" ) .getOrCreate () df = spark.sparkContext.parallelize ([( 1 , 2 , 3 , 'a b c' ) , ( 4 , 5 , 6 , 'd e f' ) , ( 7 , 8 , 9 , 'g h i' )]) .toDF ([ 'col1' , 'col2' , 'col3' , 'col4' ]) print ( df ) createDataFrame method from pyspark.sql import SparkSession spark = SparkSession.builder.appName ( \"Python Spark create RDD example\" ) .config ( \"local[*]\" ) .getOrCreate () df = spark.createDataFrame ([ ( 1 , 'Joe' ,2000,1 ) , ( 1 , 'Joe' , 2000 , 1 ) , ( 1 , 'Joe' , 2000 , 1 ) , ( 1 , 'Joe' , 2000 , 1 ) , ] , [ 'ID' , 'Name' , 'Salary' , 'Department' ] ) df.show () df.printSchema () + ---+----+------+----------+ | ID | Name | Salary | Department | + ---+----+------+----------+ | 1 | Joe | 2000 | 1 | | 1 | Joe | 2000 | 1 | | 1 | Joe | 2000 | 1 | | 1 | Joe | 2000 | 1 | + ---+----+------+----------+ root | -- ID: long (nullable = true) | -- Name: string (nullable = true) | -- Salary: long (nullable = true) | -- Department: long (nullable = true) Data can also be read from - csv files - postgres database - HDFS Lets see some examples in Scala as well. # Scala example # Create an array scala > val intArray = Array ( 1 , 2 , 3 , 4 , 5 , 6 ) intArray : Array [ Int ] = Array ( 1 , 2 , 3 , 4 , 5 , 6 ) # Creating an RDD scala > val intRdd = sc . parallelize ( intArray ) # See the first method in your RDD scala > intRdd . first () res1 : Int = 1 # Use the take method to get various elements from RDD # Refer to spark docs for scala / python etc ... for details on methods such as take . scala > intRdd . take ( 2 ) # takes first n elements . res2 : Array [ Int ] = Array ( 1 , 2 ) # to get all elements from an rdd scala > intRdd . collect () res4 : Array [ Int ] = Array ( 1 , 2 , 3 , 4 , 5 , 6 ) # Executing on each rdd scala > intRdd . collect (). foreach ( println ) 1 2 3 4 5 6 # Checking partition size . scala > intRdd . partitions . size res10 : Int = 4 # Python example # Similarly the RDD can be created in python. (Perf implications) >>> a = [ 1 , 2 , 3 , 4 , 5 ] >>> intRdd = sc . parallelize ( a ) >>> type ( intRdd . take ( 2 )) < class ' list '> >>> for x in intRdd . take ( 2 ): ... print ( x ** 2 ) ... 1 4 There are 2 main types of Spark operations (Details in the below sections) - Spark Transformations - Construct a new RDD from a previous one. - Spark Actions - compute a result based on RDD, either return it to driver program or save it to external storage. Lets look at examples. The methods which we will see (which we will call on lists/sequences are standard scala methods such as map , filter , flatmap or distinct . So probably studying scala is not just about the per for RDD but its also how scala functions and what scala functions are utilized in spark (even though they are performed on an RDD and not scala Array or Vector)) scala> val sentences = Array ( \"Today is monday\" , \"The quick brown fox jumped over the lazy dog\" , \"Hi There\" ) scala> val sentRdd = sc.parallelize ( sentences ) # Filter function (a standard scala function as well) scala> val filterRdd = sentRdd.filter ( line = > line.length > 12 ) scala> filterRdd.collect.foreach ( println ) Today is monday The quick brown fox jumped over the lazy dog # Map function (a standard scala map function) scala> val mapRdd = sentRdd.map ( line = > line.length ) scala> mapRdd.collect () res14: Array [ Int ] = Array ( 15 , 44 , 8 ) # ex 2 with map --> here it creates an array of array scala> val mapRdd2 = sentRdd.map ( line = > line.split ( \" \" )) scala> mapRdd2.collect () res15: Array [ Array [ String ]] = Array ( Array ( Today, is, monday ) , Array ( The, quick, brown, fox, jumped, over, the, lazy, dog ) , Array ( Hi, There )) # Flatmap will flatten out the array of arrays and create just array. scala> val mapRdd3 = sentRdd.flatMap ( line = > line.split ( \" \" )) mapRdd3: org.apache.spark.rdd.RDD [ String ] = MapPartitionsRDD [ 7 ] at flatMap at <console>:25 scala> mapRdd3.collect () res16: Array [ String ] = Array ( Today, is, monday, The, quick, brown, fox, jumped, over, the, lazy, dog, Hi, There ) Lets see very similar examples in Python. You will see that its very similar the diff is mainly in using lambda funcs in Python vs fatarrow => functions in Scala. >>> sentences = [ \"Today is monday\" , \"The quick brown fox jumped over the lazy dog\" , \"Hi There\" ] >>> sentRdd = sc . parallelize ( sentences ) >>> mapRdd = sentRdd . map ( lambda line : len ( line ) > 12 ) >>> mapRdd . collect () [ True , True , False ] >>> mapRdd2 = sentRdd . filter ( lambda line : len ( line ) > 12 ) >>> mapRdd2 . collect () [ 'Today is monday' , 'The quick brown fox jumped over the lazy dog' ] >>> mapRdd3 = sentRdd . flatMap ( lambda line : line . split ( \" \" ) ) >>> mapRdd3 . collect () [ 'Today' , 'is' , 'monday' , 'The' , 'quick' , 'brown' , 'fox' , 'jumped' , 'over' , 'the' , 'lazy' , 'dog' , 'Hi' , 'There' ] Spark applications consist of Driver program - This runs your main() function A set of executor processes. The executors will pretty much be running spark code but the driver program can be written in number of different languages exposed via spark api. Scala or Python Lets look at Spark toolkit. Low Level API's RDD's Distributed variables. Structured API's Datasets Dataframes SQL If you are using Structured API's the choice of language does not matter because Spark will reduce the code to low level API's (mor efficiently than you could ever write RDD or distributed variable). If for some reason the structured api does not fit your need then its recommended you switch to Scala. (Note that you can sill write everything in Python but just write a small portion in Scala where it leverages custom RDD, Distribute variables). My strategy is to stick to Python for structured API's and dive in Scala for low level api's. DataFrame The dataframe concept in spark is a little different than Python/R where the data can reside only on 1 machine. However in spark the data underneath that dataframe can reside on multiple machines. Its easy to convert pandas DF to spark. >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"My App\" ) . config ( \"local[*]\" ) . getOrCreate () >>> number = spark . range ( 1000 ) . toDF ( \"number\" ) >>> number . show () +------+ | number | +------+ | 0 | | 1 | | 2 | | 3 | | 4 | | 5 | | 6 | | 7 | | 8 | | 9 | | 10 | | 11 | | 12 | | 13 | | 14 | | 15 | | 16 | | 17 | | 18 | | 19 | +------+ only showing top 20 rows Transformations In spark the core datastructures are immutable, they cannot be changed once created. >>> div_by_2 = number . where ( \"number % 2 = 0\" ) # At this point a new df is not created as it will execute/evaluate this lazyly when someone will reqeust acess to it as we'll do next. >>> div_by_2 . show () +------+ | number | +------+ | 0 | | 2 | | 4 | | 6 | | 8 | | 10 | | 12 | | 14 | | 16 | | 18 | | 20 | | 22 | | 24 | | 26 | | 28 | | 30 | | 32 | | 34 | | 36 | | 38 | +------+ only showing top 20 rows There are 2 types of transformations - narrow : each partition will contribute to only 1 output partition - wide : input partition contributing to many output partitions. (shuffle) Action Transformations build a logical data transformation plan. To trigger computation we run action. e.g. >>> div_by_2 . count () 500 By doing count we - Started a spark job that runs filter transformation (divisible by 2), a narrow transformation. - Aggregation (counting total) which is a wide transformation. Spark UI You can check out spark UI @ http://localhost:4040/ Example A simple example where we read data from csv into a spark df. from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"P\" ) . config ( \"local[*]\" ) . getOrCreate () flight_df = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( \"2015-summary.csv\" ) # As of now the Dataframe has set # of cols and unspecified rows (because it did not read all rows) # -- Think lazy evaulation print ( flight_df . printSchema ()) # Returns first 'n' # of rows. print ( flight_df . take ( 3 )) # The explain command prints out the 'Plan' which spark will take provide output. # Read from bottom to top. Bottom being step 1 and top being last step. # Remember .sort() does not modify the df but returns a new df. print ( flight_df . sort ( \"count\" ) . explain ()) \"\"\" You can also adjust the # of partitions using the below config spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\") \"\"\" Dataframes and SQL Spark can run transformations regardless of the language SQL or DataFrame (Scala, Python, R or Java). In the end all transformations (high level api's) are converted down into a physical plan. This example shows how we can do SQL queries on a df which we convert to a view (temp table). There is no performance diff as they compile down to the same plan. from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"P\" ) . config ( \"local[*]\" ) . getOrCreate () flight_df = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( \"2015-summary.csv\" ) # SQL WAY # convert the df into a temp view table which you can run sql queries on flight_df . createOrReplaceTempView ( \"flight_df_sql\" ) # The df flight_df creates a table (view) flight_df_sql # Now we can do select query on flight_df_sql as though it was a table sqlWay = spark . sql ( \"\"\" SELECT DEST_COUNTRY_NAME, count(1) FROM flight_df_sql GROUP BY DEST_COUNTRY_NAME \"\"\" ) # DF WAY dfWay = flight_df . groupBy ( \"DEST_COUNTRY_NAME\" ) . count () sqlWay . explain () dfWay . explain () == Physical Plan == * ( 2 ) HashAggregate ( keys =[ DEST_COUNTRY_NAME#10 ] , functions =[ count ( 1 )]) +- Exchange hashpartitioning ( DEST_COUNTRY_NAME#10, 200 ) +- * ( 1 ) HashAggregate ( keys =[ DEST_COUNTRY_NAME#10 ] , functions =[ partial_count ( 1 )]) +- * ( 1 ) FileScan csv [ DEST_COUNTRY_NAME#10 ] Batched: false, Format: CSV, Location: InMemoryFileIndex [ file:/home/rs/MEGA/repositories/technotes/docs/Data/spark/2015-summary.csv ] , PartitionFilters: [] , PushedFilters: [] , ReadSchema: struct<DEST_COUNTRY_NAME:string> == Physical Plan == * ( 2 ) HashAggregate ( keys =[ DEST_COUNTRY_NAME#10 ] , functions =[ count ( 1 )]) +- Exchange hashpartitioning ( DEST_COUNTRY_NAME#10, 200 ) +- * ( 1 ) HashAggregate ( keys =[ DEST_COUNTRY_NAME#10 ] , functions =[ partial_count ( 1 )]) +- * ( 1 ) FileScan csv [ DEST_COUNTRY_NAME#10 ] Batched: false, Format: CSV, Location: InMemoryFileIndex [ file:/home/rs/MEGA/repositories/technotes/docs/Data/spark/2015-summary.csv ] , PartitionFilters: [] , PushedFilters: [] , ReadSchema: struct<DEST_COUNTRY_NAME:string> Executing simple queries in DF or SQL >>> print ( spark . sql ( \"\"\" SELECT max(count) from flight_df_sql \"\"\" ) . take ( 1 )) [ Row ( max ( count )= 370002 )] A little more complicated example maxSql = spark . sql ( \"\"\" SELECT DEST_COUNTRY_NAME, sum(count) as destination_total FROM flight_df_sql GROUP BY DEST_COUNTRY_NAME ORDER BY sum(count) DESC LIMIT 5 \"\"\" ) maxSql . show () +-----------------+-----------------+ | DEST_COUNTRY_NAME | destination_total | +-----------------+-----------------+ | United States | 411352 | | Canada | 8399 | | Mexico | 7140 | | United Kingdom | 2025 | | Japan | 1548 | +-----------------+-----------------+ Spark Toolset Overview Running prod applications You can use spark-submit to send your code to a cluster and execute it there. Scala example ./spark-submit --class org.apache.spark.examples.SparkPi --master local ../examples/jars/spark-examples_2.11-2.4.5.jar 10 Python example ./spark-submit --master local ../examples/src/main/python/pi.py 10 By changing the master argument of spark-submit , we can also submit the same application to a cluster running Spark\u2019s standalone cluster manager, Mesos or YARN. Datasets Datasets are used for writing statically typed code in Java & Scala. Its not avaibale in Python or R. The Dataset API gives users the ability to assign a Java/Scala class to the records within a DataFrame and manipulate it as a collection of typed objects, similar to a Java ArrayList or Scala Seq. e.g. for our flights data which has 3 cols. So in a sense each row in our csv or parquet file (in this instance) becomes an object of class Flight. case class Flight ( DEST_COUNTRY_NAME : String , ORIGIN_COUNTRY_NAME : String , count : BigInt ) val flightsDF = spark . read . parquet ( \"/data/flight-data/parquet/2010-summary.parquet/\" ) val flights = flightsDF . as [ Flight ] One final advantage is that when you call collect or take on a Dataset, it will collect objects of the proper type in your Dataset, not DataFrame Rows One great thing about Datasets is that you can use them only when you need or want to. After we\u2019ve performed our manipulations, Spark can automatically turn it back into a DataFrame, and we can manipulate it further by using the hundreds of functions that Spark includes. This makes it easy to drop down to lower level, perform type-safe coding when necessary, and move higher up to SQL for more rapid analysis. Structured Streaming Structured Streaming is a high-level API for stream processing. ML & Advanced Analytics Lower Level API's There are some things that you might use RDDs for, especially when you\u2019re reading or manipulating raw data, but for the most part you should stick to the Structured APIs. RDDs are available in Scala as well as Python. However, they\u2019re not equivalent. (Hint : at RDD level prefer Scala) There are basically no instances in modern Spark, for which you should be using RDDs instead of the structured APIs beyond manipulating some very raw unprocessed and unstructured data. Structured API DataFrames are untyped and DataSets are typed. Datasets API are only available in Scala or Java. Overview of structured api execution Write DF, SQL, Dataset code. Spark converts to a logical plan. Spark converts from logical to physical plan + optimizations. Executes physical plan as RDD manipulations. Spark Types Listed below how to create a Spark Type in scala and py. For list of all sparktypes check out the reference . # Scala import org.apache.spark.sql.types._ val b = ByteType # Python from pyspark.sql.types import * b = ByteType () These sparktypes are used to instanciate or declare a col to be of certain type. Lets create a dataframe using a json file. As we have seen that scala and py can both be used for these high level api's I'll use either. (No prferences). # Creating a dataframe >>> df = spark . read . format ( \"json\" ) . load ( \"D:/Documents/MEGA/repositories/technotes/docs/Data/spark/2015-summary.json\" ) >>> df DataFrame [ DEST_COUNTRY_NAME : string , ORIGIN_COUNTRY_NAME : string , count : bigint ] >>> df . printSchema () root |-- DEST_COUNTRY_NAME : string ( nullable = true ) |-- ORIGIN_COUNTRY_NAME : string ( nullable = true ) |-- count : long ( nullable = true ) >>> df . take ( 1 ) [ Row ( DEST_COUNTRY_NAME = 'United States' , ORIGIN_COUNTRY_NAME = 'Romania' , count = 15 )] Schema defines the col names of a DataFrame. (We can let spark read/infer the schema or define explicitly ourselves.) Note for spark in prod usage, its best to define explicit schema. The schema is of StructType , lets see an example as read the abvoe json file with explict manual schema. Refer to the docs here # in Python from pyspark.sql.types import StructField , StructType , StringType , LongType myManualSchema = StructType ([ StructField ( \"DEST_COUNTRY_NAME\" , StringType (), True ), StructField ( \"ORIGIN_COUNTRY_NAME\" , StringType (), True ), StructField ( \"count\" , LongType (), False , metadata = { \"hello\" : \"world\" }) ]) df = spark . read . format ( \"json\" ) . schema ( myManualSchema ) . load ( \"D:/Documents/MEGA/repositories/technotes/docs/Data/spark/2015-summary.json\" ) Checking cols of a df. (you can loop on them its a list) >>> df . columns [ 'DEST_COUNTRY_NAME' , 'ORIGIN_COUNTRY_NAME' , 'count' ] Getting the first row. >>> df . first () Row ( DEST_COUNTRY_NAME = 'United States' , ORIGIN_COUNTRY_NAME = 'Romania' , count = 15 ) >>> row1 = df . first () >>> row1 Row ( DEST_COUNTRY_NAME = 'United States' , ORIGIN_COUNTRY_NAME = 'Romania' , count = 15 ) >>> row1 [ 0 ] 'United States' >>> row1 [ 1 ] 'Romania' You can also create new rows. >>> myRow = Row ( \"US\" , \"Canada\" , 12 ) You can convert the dataframe into temporary table where you can do sql queries on them. You can also convert Row into DataFrame by using the function createDataFrame . Doing simple queries on df. >>> df . select ( \"DEST_COUNTRY_NAME\" ) . show ( 2 ) +-----------------+ | DEST_COUNTRY_NAME | +-----------------+ | United States | | United States | +-----------------+ only showing top 2 rows >>> df . select ( expr ( \"DEST_COUNTRY_NAME AS destination\" )) . show ( 2 ) +-------------+ | destination | +-------------+ | United States | | United States | +-------------+ only showing top 2 rows # SelectExpr --> Select from a DataFrame using a set of SQL expressions. # In below example it creates a new col using comparison of 2 cols. >>> df . selectExpr ( \"*\" , \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\" ) . show ( 2 ) +-----------------+-------------------+-----+-------------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | withinCountry | +-----------------+-------------------+-----+-------------+ | United States | Romania | 15 | false | | United States | Croatia | 1 | false | +-----------------+-------------------+-----+-------------+ only showing top 2 rows >>> df . selectExpr ( \"*\" ,( \"ORIGIN_COUNTRY_NAME = 'Romania'\" )) . show ( 2 ) +-----------------+-------------------+-----+-------------------------------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | ( ORIGIN_COUNTRY_NAME = Romania ) | true | | United States | Croatia | 1 | false | +-----------------+-------------------+-----+-------------------------------+ only showing top 2 rows # Instead of using the selectExpr you can also use select and use expr function # where you need to create a new col using sql statement as shown below. # expr is avaiable from pyspark.sql.functions so do the following # >>> from pyspark.sql.functions import * >>> df . select ( \"*\" , expr ( \"ORIGIN_COUNTRY_NAME = 'Romania'\" )) . show ( 2 ) +-----------------+-------------------+-----+-------------------------------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | ( ORIGIN_COUNTRY_NAME = Romania ) | +-----------------+-------------------+-----+-------------------------------+ | United States | Romania | 15 | true | | United States | Croatia | 1 | false | +-----------------+-------------------+-----+-------------------------------+ only showing top 2 rows # Creating a new col (count *2) >>> df . select ( \"*\" ,( df [ \"count\" ] * 2 )) . show ( 2 ) +-----------------+-------------------+-----+-----------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | ( count * 2 ) | +-----------------+-------------------+-----+-----------+ | United States | Romania | 15 | 30 | | United States | Croatia | 1 | 2 | +-----------------+-------------------+-----+-----------+ only showing top 2 rows # Rename that col. >>> df . select ( \"*\" ,( df [ \"count\" ] * 2 ) . alias ( \"2day\" )) . show ( 2 ) +-----------------+-------------------+-----+----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | 2 day | +-----------------+-------------------+-----+----+ | United States | Romania | 15 | 30 | | United States | Croatia | 1 | 2 | +-----------------+-------------------+-----+----+ only showing top 2 rows Adding a new cols as literals # Adding a new col with value 1 will return a new df. >>> df . select ( \"*\" , lit ( 1 )) . show ( 2 ) +-----------------+-------------------+-----+---+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | 1 | +-----------------+-------------------+-----+---+ | United States | Romania | 15 | 1 | | United States | Croatia | 1 | 1 | +-----------------+-------------------+-----+---+ only showing top 2 rows # you can do the same using withColumn <-- This is a more formal way >>> df . withColumn ( \"One\" , lit ( 1 )) . show ( 2 ) +-----------------+-------------------+-----+---+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | One | +-----------------+-------------------+-----+---+ | United States | Romania | 15 | 1 | | United States | Croatia | 1 | 1 | +-----------------+-------------------+-----+---+ only showing top 2 rows # As you see withColumn takes 2 args, 1) col name 2) expr >>> df . withColumn ( \"withinCountry\" , expr ( \"DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME\" )) . show ( 2 ) +-----------------+-------------------+-----+-------------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | withinCountry | +-----------------+-------------------+-----+-------------+ | United States | Romania | 15 | false | | United States | Croatia | 1 | false | +-----------------+-------------------+-----+-------------+ You can also rename cols using func withColumnRenamed df.withColumnRenamed(\"org_name\", \"new_name\") . If you wish to use some char which is now allowed esacpe it using ` You can remove cols df.drop(\"col_name1\", \"col_name_2\") You can cast the col into different type df.withColumn(\"count2\", col(\"count).cast(\"long\")) Filtering rows. >>> df . filter ( col ( \"count\" ) < 2 ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | United States | Croatia | 1 | | United States | Singapore | 1 | +-----------------+-------------------+-----+ only showing top 2 rows You can also use where to do the same thing. >>> df . where ( col ( \"count\" ) < 2 ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | United States | Croatia | 1 | | United States | Singapore | 1 | +-----------------+-------------------+-----+ only showing top 2 rows Adding multiple where clauses >>> df . where ( col ( \"count\" ) < 2 ) . where ( col ( \"ORIGIN_COUNTRY_NAME\" ) != \"Croatia\" ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | United States | Singapore | 1 | | Moldova | United States | 1 | +-----------------+-------------------+-----+ only showing top 2 rows Getting unique rows (based on cols selected) >>> df . select ( \"ORIGIN_COUNTRY_NAME\" ) . distinct () . count () 125 >>> df . select ( \"ORIGIN_COUNTRY_NAME\" , \"count\" ) . distinct () . count () 220 Getting a sample data out of df. >>> df . sample ( fraction = . 1 ) . count () 23 >>> df . sample ( withReplacement = False , fraction = . 5 , seed = 5 ) . count () 126 Random Splits --> Creates multiple dataframes >>> df . randomSplit ([ 0.1 , 0.5 , 0.4 ], seed = 4 ) [ DataFrame [ DEST_COUNTRY_NAME : string , ORIGIN_COUNTRY_NAME : string , count : bigint ], DataFrame [ DES _COUNTRY_NAME : string , ORIGIN_COUNTRY_NAME : string , count : bigint ], DataFrame [ DEST_COUNTRY_NAME : string , ORIGIN_COUNTRY_NAME : string , count : bigint ]] >>> >>> len ( df . randomSplit ([ 0.1 , 0.5 , 0.4 ], seed = 4 )) 3 Concatenate and Appending rows (Union) Two dataframes which are to be concatenated should have the same schema # lets first split df into df1 and df2. >>> df1 , df2 = df . randomSplit ([ . 5 , . 5 ]) >>> df1 . count () 127 >>> df2 . count () 129 # Both have same schema >>> df1 . schema == df2 . schema True >>> df1 . union ( df2 ) . count () 256 # Putting some where clauses in union >>> df1 . union ( df2 ) . where ( \"count = 1\" ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | Burkina Faso | United States | 1 | | Cyprus | United States | 1 | +-----------------+-------------------+-----+ only showing top 2 rows >>> df1 . union ( df2 ) . where ( \"count = 1\" ) . where ( col ( \"ORIGIN_COUNTRY_NAME\" ) != \"United States\" ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | United States | Croatia | 1 | | United States | Cyprus | 1 | +-----------------+-------------------+-----+ only showing top 2 rows >>> df1 . union ( df2 ) . where ( \"count = 1\" ) . where ( col ( \"ORIGIN_COUNTRY_NAME\" ) != \"United States\" ) . where ( col ( \"DEST_COUNTRY_NAME\" ) != \"United States\" ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ +-----------------+-------------------+-----+ Sort or orderBy --> they both work the same way. df.sort() df.orderBy() >>> df . orderBy ( col ( \"count\" )) . show ( 10 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | Suriname | United States | 1 | | United States | Cyprus | 1 | | United States | Gibraltar | 1 | | Cyprus | United States | 1 | | Moldova | United States | 1 | | Burkina Faso | United States | 1 | | United States | Croatia | 1 | | Djibouti | United States | 1 | | Zambia | United States | 1 | | United States | Estonia | 1 | +-----------------+-------------------+-----+ only showing top 10 rows >>> df . orderBy ( col ( \"count\" ), \"DEST_COUNTRY_NAME\" ) . show ( 10 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | Burkina Faso | United States | 1 | | Cote d 'Ivoire| United States| 1| | Cyprus | United States | 1 | | Djibouti | United States | 1 | | Indonesia | United States | 1 | | Iraq | United States | 1 | | Kosovo | United States | 1 | | Malta | United States | 1 | | Moldova | United States | 1 | | New Caledonia | United States | 1 | +-----------------+-------------------+-----+ only showing top 10 rows >>> df . orderBy ( col ( \"count\" ) . desc ()) . show ( 5 ) +-----------------+-------------------+------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+------+ | United States | United States | 370002 | | United States | Canada | 8483 | | Canada | United States | 8399 | | United States | Mexico | 7187 | | Mexico | United States | 7140 | +-----------------+-------------------+------+ only showing top 5 rows Limiting rows >>> df . limit ( 4 ) . show () +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | United States | Romania | 15 | | United States | Croatia | 1 | | United States | Ireland | 344 | | Egypt | United States | 15 | +-----------------+-------------------+-----+ You can also repartition the data using df.repartition(4) command and check the # of partitions using df.rdd.getNumPartition() in py and df.rdd.getNumPartition in scala. You can also repartition based on a column. df.repartition(col(\"DEST_COUNTRY_NAME\")) . Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions. df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2) You can also \"collect\" the data in the driver i.e. the local machine by using df.collect() There are many functions in Spark you should check them out at: - - API - http://spark.apache.org/docs/latest/api/scala/#package (Change python in link and remome #package) - DataFrameStatFunctions - DataFrameNaFunctions I have noticed that Python API docs are not as good as Scala versions. You may with to check out the scala docs and then see if they have similar in python because the structured api is almost identical in both the scenarios. Boolean Lets pick up another dataset now (retail) and do some operations on it. >>> df = spark . read . format ( \"csv\" ) . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . load ( \"D:/Documents/MEGA/repositories/technotes/docs/Data/spark/retail.csv\" ) >>> df . printSchema () root |-- InvoiceNo : string ( nullable = true ) |-- StockCode : string ( nullable = true ) |-- Description : string ( nullable = true ) |-- Quantity : integer ( nullable = true ) |-- InvoiceDate : timestamp ( nullable = true ) |-- UnitPrice : double ( nullable = true ) |-- CustomerID : double ( nullable = true ) |-- Country : string ( nullable = true ) >>> df . where ( col ( \"InvoiceNo\" ) == 'C579889' ) . show () +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | C579889 | 23245 | SET OF 3 REGENCY ...| - 8 | 2011 - 12 - 01 08 : 12 : 00 | 4.15 | 13853.0 | United Kingdom | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ >>> df . where ( col ( \"InvoiceNo\" ) == 'C579889' ) . select ( \"Country\" , \"StockCode\" ) . show () +--------------+---------+ | Country | StockCode | +--------------+---------+ | United Kingdom | 23245 | +--------------+---------+ # Stacking records vertically --> show function argument >>> df . where ( col ( \"InvoiceNo\" ) != 'C579889' ) . select ( \"Country\" , \"StockCode\" ) . show ( n = 2 , vertical = True ) - RECORD 0 ------------------- Country | United Kingdom StockCode | 84947 - RECORD 1 ------------------- Country | United Kingdom StockCode | 23374 only showing top 2 rows >>> df . where ( \"InvoiceNo <> 579889\" ) . show ( 1 ) +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | 579899 | 23301 | GARDENERS KNEELIN ...| 24 | 2011 - 12 - 01 08 : 33 : 00 | 1.65 | 15687.0 | United Kingdom | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ ``` If Boolean statements ( which we were doing above using where clause or other conditions e . g . == or <> or != ) are expressed serially ( one after the other ), Spark will flatten all of these filters into one statement and perform the filter at the same time , creating the and statement for us . Lets check out another function ` instr ` --> which finds substring . If you make it ` == 1 ` it means that the substring is on position 1 ( index 0 usually ) . If you make it ` > 1 ` would mean that the substring is not the starting of the string but is present . ` 0 ` would mean its not present . ``` py >>> df . where ( instr ( col ( \"StockCode\" ), \"22699\" ) == 1 ) . show () +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | 579927 | 22699 | ROSES REGENCY TEA ...| 6 | 2011 - 12 - 01 09 : 20 : 00 | 2.95 | 12572.0 | Germany | | 579938 | 22699 | ROSES REGENCY TEA ...| 6 | 2011 - 12 - 01 10 : 18 : 00 | 2.95 | 14146.0 | United Kingdom | | C579945 | 22699 | ROSES REGENCY TEA ...| - 1 | 2011 - 12 - 01 10 : 41 : 00 | 2.95 | 15993.0 | United Kingdom | | 580066 | 22699 | ROSES REGENCY TEA ...| 6 | 2011 - 12 - 01 13 : 33 : 00 | 2.95 | 14309.0 | United Kingdom | | 580115 | 22699 | ROSES REGENCY TEA ...| 4 | 2011 - 12 - 01 16 : 22 : 00 | 5.79 | null | United Kingdom | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ You can chain where clause and make & or | statements DOTCodeFilter = col ( \"StockCode\" ) == \"DOT\" priceFilter = col ( \"UnitPrice\" ) > 600 descripFilter = instr ( col ( \"Description\" ), \"POSTAGE\" ) >= 1 df . withColumn ( \"isExpensive\" , DOTCodeFilter & ( priceFilter | descripFilter )) \\ . where ( \"isExpensive\" ) \\ . select ( \"unitPrice\" , \"isExpensive\" ) . show ( 5 ) Numbers >>> df . selectExpr ( ... \"CustomerId\" , ... \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\" ) . show ( 2 ) +----------+------------------+ | CustomerId | realQuantity | +----------+------------------+ | 13853.0 | 1107.2400000000002 | | 15197.0 | 6.5625 | +----------+------------------+ only showing top 2 rows For rounding use round or bround functions. You can calculate the co-relation b/w cols (numerical only) >>> df . stat . corr ( \"Quantity\" , \"UnitPrice\" ) - 0.027002171285054978 Similar to pandas you can call the describe function describe() to get statistical info about the data. >>> df . describe () . show () +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+ | summary | InvoiceNo | StockCode | Description | Quantity | UnitPrice | CustomerID | Country | +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+ | count | 2901 | 2901 | 2900 | 2901 | 2901 | 2226 | 2901 | | mean | 580069.6631130064 | 27023.362062615102 | null | 9.244743192002758 | 3.5609996552912917 | 15423.48382749326 | null | | stddev | 64.35305942291521 | 15666.751318292843 | null | 28.675161860070975 | 19.63596763946906 | 1701.1005317996028 | null | | min | 579899 | 10135 | 50 'S CHRISTMAS G...| -18| 0.0| 12553.0| EIRE| | max | C580131 | POST | ZINC WILLIE WINKI ...| 1200 | 1042.84 | 18130.0 | United Kingdom | +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+ There are many other statistical functions avaiable such as freqItems or approxQuantile etc... Also there is a function monotonically_increasing_id() which generates row # starting from 0. Strings initcap -- makes first word capital >>> df . select ( initcap ( col ( \"Description\" ))) . show ( 10 ) +--------------------+ | initcap ( Description ) | +--------------------+ | Set Of 3 Regency ...| | Antique Silver Te ...| | Red Spot Paper Gi ...| | Multi Colour Silv ...| | Botanical Gardens ...| | French Style Stor ...| | Sweetheart Bird H ...| | Ceramic Cake Stan ...| | Glass Apothecary ...| | Egg Cup Henrietta ...| +--------------------+ only showing top 10 rows Other functions to manipulate the case --> lower , upper Another trivial task is adding or removing spaces around a string. You can do this by using lpad, ltrim, rpad and rtrim, trim: Spark also supports regular expressions. Spark takes advantage of the complete power of Java regular expressions. There are two key functions in Spark that you\u2019ll need in order to perform regular expression tasks: regexp_extract and regexp_replace . These functions extract values and replace values, respectively. Below we ceate a new col 'No COLOR' by replacing individual colors with text \"COLOR\" on description col. >>> regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\" >>> df . select ( regexp_replace ( col ( \"Description\" ), regex_string , \"COLOR\" ) . alias ( \"NO COLOR\" ), col ( \"Description\" )) . show ( 3 ) +--------------------+--------------------+ | NO COLOR | Description | +--------------------+--------------------+ | SET OF 3 REGENCY ...| SET OF 3 REGENCY ...| | ANTIQUE SILVER TE ...| ANTIQUE SILVER TE ...| | COLOR SPOT PAPER ...| RED SPOT PAPER GI ...| +--------------------+--------------------+ To replace every occurance of a character use translate function. >>> df . select ( translate ( col ( \"Description\" ), \"LEET\" , \"1337\" ) . alias ( \"New\" ), col ( \"Description\" ) . alias ( \"OLD\" )) . show ( 10 ) +--------------------+--------------------+ | New | OLD | +--------------------+--------------------+ | S37 OF 3 R3G3NCY ...| SET OF 3 REGENCY ...| | AN7IQU3 SI1V3R 73. ..| ANTIQUE SILVER TE ...| | R3D SPO7 PAP3R GI ...| RED SPOT PAPER GI ...| | MU17I CO1OUR SI1V ...| MULTI COLOUR SILV ...| | BO7ANICA1 GARD3NS ...| BOTANICAL GARDENS ...| | FR3NCH S7Y13 S7OR ...| FRENCH STYLE STOR ...| | SW337H3AR7 BIRD H ...| SWEETHEART BIRD H ...| | C3RAMIC CAK3 S7AN ...| CERAMIC CAKE STAN ...| | G1ASS APO7H3CARY ...| GLASS APOTHECARY ...| | 3 GG CUP H3NRI377A ...| EGG CUP HENRIETTA ...| +--------------------+--------------------+ only showing top 10 rows You can use the method contains to check for a substring. In python you don't need to explicityly call contains but can use the instr method as shown above. Date and timestamps Creating a df with current date and current timestamp. >>> spark . range ( 10 ) . withColumn ( \"today\" , current_date ()) . withColumn ( \"Time\" , current_timestamp ()) . show ( 4 , False ) +---+----------+-----------------------+ | id | today | Time | +---+----------+-----------------------+ | 0 | 2020 - 06 - 13 | 2020 - 06 - 13 21 : 13 : 44.232 | | 1 | 2020 - 06 - 13 | 2020 - 06 - 13 21 : 13 : 44.232 | | 2 | 2020 - 06 - 13 | 2020 - 06 - 13 21 : 13 : 44.232 | | 3 | 2020 - 06 - 13 | 2020 - 06 - 13 21 : 13 : 44.232 | +---+----------+-----------------------+ only showing top 4 rows The schema will look something like this root |-- id: long (nullable = false) |-- today: date (nullable = false) |-- Time: timestamp (nullable = false) Adding or sub dates date_add() date_sub() >>> df . select ( col ( \"today\" ), date_sub ( col ( \"today\" ), 5 ), date_add ( col ( \"today\" ), 5 ) ) . show ( 1 ) +----------+------------------+------------------+ | today | date_sub ( today , 5 ) | date_add ( today , 5 ) | +----------+------------------+------------------+ | 2020 - 06 - 13 | 2020 - 06 - 08 | 2020 - 06 - 18 | +----------+------------------+------------------+ only showing top 1 row To calculate the diff b/w dates you can use the functions datediff , months_between . The to_date function allows you to convert a string to a date, optionally with a specified format. You can also convert to timestamp using to_timestamp function which always requires a format to be passed in. You can compare dates using > and other usual operators. Working with nulls in the data As a best practice, you should always use nulls to represent missing or empty data in your DataFrames. Spark can optimize working with null values more than it can if you use empty strings etc... When interacting with nulls use the .na subpackage on a DataFrame. Various functions to use -- (check out the docs if you wish to learn more) -- df.na.drop() df.na.drop('any') df.na.fill() df.na.replace() Working with complex types Structs Arrays Map Structs You can think of structs as DataFrames within DataFrames. complexDF = df . select ( struct ( \"Description\" , \"InvoiceNo\" ) . alias ( \"complex\" )) # creating a struct. # The above will create a df with col 'complex' which has Description adn InvoiceNo in it. # You can query it as below. complexDF . select ( \"complex.Description\" ) complexDF . select ( col ( \"complex\" ) . getField ( \"Description\" )) We can also query all values in the struct by using *. This brings up all the columns to the top-level DataFrame: complexDF . select ( \"complex.*\" ) Arrays Lets say you have a column such as description, you can split that by space into an array of words. split ( col ( \"Description\" ), \" \" ) Then you can use array_contains methods to look into the array. You can also explode an array into multiple fields using explode explode(col(\"splitted\"))) . Maps Maps are created by using the map function and key-value pairs of columns. df . select ( map ( col ( \"Description\" ), col ( \"InvoiceNo\" )) . alias ( \"complex_map\" )) \\ . selectExpr ( \"complex_map['WHITE METAL LANTERN']\" ) . show ( 2 ) Json TBD UDF Udf's are user defined functions. Aggregations df.count() df.countDistinct() e.g. df.select(countDistinct(\"StockCode\")).show() approx_count_distinct() first() - e.g. df.select(first(\"StockCode\"), last(\"StockCode\")).show() last() min & max --> df.select(min(\"Quantity\"), max(\"Quantity\")).show() sum() sumdistinct() avg() variance - var_samp() or var_pop() standard deviation - stddev_pop or stddev_samp and others Grouping df . groupBy ( \"InvoiceNo\" ) . agg ( count ( \"Quantity\" ) . alias ( \"quan\" ), expr ( \"count(Quantity)\" )) . show () References Spark Submit - Python Jobs","title":"Spark"},{"location":"Data/Spark/#spark","text":"","title":"Spark"},{"location":"Data/Spark/#pyspark-install-steps-taken","text":"Spark Download Trial 1: Pyspark 2.4.5 was only working with python 3.7 and not python 3.8 Ubuntu came by default with python 3.7 so I had to install python 3.7 I built from source in /opt Created a link as /usr/bin/python3.7 pointing to /opt/python3.3.7/python Configured update-alternatives to make python3.7 default. Installed Spark in /opt (nothing more than wget the 2.4.5 spark) Created a system link /opt/spark pointing to /opt/spark2.xxxx As I did not have python on my system and only had python3 I added the following to /spark/conf/spark-env.sh export PYSPARK_PYTHON = python3 Added the following in my .bashrc export SPARK_HOME = /opt/spark export PATH = $SPARK_HOME /bin: $PATH Trial 2: Download spark from Apache spark website, un-tar it in some dir. Create a python venv or download Anaconda distribution, whatever you feel comfortable. Set the PYSPARK_PYTHON variable in conf/spark-env.sh . For example, if Python executable is installed under /opt/anaconda3/bin/python3: PYSPARK_PYTHON='/opt/anaconda3/bin/python3 PIP Pyspark This time I did things a little differently. I created a venv of python3.7 and installed pyspark. The system does not even have apache spark downloaded and insalled. Looks like this is not needed and pyspark (which is of size 225MB) comes with spark built in. So this means we can test spark applications on my laptop with just a pip3 install pyspark and we'll see how to submit them to yarn cluster when the time comes.","title":"Pyspark Install steps taken"},{"location":"Data/Spark/#spark-overview","text":"In Memory Computing Spark shell spark-shell is located in spark/bin To goto spark shell execute the above ./spark-shell In spark-shell you have 2 variables by default sc and spark :help :history RDD RDD is basic unit of data on which all operations are performed. RDD is immutable collection of data (which can be distributed). RDD is split into multiple partitions which are computed on different nodes. RDD can be created on any hadoop input source (that is supported by hadoop). Spark automatically partitions RDDs and distributes the partitions across different nodes. A partition in spark is an atomic chunk of data (logical division of data) stored on a node in the cluster. We can create RDD using - Parallalize method - Takes in a sequence (like list or array) and # of partitions you need the data split on. - CreateDataFrame method parallelize method from pyspark.sql import SparkSession spark = SparkSession.builder.appName ( \"Python Spark create RDD example\" ) .config ( \"local[*]\" ) .getOrCreate () df = spark.sparkContext.parallelize ([( 1 , 2 , 3 , 'a b c' ) , ( 4 , 5 , 6 , 'd e f' ) , ( 7 , 8 , 9 , 'g h i' )]) .toDF ([ 'col1' , 'col2' , 'col3' , 'col4' ]) print ( df ) createDataFrame method from pyspark.sql import SparkSession spark = SparkSession.builder.appName ( \"Python Spark create RDD example\" ) .config ( \"local[*]\" ) .getOrCreate () df = spark.createDataFrame ([ ( 1 , 'Joe' ,2000,1 ) , ( 1 , 'Joe' , 2000 , 1 ) , ( 1 , 'Joe' , 2000 , 1 ) , ( 1 , 'Joe' , 2000 , 1 ) , ] , [ 'ID' , 'Name' , 'Salary' , 'Department' ] ) df.show () df.printSchema () + ---+----+------+----------+ | ID | Name | Salary | Department | + ---+----+------+----------+ | 1 | Joe | 2000 | 1 | | 1 | Joe | 2000 | 1 | | 1 | Joe | 2000 | 1 | | 1 | Joe | 2000 | 1 | + ---+----+------+----------+ root | -- ID: long (nullable = true) | -- Name: string (nullable = true) | -- Salary: long (nullable = true) | -- Department: long (nullable = true) Data can also be read from - csv files - postgres database - HDFS Lets see some examples in Scala as well. # Scala example # Create an array scala > val intArray = Array ( 1 , 2 , 3 , 4 , 5 , 6 ) intArray : Array [ Int ] = Array ( 1 , 2 , 3 , 4 , 5 , 6 ) # Creating an RDD scala > val intRdd = sc . parallelize ( intArray ) # See the first method in your RDD scala > intRdd . first () res1 : Int = 1 # Use the take method to get various elements from RDD # Refer to spark docs for scala / python etc ... for details on methods such as take . scala > intRdd . take ( 2 ) # takes first n elements . res2 : Array [ Int ] = Array ( 1 , 2 ) # to get all elements from an rdd scala > intRdd . collect () res4 : Array [ Int ] = Array ( 1 , 2 , 3 , 4 , 5 , 6 ) # Executing on each rdd scala > intRdd . collect (). foreach ( println ) 1 2 3 4 5 6 # Checking partition size . scala > intRdd . partitions . size res10 : Int = 4 # Python example # Similarly the RDD can be created in python. (Perf implications) >>> a = [ 1 , 2 , 3 , 4 , 5 ] >>> intRdd = sc . parallelize ( a ) >>> type ( intRdd . take ( 2 )) < class ' list '> >>> for x in intRdd . take ( 2 ): ... print ( x ** 2 ) ... 1 4 There are 2 main types of Spark operations (Details in the below sections) - Spark Transformations - Construct a new RDD from a previous one. - Spark Actions - compute a result based on RDD, either return it to driver program or save it to external storage. Lets look at examples. The methods which we will see (which we will call on lists/sequences are standard scala methods such as map , filter , flatmap or distinct . So probably studying scala is not just about the per for RDD but its also how scala functions and what scala functions are utilized in spark (even though they are performed on an RDD and not scala Array or Vector)) scala> val sentences = Array ( \"Today is monday\" , \"The quick brown fox jumped over the lazy dog\" , \"Hi There\" ) scala> val sentRdd = sc.parallelize ( sentences ) # Filter function (a standard scala function as well) scala> val filterRdd = sentRdd.filter ( line = > line.length > 12 ) scala> filterRdd.collect.foreach ( println ) Today is monday The quick brown fox jumped over the lazy dog # Map function (a standard scala map function) scala> val mapRdd = sentRdd.map ( line = > line.length ) scala> mapRdd.collect () res14: Array [ Int ] = Array ( 15 , 44 , 8 ) # ex 2 with map --> here it creates an array of array scala> val mapRdd2 = sentRdd.map ( line = > line.split ( \" \" )) scala> mapRdd2.collect () res15: Array [ Array [ String ]] = Array ( Array ( Today, is, monday ) , Array ( The, quick, brown, fox, jumped, over, the, lazy, dog ) , Array ( Hi, There )) # Flatmap will flatten out the array of arrays and create just array. scala> val mapRdd3 = sentRdd.flatMap ( line = > line.split ( \" \" )) mapRdd3: org.apache.spark.rdd.RDD [ String ] = MapPartitionsRDD [ 7 ] at flatMap at <console>:25 scala> mapRdd3.collect () res16: Array [ String ] = Array ( Today, is, monday, The, quick, brown, fox, jumped, over, the, lazy, dog, Hi, There ) Lets see very similar examples in Python. You will see that its very similar the diff is mainly in using lambda funcs in Python vs fatarrow => functions in Scala. >>> sentences = [ \"Today is monday\" , \"The quick brown fox jumped over the lazy dog\" , \"Hi There\" ] >>> sentRdd = sc . parallelize ( sentences ) >>> mapRdd = sentRdd . map ( lambda line : len ( line ) > 12 ) >>> mapRdd . collect () [ True , True , False ] >>> mapRdd2 = sentRdd . filter ( lambda line : len ( line ) > 12 ) >>> mapRdd2 . collect () [ 'Today is monday' , 'The quick brown fox jumped over the lazy dog' ] >>> mapRdd3 = sentRdd . flatMap ( lambda line : line . split ( \" \" ) ) >>> mapRdd3 . collect () [ 'Today' , 'is' , 'monday' , 'The' , 'quick' , 'brown' , 'fox' , 'jumped' , 'over' , 'the' , 'lazy' , 'dog' , 'Hi' , 'There' ] Spark applications consist of Driver program - This runs your main() function A set of executor processes. The executors will pretty much be running spark code but the driver program can be written in number of different languages exposed via spark api. Scala or Python Lets look at Spark toolkit. Low Level API's RDD's Distributed variables. Structured API's Datasets Dataframes SQL If you are using Structured API's the choice of language does not matter because Spark will reduce the code to low level API's (mor efficiently than you could ever write RDD or distributed variable). If for some reason the structured api does not fit your need then its recommended you switch to Scala. (Note that you can sill write everything in Python but just write a small portion in Scala where it leverages custom RDD, Distribute variables). My strategy is to stick to Python for structured API's and dive in Scala for low level api's. DataFrame The dataframe concept in spark is a little different than Python/R where the data can reside only on 1 machine. However in spark the data underneath that dataframe can reside on multiple machines. Its easy to convert pandas DF to spark. >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"My App\" ) . config ( \"local[*]\" ) . getOrCreate () >>> number = spark . range ( 1000 ) . toDF ( \"number\" ) >>> number . show () +------+ | number | +------+ | 0 | | 1 | | 2 | | 3 | | 4 | | 5 | | 6 | | 7 | | 8 | | 9 | | 10 | | 11 | | 12 | | 13 | | 14 | | 15 | | 16 | | 17 | | 18 | | 19 | +------+ only showing top 20 rows Transformations In spark the core datastructures are immutable, they cannot be changed once created. >>> div_by_2 = number . where ( \"number % 2 = 0\" ) # At this point a new df is not created as it will execute/evaluate this lazyly when someone will reqeust acess to it as we'll do next. >>> div_by_2 . show () +------+ | number | +------+ | 0 | | 2 | | 4 | | 6 | | 8 | | 10 | | 12 | | 14 | | 16 | | 18 | | 20 | | 22 | | 24 | | 26 | | 28 | | 30 | | 32 | | 34 | | 36 | | 38 | +------+ only showing top 20 rows There are 2 types of transformations - narrow : each partition will contribute to only 1 output partition - wide : input partition contributing to many output partitions. (shuffle) Action Transformations build a logical data transformation plan. To trigger computation we run action. e.g. >>> div_by_2 . count () 500 By doing count we - Started a spark job that runs filter transformation (divisible by 2), a narrow transformation. - Aggregation (counting total) which is a wide transformation. Spark UI You can check out spark UI @ http://localhost:4040/ Example A simple example where we read data from csv into a spark df. from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"P\" ) . config ( \"local[*]\" ) . getOrCreate () flight_df = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( \"2015-summary.csv\" ) # As of now the Dataframe has set # of cols and unspecified rows (because it did not read all rows) # -- Think lazy evaulation print ( flight_df . printSchema ()) # Returns first 'n' # of rows. print ( flight_df . take ( 3 )) # The explain command prints out the 'Plan' which spark will take provide output. # Read from bottom to top. Bottom being step 1 and top being last step. # Remember .sort() does not modify the df but returns a new df. print ( flight_df . sort ( \"count\" ) . explain ()) \"\"\" You can also adjust the # of partitions using the below config spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\") \"\"\" Dataframes and SQL Spark can run transformations regardless of the language SQL or DataFrame (Scala, Python, R or Java). In the end all transformations (high level api's) are converted down into a physical plan. This example shows how we can do SQL queries on a df which we convert to a view (temp table). There is no performance diff as they compile down to the same plan. from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"P\" ) . config ( \"local[*]\" ) . getOrCreate () flight_df = spark . read . option ( \"inferSchema\" , \"true\" ) . option ( \"header\" , \"true\" ) . csv ( \"2015-summary.csv\" ) # SQL WAY # convert the df into a temp view table which you can run sql queries on flight_df . createOrReplaceTempView ( \"flight_df_sql\" ) # The df flight_df creates a table (view) flight_df_sql # Now we can do select query on flight_df_sql as though it was a table sqlWay = spark . sql ( \"\"\" SELECT DEST_COUNTRY_NAME, count(1) FROM flight_df_sql GROUP BY DEST_COUNTRY_NAME \"\"\" ) # DF WAY dfWay = flight_df . groupBy ( \"DEST_COUNTRY_NAME\" ) . count () sqlWay . explain () dfWay . explain () == Physical Plan == * ( 2 ) HashAggregate ( keys =[ DEST_COUNTRY_NAME#10 ] , functions =[ count ( 1 )]) +- Exchange hashpartitioning ( DEST_COUNTRY_NAME#10, 200 ) +- * ( 1 ) HashAggregate ( keys =[ DEST_COUNTRY_NAME#10 ] , functions =[ partial_count ( 1 )]) +- * ( 1 ) FileScan csv [ DEST_COUNTRY_NAME#10 ] Batched: false, Format: CSV, Location: InMemoryFileIndex [ file:/home/rs/MEGA/repositories/technotes/docs/Data/spark/2015-summary.csv ] , PartitionFilters: [] , PushedFilters: [] , ReadSchema: struct<DEST_COUNTRY_NAME:string> == Physical Plan == * ( 2 ) HashAggregate ( keys =[ DEST_COUNTRY_NAME#10 ] , functions =[ count ( 1 )]) +- Exchange hashpartitioning ( DEST_COUNTRY_NAME#10, 200 ) +- * ( 1 ) HashAggregate ( keys =[ DEST_COUNTRY_NAME#10 ] , functions =[ partial_count ( 1 )]) +- * ( 1 ) FileScan csv [ DEST_COUNTRY_NAME#10 ] Batched: false, Format: CSV, Location: InMemoryFileIndex [ file:/home/rs/MEGA/repositories/technotes/docs/Data/spark/2015-summary.csv ] , PartitionFilters: [] , PushedFilters: [] , ReadSchema: struct<DEST_COUNTRY_NAME:string> Executing simple queries in DF or SQL >>> print ( spark . sql ( \"\"\" SELECT max(count) from flight_df_sql \"\"\" ) . take ( 1 )) [ Row ( max ( count )= 370002 )] A little more complicated example maxSql = spark . sql ( \"\"\" SELECT DEST_COUNTRY_NAME, sum(count) as destination_total FROM flight_df_sql GROUP BY DEST_COUNTRY_NAME ORDER BY sum(count) DESC LIMIT 5 \"\"\" ) maxSql . show () +-----------------+-----------------+ | DEST_COUNTRY_NAME | destination_total | +-----------------+-----------------+ | United States | 411352 | | Canada | 8399 | | Mexico | 7140 | | United Kingdom | 2025 | | Japan | 1548 | +-----------------+-----------------+","title":"Spark Overview"},{"location":"Data/Spark/#spark-toolset-overview","text":"Running prod applications You can use spark-submit to send your code to a cluster and execute it there. Scala example ./spark-submit --class org.apache.spark.examples.SparkPi --master local ../examples/jars/spark-examples_2.11-2.4.5.jar 10 Python example ./spark-submit --master local ../examples/src/main/python/pi.py 10 By changing the master argument of spark-submit , we can also submit the same application to a cluster running Spark\u2019s standalone cluster manager, Mesos or YARN. Datasets Datasets are used for writing statically typed code in Java & Scala. Its not avaibale in Python or R. The Dataset API gives users the ability to assign a Java/Scala class to the records within a DataFrame and manipulate it as a collection of typed objects, similar to a Java ArrayList or Scala Seq. e.g. for our flights data which has 3 cols. So in a sense each row in our csv or parquet file (in this instance) becomes an object of class Flight. case class Flight ( DEST_COUNTRY_NAME : String , ORIGIN_COUNTRY_NAME : String , count : BigInt ) val flightsDF = spark . read . parquet ( \"/data/flight-data/parquet/2010-summary.parquet/\" ) val flights = flightsDF . as [ Flight ] One final advantage is that when you call collect or take on a Dataset, it will collect objects of the proper type in your Dataset, not DataFrame Rows One great thing about Datasets is that you can use them only when you need or want to. After we\u2019ve performed our manipulations, Spark can automatically turn it back into a DataFrame, and we can manipulate it further by using the hundreds of functions that Spark includes. This makes it easy to drop down to lower level, perform type-safe coding when necessary, and move higher up to SQL for more rapid analysis. Structured Streaming Structured Streaming is a high-level API for stream processing. ML & Advanced Analytics Lower Level API's There are some things that you might use RDDs for, especially when you\u2019re reading or manipulating raw data, but for the most part you should stick to the Structured APIs. RDDs are available in Scala as well as Python. However, they\u2019re not equivalent. (Hint : at RDD level prefer Scala) There are basically no instances in modern Spark, for which you should be using RDDs instead of the structured APIs beyond manipulating some very raw unprocessed and unstructured data.","title":"Spark Toolset Overview"},{"location":"Data/Spark/#structured-api","text":"DataFrames are untyped and DataSets are typed. Datasets API are only available in Scala or Java. Overview of structured api execution Write DF, SQL, Dataset code. Spark converts to a logical plan. Spark converts from logical to physical plan + optimizations. Executes physical plan as RDD manipulations. Spark Types Listed below how to create a Spark Type in scala and py. For list of all sparktypes check out the reference . # Scala import org.apache.spark.sql.types._ val b = ByteType # Python from pyspark.sql.types import * b = ByteType () These sparktypes are used to instanciate or declare a col to be of certain type. Lets create a dataframe using a json file. As we have seen that scala and py can both be used for these high level api's I'll use either. (No prferences). # Creating a dataframe >>> df = spark . read . format ( \"json\" ) . load ( \"D:/Documents/MEGA/repositories/technotes/docs/Data/spark/2015-summary.json\" ) >>> df DataFrame [ DEST_COUNTRY_NAME : string , ORIGIN_COUNTRY_NAME : string , count : bigint ] >>> df . printSchema () root |-- DEST_COUNTRY_NAME : string ( nullable = true ) |-- ORIGIN_COUNTRY_NAME : string ( nullable = true ) |-- count : long ( nullable = true ) >>> df . take ( 1 ) [ Row ( DEST_COUNTRY_NAME = 'United States' , ORIGIN_COUNTRY_NAME = 'Romania' , count = 15 )] Schema defines the col names of a DataFrame. (We can let spark read/infer the schema or define explicitly ourselves.) Note for spark in prod usage, its best to define explicit schema. The schema is of StructType , lets see an example as read the abvoe json file with explict manual schema. Refer to the docs here # in Python from pyspark.sql.types import StructField , StructType , StringType , LongType myManualSchema = StructType ([ StructField ( \"DEST_COUNTRY_NAME\" , StringType (), True ), StructField ( \"ORIGIN_COUNTRY_NAME\" , StringType (), True ), StructField ( \"count\" , LongType (), False , metadata = { \"hello\" : \"world\" }) ]) df = spark . read . format ( \"json\" ) . schema ( myManualSchema ) . load ( \"D:/Documents/MEGA/repositories/technotes/docs/Data/spark/2015-summary.json\" ) Checking cols of a df. (you can loop on them its a list) >>> df . columns [ 'DEST_COUNTRY_NAME' , 'ORIGIN_COUNTRY_NAME' , 'count' ] Getting the first row. >>> df . first () Row ( DEST_COUNTRY_NAME = 'United States' , ORIGIN_COUNTRY_NAME = 'Romania' , count = 15 ) >>> row1 = df . first () >>> row1 Row ( DEST_COUNTRY_NAME = 'United States' , ORIGIN_COUNTRY_NAME = 'Romania' , count = 15 ) >>> row1 [ 0 ] 'United States' >>> row1 [ 1 ] 'Romania' You can also create new rows. >>> myRow = Row ( \"US\" , \"Canada\" , 12 ) You can convert the dataframe into temporary table where you can do sql queries on them. You can also convert Row into DataFrame by using the function createDataFrame . Doing simple queries on df. >>> df . select ( \"DEST_COUNTRY_NAME\" ) . show ( 2 ) +-----------------+ | DEST_COUNTRY_NAME | +-----------------+ | United States | | United States | +-----------------+ only showing top 2 rows >>> df . select ( expr ( \"DEST_COUNTRY_NAME AS destination\" )) . show ( 2 ) +-------------+ | destination | +-------------+ | United States | | United States | +-------------+ only showing top 2 rows # SelectExpr --> Select from a DataFrame using a set of SQL expressions. # In below example it creates a new col using comparison of 2 cols. >>> df . selectExpr ( \"*\" , \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\" ) . show ( 2 ) +-----------------+-------------------+-----+-------------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | withinCountry | +-----------------+-------------------+-----+-------------+ | United States | Romania | 15 | false | | United States | Croatia | 1 | false | +-----------------+-------------------+-----+-------------+ only showing top 2 rows >>> df . selectExpr ( \"*\" ,( \"ORIGIN_COUNTRY_NAME = 'Romania'\" )) . show ( 2 ) +-----------------+-------------------+-----+-------------------------------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | ( ORIGIN_COUNTRY_NAME = Romania ) | true | | United States | Croatia | 1 | false | +-----------------+-------------------+-----+-------------------------------+ only showing top 2 rows # Instead of using the selectExpr you can also use select and use expr function # where you need to create a new col using sql statement as shown below. # expr is avaiable from pyspark.sql.functions so do the following # >>> from pyspark.sql.functions import * >>> df . select ( \"*\" , expr ( \"ORIGIN_COUNTRY_NAME = 'Romania'\" )) . show ( 2 ) +-----------------+-------------------+-----+-------------------------------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | ( ORIGIN_COUNTRY_NAME = Romania ) | +-----------------+-------------------+-----+-------------------------------+ | United States | Romania | 15 | true | | United States | Croatia | 1 | false | +-----------------+-------------------+-----+-------------------------------+ only showing top 2 rows # Creating a new col (count *2) >>> df . select ( \"*\" ,( df [ \"count\" ] * 2 )) . show ( 2 ) +-----------------+-------------------+-----+-----------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | ( count * 2 ) | +-----------------+-------------------+-----+-----------+ | United States | Romania | 15 | 30 | | United States | Croatia | 1 | 2 | +-----------------+-------------------+-----+-----------+ only showing top 2 rows # Rename that col. >>> df . select ( \"*\" ,( df [ \"count\" ] * 2 ) . alias ( \"2day\" )) . show ( 2 ) +-----------------+-------------------+-----+----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | 2 day | +-----------------+-------------------+-----+----+ | United States | Romania | 15 | 30 | | United States | Croatia | 1 | 2 | +-----------------+-------------------+-----+----+ only showing top 2 rows Adding a new cols as literals # Adding a new col with value 1 will return a new df. >>> df . select ( \"*\" , lit ( 1 )) . show ( 2 ) +-----------------+-------------------+-----+---+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | 1 | +-----------------+-------------------+-----+---+ | United States | Romania | 15 | 1 | | United States | Croatia | 1 | 1 | +-----------------+-------------------+-----+---+ only showing top 2 rows # you can do the same using withColumn <-- This is a more formal way >>> df . withColumn ( \"One\" , lit ( 1 )) . show ( 2 ) +-----------------+-------------------+-----+---+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | One | +-----------------+-------------------+-----+---+ | United States | Romania | 15 | 1 | | United States | Croatia | 1 | 1 | +-----------------+-------------------+-----+---+ only showing top 2 rows # As you see withColumn takes 2 args, 1) col name 2) expr >>> df . withColumn ( \"withinCountry\" , expr ( \"DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME\" )) . show ( 2 ) +-----------------+-------------------+-----+-------------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | withinCountry | +-----------------+-------------------+-----+-------------+ | United States | Romania | 15 | false | | United States | Croatia | 1 | false | +-----------------+-------------------+-----+-------------+ You can also rename cols using func withColumnRenamed df.withColumnRenamed(\"org_name\", \"new_name\") . If you wish to use some char which is now allowed esacpe it using ` You can remove cols df.drop(\"col_name1\", \"col_name_2\") You can cast the col into different type df.withColumn(\"count2\", col(\"count).cast(\"long\")) Filtering rows. >>> df . filter ( col ( \"count\" ) < 2 ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | United States | Croatia | 1 | | United States | Singapore | 1 | +-----------------+-------------------+-----+ only showing top 2 rows You can also use where to do the same thing. >>> df . where ( col ( \"count\" ) < 2 ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | United States | Croatia | 1 | | United States | Singapore | 1 | +-----------------+-------------------+-----+ only showing top 2 rows Adding multiple where clauses >>> df . where ( col ( \"count\" ) < 2 ) . where ( col ( \"ORIGIN_COUNTRY_NAME\" ) != \"Croatia\" ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | United States | Singapore | 1 | | Moldova | United States | 1 | +-----------------+-------------------+-----+ only showing top 2 rows Getting unique rows (based on cols selected) >>> df . select ( \"ORIGIN_COUNTRY_NAME\" ) . distinct () . count () 125 >>> df . select ( \"ORIGIN_COUNTRY_NAME\" , \"count\" ) . distinct () . count () 220 Getting a sample data out of df. >>> df . sample ( fraction = . 1 ) . count () 23 >>> df . sample ( withReplacement = False , fraction = . 5 , seed = 5 ) . count () 126 Random Splits --> Creates multiple dataframes >>> df . randomSplit ([ 0.1 , 0.5 , 0.4 ], seed = 4 ) [ DataFrame [ DEST_COUNTRY_NAME : string , ORIGIN_COUNTRY_NAME : string , count : bigint ], DataFrame [ DES _COUNTRY_NAME : string , ORIGIN_COUNTRY_NAME : string , count : bigint ], DataFrame [ DEST_COUNTRY_NAME : string , ORIGIN_COUNTRY_NAME : string , count : bigint ]] >>> >>> len ( df . randomSplit ([ 0.1 , 0.5 , 0.4 ], seed = 4 )) 3 Concatenate and Appending rows (Union) Two dataframes which are to be concatenated should have the same schema # lets first split df into df1 and df2. >>> df1 , df2 = df . randomSplit ([ . 5 , . 5 ]) >>> df1 . count () 127 >>> df2 . count () 129 # Both have same schema >>> df1 . schema == df2 . schema True >>> df1 . union ( df2 ) . count () 256 # Putting some where clauses in union >>> df1 . union ( df2 ) . where ( \"count = 1\" ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | Burkina Faso | United States | 1 | | Cyprus | United States | 1 | +-----------------+-------------------+-----+ only showing top 2 rows >>> df1 . union ( df2 ) . where ( \"count = 1\" ) . where ( col ( \"ORIGIN_COUNTRY_NAME\" ) != \"United States\" ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | United States | Croatia | 1 | | United States | Cyprus | 1 | +-----------------+-------------------+-----+ only showing top 2 rows >>> df1 . union ( df2 ) . where ( \"count = 1\" ) . where ( col ( \"ORIGIN_COUNTRY_NAME\" ) != \"United States\" ) . where ( col ( \"DEST_COUNTRY_NAME\" ) != \"United States\" ) . show ( 2 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ +-----------------+-------------------+-----+ Sort or orderBy --> they both work the same way. df.sort() df.orderBy() >>> df . orderBy ( col ( \"count\" )) . show ( 10 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | Suriname | United States | 1 | | United States | Cyprus | 1 | | United States | Gibraltar | 1 | | Cyprus | United States | 1 | | Moldova | United States | 1 | | Burkina Faso | United States | 1 | | United States | Croatia | 1 | | Djibouti | United States | 1 | | Zambia | United States | 1 | | United States | Estonia | 1 | +-----------------+-------------------+-----+ only showing top 10 rows >>> df . orderBy ( col ( \"count\" ), \"DEST_COUNTRY_NAME\" ) . show ( 10 ) +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | Burkina Faso | United States | 1 | | Cote d 'Ivoire| United States| 1| | Cyprus | United States | 1 | | Djibouti | United States | 1 | | Indonesia | United States | 1 | | Iraq | United States | 1 | | Kosovo | United States | 1 | | Malta | United States | 1 | | Moldova | United States | 1 | | New Caledonia | United States | 1 | +-----------------+-------------------+-----+ only showing top 10 rows >>> df . orderBy ( col ( \"count\" ) . desc ()) . show ( 5 ) +-----------------+-------------------+------+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+------+ | United States | United States | 370002 | | United States | Canada | 8483 | | Canada | United States | 8399 | | United States | Mexico | 7187 | | Mexico | United States | 7140 | +-----------------+-------------------+------+ only showing top 5 rows Limiting rows >>> df . limit ( 4 ) . show () +-----------------+-------------------+-----+ | DEST_COUNTRY_NAME | ORIGIN_COUNTRY_NAME | count | +-----------------+-------------------+-----+ | United States | Romania | 15 | | United States | Croatia | 1 | | United States | Ireland | 344 | | Egypt | United States | 15 | +-----------------+-------------------+-----+ You can also repartition the data using df.repartition(4) command and check the # of partitions using df.rdd.getNumPartition() in py and df.rdd.getNumPartition in scala. You can also repartition based on a column. df.repartition(col(\"DEST_COUNTRY_NAME\")) . Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions. df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2) You can also \"collect\" the data in the driver i.e. the local machine by using df.collect() There are many functions in Spark you should check them out at: - - API - http://spark.apache.org/docs/latest/api/scala/#package (Change python in link and remome #package) - DataFrameStatFunctions - DataFrameNaFunctions I have noticed that Python API docs are not as good as Scala versions. You may with to check out the scala docs and then see if they have similar in python because the structured api is almost identical in both the scenarios. Boolean Lets pick up another dataset now (retail) and do some operations on it. >>> df = spark . read . format ( \"csv\" ) . option ( \"header\" , \"true\" ) . option ( \"inferSchema\" , \"true\" ) . load ( \"D:/Documents/MEGA/repositories/technotes/docs/Data/spark/retail.csv\" ) >>> df . printSchema () root |-- InvoiceNo : string ( nullable = true ) |-- StockCode : string ( nullable = true ) |-- Description : string ( nullable = true ) |-- Quantity : integer ( nullable = true ) |-- InvoiceDate : timestamp ( nullable = true ) |-- UnitPrice : double ( nullable = true ) |-- CustomerID : double ( nullable = true ) |-- Country : string ( nullable = true ) >>> df . where ( col ( \"InvoiceNo\" ) == 'C579889' ) . show () +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | C579889 | 23245 | SET OF 3 REGENCY ...| - 8 | 2011 - 12 - 01 08 : 12 : 00 | 4.15 | 13853.0 | United Kingdom | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ >>> df . where ( col ( \"InvoiceNo\" ) == 'C579889' ) . select ( \"Country\" , \"StockCode\" ) . show () +--------------+---------+ | Country | StockCode | +--------------+---------+ | United Kingdom | 23245 | +--------------+---------+ # Stacking records vertically --> show function argument >>> df . where ( col ( \"InvoiceNo\" ) != 'C579889' ) . select ( \"Country\" , \"StockCode\" ) . show ( n = 2 , vertical = True ) - RECORD 0 ------------------- Country | United Kingdom StockCode | 84947 - RECORD 1 ------------------- Country | United Kingdom StockCode | 23374 only showing top 2 rows >>> df . where ( \"InvoiceNo <> 579889\" ) . show ( 1 ) +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | 579899 | 23301 | GARDENERS KNEELIN ...| 24 | 2011 - 12 - 01 08 : 33 : 00 | 1.65 | 15687.0 | United Kingdom | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ ``` If Boolean statements ( which we were doing above using where clause or other conditions e . g . == or <> or != ) are expressed serially ( one after the other ), Spark will flatten all of these filters into one statement and perform the filter at the same time , creating the and statement for us . Lets check out another function ` instr ` --> which finds substring . If you make it ` == 1 ` it means that the substring is on position 1 ( index 0 usually ) . If you make it ` > 1 ` would mean that the substring is not the starting of the string but is present . ` 0 ` would mean its not present . ``` py >>> df . where ( instr ( col ( \"StockCode\" ), \"22699\" ) == 1 ) . show () +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | InvoiceNo | StockCode | Description | Quantity | InvoiceDate | UnitPrice | CustomerID | Country | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ | 579927 | 22699 | ROSES REGENCY TEA ...| 6 | 2011 - 12 - 01 09 : 20 : 00 | 2.95 | 12572.0 | Germany | | 579938 | 22699 | ROSES REGENCY TEA ...| 6 | 2011 - 12 - 01 10 : 18 : 00 | 2.95 | 14146.0 | United Kingdom | | C579945 | 22699 | ROSES REGENCY TEA ...| - 1 | 2011 - 12 - 01 10 : 41 : 00 | 2.95 | 15993.0 | United Kingdom | | 580066 | 22699 | ROSES REGENCY TEA ...| 6 | 2011 - 12 - 01 13 : 33 : 00 | 2.95 | 14309.0 | United Kingdom | | 580115 | 22699 | ROSES REGENCY TEA ...| 4 | 2011 - 12 - 01 16 : 22 : 00 | 5.79 | null | United Kingdom | +---------+---------+--------------------+--------+-------------------+---------+----------+--------------+ You can chain where clause and make & or | statements DOTCodeFilter = col ( \"StockCode\" ) == \"DOT\" priceFilter = col ( \"UnitPrice\" ) > 600 descripFilter = instr ( col ( \"Description\" ), \"POSTAGE\" ) >= 1 df . withColumn ( \"isExpensive\" , DOTCodeFilter & ( priceFilter | descripFilter )) \\ . where ( \"isExpensive\" ) \\ . select ( \"unitPrice\" , \"isExpensive\" ) . show ( 5 ) Numbers >>> df . selectExpr ( ... \"CustomerId\" , ... \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\" ) . show ( 2 ) +----------+------------------+ | CustomerId | realQuantity | +----------+------------------+ | 13853.0 | 1107.2400000000002 | | 15197.0 | 6.5625 | +----------+------------------+ only showing top 2 rows For rounding use round or bround functions. You can calculate the co-relation b/w cols (numerical only) >>> df . stat . corr ( \"Quantity\" , \"UnitPrice\" ) - 0.027002171285054978 Similar to pandas you can call the describe function describe() to get statistical info about the data. >>> df . describe () . show () +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+ | summary | InvoiceNo | StockCode | Description | Quantity | UnitPrice | CustomerID | Country | +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+ | count | 2901 | 2901 | 2900 | 2901 | 2901 | 2226 | 2901 | | mean | 580069.6631130064 | 27023.362062615102 | null | 9.244743192002758 | 3.5609996552912917 | 15423.48382749326 | null | | stddev | 64.35305942291521 | 15666.751318292843 | null | 28.675161860070975 | 19.63596763946906 | 1701.1005317996028 | null | | min | 579899 | 10135 | 50 'S CHRISTMAS G...| -18| 0.0| 12553.0| EIRE| | max | C580131 | POST | ZINC WILLIE WINKI ...| 1200 | 1042.84 | 18130.0 | United Kingdom | +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+ There are many other statistical functions avaiable such as freqItems or approxQuantile etc... Also there is a function monotonically_increasing_id() which generates row # starting from 0. Strings initcap -- makes first word capital >>> df . select ( initcap ( col ( \"Description\" ))) . show ( 10 ) +--------------------+ | initcap ( Description ) | +--------------------+ | Set Of 3 Regency ...| | Antique Silver Te ...| | Red Spot Paper Gi ...| | Multi Colour Silv ...| | Botanical Gardens ...| | French Style Stor ...| | Sweetheart Bird H ...| | Ceramic Cake Stan ...| | Glass Apothecary ...| | Egg Cup Henrietta ...| +--------------------+ only showing top 10 rows Other functions to manipulate the case --> lower , upper Another trivial task is adding or removing spaces around a string. You can do this by using lpad, ltrim, rpad and rtrim, trim: Spark also supports regular expressions. Spark takes advantage of the complete power of Java regular expressions. There are two key functions in Spark that you\u2019ll need in order to perform regular expression tasks: regexp_extract and regexp_replace . These functions extract values and replace values, respectively. Below we ceate a new col 'No COLOR' by replacing individual colors with text \"COLOR\" on description col. >>> regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\" >>> df . select ( regexp_replace ( col ( \"Description\" ), regex_string , \"COLOR\" ) . alias ( \"NO COLOR\" ), col ( \"Description\" )) . show ( 3 ) +--------------------+--------------------+ | NO COLOR | Description | +--------------------+--------------------+ | SET OF 3 REGENCY ...| SET OF 3 REGENCY ...| | ANTIQUE SILVER TE ...| ANTIQUE SILVER TE ...| | COLOR SPOT PAPER ...| RED SPOT PAPER GI ...| +--------------------+--------------------+ To replace every occurance of a character use translate function. >>> df . select ( translate ( col ( \"Description\" ), \"LEET\" , \"1337\" ) . alias ( \"New\" ), col ( \"Description\" ) . alias ( \"OLD\" )) . show ( 10 ) +--------------------+--------------------+ | New | OLD | +--------------------+--------------------+ | S37 OF 3 R3G3NCY ...| SET OF 3 REGENCY ...| | AN7IQU3 SI1V3R 73. ..| ANTIQUE SILVER TE ...| | R3D SPO7 PAP3R GI ...| RED SPOT PAPER GI ...| | MU17I CO1OUR SI1V ...| MULTI COLOUR SILV ...| | BO7ANICA1 GARD3NS ...| BOTANICAL GARDENS ...| | FR3NCH S7Y13 S7OR ...| FRENCH STYLE STOR ...| | SW337H3AR7 BIRD H ...| SWEETHEART BIRD H ...| | C3RAMIC CAK3 S7AN ...| CERAMIC CAKE STAN ...| | G1ASS APO7H3CARY ...| GLASS APOTHECARY ...| | 3 GG CUP H3NRI377A ...| EGG CUP HENRIETTA ...| +--------------------+--------------------+ only showing top 10 rows You can use the method contains to check for a substring. In python you don't need to explicityly call contains but can use the instr method as shown above. Date and timestamps Creating a df with current date and current timestamp. >>> spark . range ( 10 ) . withColumn ( \"today\" , current_date ()) . withColumn ( \"Time\" , current_timestamp ()) . show ( 4 , False ) +---+----------+-----------------------+ | id | today | Time | +---+----------+-----------------------+ | 0 | 2020 - 06 - 13 | 2020 - 06 - 13 21 : 13 : 44.232 | | 1 | 2020 - 06 - 13 | 2020 - 06 - 13 21 : 13 : 44.232 | | 2 | 2020 - 06 - 13 | 2020 - 06 - 13 21 : 13 : 44.232 | | 3 | 2020 - 06 - 13 | 2020 - 06 - 13 21 : 13 : 44.232 | +---+----------+-----------------------+ only showing top 4 rows The schema will look something like this root |-- id: long (nullable = false) |-- today: date (nullable = false) |-- Time: timestamp (nullable = false) Adding or sub dates date_add() date_sub() >>> df . select ( col ( \"today\" ), date_sub ( col ( \"today\" ), 5 ), date_add ( col ( \"today\" ), 5 ) ) . show ( 1 ) +----------+------------------+------------------+ | today | date_sub ( today , 5 ) | date_add ( today , 5 ) | +----------+------------------+------------------+ | 2020 - 06 - 13 | 2020 - 06 - 08 | 2020 - 06 - 18 | +----------+------------------+------------------+ only showing top 1 row To calculate the diff b/w dates you can use the functions datediff , months_between . The to_date function allows you to convert a string to a date, optionally with a specified format. You can also convert to timestamp using to_timestamp function which always requires a format to be passed in. You can compare dates using > and other usual operators. Working with nulls in the data As a best practice, you should always use nulls to represent missing or empty data in your DataFrames. Spark can optimize working with null values more than it can if you use empty strings etc... When interacting with nulls use the .na subpackage on a DataFrame. Various functions to use -- (check out the docs if you wish to learn more) -- df.na.drop() df.na.drop('any') df.na.fill() df.na.replace() Working with complex types Structs Arrays Map Structs You can think of structs as DataFrames within DataFrames. complexDF = df . select ( struct ( \"Description\" , \"InvoiceNo\" ) . alias ( \"complex\" )) # creating a struct. # The above will create a df with col 'complex' which has Description adn InvoiceNo in it. # You can query it as below. complexDF . select ( \"complex.Description\" ) complexDF . select ( col ( \"complex\" ) . getField ( \"Description\" )) We can also query all values in the struct by using *. This brings up all the columns to the top-level DataFrame: complexDF . select ( \"complex.*\" ) Arrays Lets say you have a column such as description, you can split that by space into an array of words. split ( col ( \"Description\" ), \" \" ) Then you can use array_contains methods to look into the array. You can also explode an array into multiple fields using explode explode(col(\"splitted\"))) . Maps Maps are created by using the map function and key-value pairs of columns. df . select ( map ( col ( \"Description\" ), col ( \"InvoiceNo\" )) . alias ( \"complex_map\" )) \\ . selectExpr ( \"complex_map['WHITE METAL LANTERN']\" ) . show ( 2 ) Json TBD UDF Udf's are user defined functions. Aggregations df.count() df.countDistinct() e.g. df.select(countDistinct(\"StockCode\")).show() approx_count_distinct() first() - e.g. df.select(first(\"StockCode\"), last(\"StockCode\")).show() last() min & max --> df.select(min(\"Quantity\"), max(\"Quantity\")).show() sum() sumdistinct() avg() variance - var_samp() or var_pop() standard deviation - stddev_pop or stddev_samp and others Grouping df . groupBy ( \"InvoiceNo\" ) . agg ( count ( \"Quantity\" ) . alias ( \"quan\" ), expr ( \"count(Quantity)\" )) . show ()","title":"Structured API"},{"location":"Data/Spark/#references","text":"Spark Submit - Python Jobs","title":"References"},{"location":"Data/avro/","text":"Avro File Avro Schema Details Basics Avro Docs Tutorial A quick poc where I used a maven project for dependencies on Intellij with Java8 to create avro file. Steps given below. Create a pom.xml with org.apache.avro dependency and the below mentioned build plugin. <!-- pom.xml --> <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> org.me </groupId> <artifactId> PracticingJava </artifactId> <version> 1.0-SNAPSHOT </version> <dependencies> <dependency> <groupId> org.apache.avro </groupId> <artifactId> avro </artifactId> <version> 1.10.0 </version> </dependency> </dependencies> <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <build> <plugins> <plugin> <groupId> org.apache.avro </groupId> <artifactId> avro-maven-plugin </artifactId> <version> 1.10.0 </version> <executions> <execution> <phase> generate-sources </phase> <goals> <goal> schema </goal> </goals> <configuration> <sourceDirectory> ${project.basedir}/src/main/resources/avro/ </sourceDirectory> <outputDirectory> ${project.basedir}/src/main/java/ </outputDirectory> </configuration> </execution> </executions> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> </plugins> </build> </project> To create a avro file you need to define a avro schema in .avsc file. as shown below. I created a user.avsc file in Resources dir. Pay attention to sourceDirectory and outputDirectory in pom xml. sourceDirectory should have the avsc file and the outputDirectory is where the build class will appear. { \"namespace\" : \"example.avro\" , \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"favorite_number\" , \"type\" : [ \"int\" , \"null\" ]}, { \"name\" : \"favorite_color\" , \"type\" : [ \"string\" , \"null\" ]} ] } Once done creating the avsc file, you can go to maven tab/side bar in intellij and click lifecycle --> package which will build the package and create the User class in the sourceDirectory . Now you can use the class in your code. import example.avro.User ; import org.apache.avro.file.DataFileWriter ; import org.apache.avro.io.DatumWriter ; import org.apache.avro.specific.SpecificDatumWriter ; import java.io.File ; import java.io.IOException ; public class Main12 { public static void main ( String [] args ) throws IOException { User user = new User ( \"Sam Nelson\" , 11 , \"red12\" ); // User class generated by maven. // Setting data user . setName ( \"Sam Nelson\" ); user . setFavoriteColor ( \"red\" ); user . setFavoriteNumber ( 12 ); // Saving the avro file on disk. File file = new File ( \"user.avro\" ); DatumWriter < User > userDatumWriter = new SpecificDatumWriter <> ( User . class ); DataFileWriter < User > dataFileWriter = new DataFileWriter < User > ( userDatumWriter ); dataFileWriter . create ( user . getSchema (), file ); dataFileWriter . append ( user ); dataFileWriter . close (); } } Once you run the above it will create a \"user.avro\" file. Another working avro schema given below. { \"type\" : \"record\" , \"name\" : \"userInfo\" , \"namespace\" : \"example.avro\" , \"fields\" : [ { \"name\" : \"username\" , \"type\" : \"string\" , \"default\" : \"NONE\" }, { \"name\" : \"age\" , \"type\" : \"int\" , \"default\" : -1 }, { \"name\" : \"address\" , \"type\" : { \"type\" : \"record\" , \"name\" : \"mailing_address\" , \"fields\" : [ { \"name\" : \"street\" , \"type\" : \"string\" , \"default\" : \"NONE\" }, { \"name\" : \"city\" , \"type\" : \"string\" , \"default\" : \"NONE\" }, { \"name\" : \"state_prov\" , \"type\" : \"string\" , \"default\" : \"NONE\" }, { \"name\" : \"country\" , \"type\" : \"string\" , \"default\" : \"NONE\" }, { \"name\" : \"zip\" , \"type\" : \"string\" , \"default\" : \"NONE\" } ] }, \"default\" : {} } ] }","title":"Avro File"},{"location":"Data/avro/#avro-file","text":"Avro Schema Details","title":"Avro File"},{"location":"Data/avro/#basics","text":"Avro Docs Tutorial A quick poc where I used a maven project for dependencies on Intellij with Java8 to create avro file. Steps given below. Create a pom.xml with org.apache.avro dependency and the below mentioned build plugin. <!-- pom.xml --> <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> org.me </groupId> <artifactId> PracticingJava </artifactId> <version> 1.0-SNAPSHOT </version> <dependencies> <dependency> <groupId> org.apache.avro </groupId> <artifactId> avro </artifactId> <version> 1.10.0 </version> </dependency> </dependencies> <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <build> <plugins> <plugin> <groupId> org.apache.avro </groupId> <artifactId> avro-maven-plugin </artifactId> <version> 1.10.0 </version> <executions> <execution> <phase> generate-sources </phase> <goals> <goal> schema </goal> </goals> <configuration> <sourceDirectory> ${project.basedir}/src/main/resources/avro/ </sourceDirectory> <outputDirectory> ${project.basedir}/src/main/java/ </outputDirectory> </configuration> </execution> </executions> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> </plugins> </build> </project> To create a avro file you need to define a avro schema in .avsc file. as shown below. I created a user.avsc file in Resources dir. Pay attention to sourceDirectory and outputDirectory in pom xml. sourceDirectory should have the avsc file and the outputDirectory is where the build class will appear. { \"namespace\" : \"example.avro\" , \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"favorite_number\" , \"type\" : [ \"int\" , \"null\" ]}, { \"name\" : \"favorite_color\" , \"type\" : [ \"string\" , \"null\" ]} ] } Once done creating the avsc file, you can go to maven tab/side bar in intellij and click lifecycle --> package which will build the package and create the User class in the sourceDirectory . Now you can use the class in your code. import example.avro.User ; import org.apache.avro.file.DataFileWriter ; import org.apache.avro.io.DatumWriter ; import org.apache.avro.specific.SpecificDatumWriter ; import java.io.File ; import java.io.IOException ; public class Main12 { public static void main ( String [] args ) throws IOException { User user = new User ( \"Sam Nelson\" , 11 , \"red12\" ); // User class generated by maven. // Setting data user . setName ( \"Sam Nelson\" ); user . setFavoriteColor ( \"red\" ); user . setFavoriteNumber ( 12 ); // Saving the avro file on disk. File file = new File ( \"user.avro\" ); DatumWriter < User > userDatumWriter = new SpecificDatumWriter <> ( User . class ); DataFileWriter < User > dataFileWriter = new DataFileWriter < User > ( userDatumWriter ); dataFileWriter . create ( user . getSchema (), file ); dataFileWriter . append ( user ); dataFileWriter . close (); } } Once you run the above it will create a \"user.avro\" file. Another working avro schema given below. { \"type\" : \"record\" , \"name\" : \"userInfo\" , \"namespace\" : \"example.avro\" , \"fields\" : [ { \"name\" : \"username\" , \"type\" : \"string\" , \"default\" : \"NONE\" }, { \"name\" : \"age\" , \"type\" : \"int\" , \"default\" : -1 }, { \"name\" : \"address\" , \"type\" : { \"type\" : \"record\" , \"name\" : \"mailing_address\" , \"fields\" : [ { \"name\" : \"street\" , \"type\" : \"string\" , \"default\" : \"NONE\" }, { \"name\" : \"city\" , \"type\" : \"string\" , \"default\" : \"NONE\" }, { \"name\" : \"state_prov\" , \"type\" : \"string\" , \"default\" : \"NONE\" }, { \"name\" : \"country\" , \"type\" : \"string\" , \"default\" : \"NONE\" }, { \"name\" : \"zip\" , \"type\" : \"string\" , \"default\" : \"NONE\" } ] }, \"default\" : {} } ] }","title":"Basics"},{"location":"Data/scala/","text":"Scala The basics Declaring variables. In Scala, you are encouraged to use a val unless you really need to change the contents. Perhaps surprisingly for Java or C++ programmers, most programs don\u2019t need many var variables. val a = 10 // val declares a constant val xmax , ymax = 100 var x = 20 // var declares a variable You can specify the type of variable if you wish to. val a : Int = 10 val b : String = \"Hello World\" var greeting , message : String = null Semicolons are optional and required only if multiple statements on the same line. Scala has 7 numeric types, all these are classes though. Thus you can call methods on them. - Byte - Char - Short - Int - Long - Float - Double val a = 10 a . toString You also have - BigInt - BigDecimal Simple math operations a + 10 // This is a shorthand for a.+(10) In general, you can write method b as a shorthand for a.method(b) e.g. 1.to(10) can be written as 1 to 10 If the method has no parameters, you don\u2019t have to use parentheses e.g. \"Bonjour\".sorted and \"Bonjour\".sorted() are pretty much the same. Apply Method s.apply(4) shorthand is s(4) val s = \"Hello\" s ( 4 ) // o Conditionals Single liner if statement if (cond) statement1 else statement2 . This if statement has a value as shown below. scala > if ( x > 1 ) 1 else 0 val res24 : Int = 1 if ( x > 10 ){ println ( \"x is greater than 10\" ) } else if ( x == 10 ){ println ( \"x is equal to 10\" ) } else { println ( \"x is less than 10\" ) } Blocks The value of the block is the value of the last expression. val distance = { val dx = x - x0 ; val dy = y - y0 ; sqrt ( dx * dx + dy * dy ) } A block which ends in assignment has a Unit value which is same as void in java. { r = r * n ; n -= 1 } print print () println () print ( f\"Hello, $name , your age in 6 months will be ${ age + 6 } \" ) // formatted string reading input import scala.io.StdIn val name = StdIn . readLine ( \"Your name : \" ) println ( name ) To read Int, Double etc... you can use readInt , readDouble loops while loop var n = 1 while ( n < 10 ){ n += 1 println ( n ) } for loop for ( i <- 1 to 10 ){ println ( i ) } val name = \"Sam Nelson\" for ( c <- name ) println ( c ) val name = \"Sam Nelson\" for ( i <- 0 until name . length ) println ( name ( i )) for ( i <- 1 to 3 ; j <- 1 to 4 ) println ( f\"i - $i j = $j \" ) generates the output i - 1 j = 1 i - 1 j = 2 i - 1 j = 3 i - 1 j = 4 i - 2 j = 1 i - 2 j = 2 i - 2 j = 3 i - 2 j = 4 i - 3 j = 1 i - 3 j = 2 i - 3 j = 3 i - 3 j = 4 for ( i <- 1 to 3 ; j <- 1 to 4 if i != j ) println ( f\"i - $i j = $j \" ) generates i - 1 j = 2 i - 1 j = 3 i - 1 j = 4 i - 2 j = 1 i - 2 j = 3 i - 2 j = 4 i - 3 j = 1 i - 3 j = 2 i - 3 j = 4 When body of loop starts with yield it constructs a collection of values. for ( i <- 1 to 10 ) yield i % 3 // Yields Vector(1, 2, 0, 1, 2, 0, 1, 2, 0, 1) functions To define a function use the def keyword, you must specify the types of all parameters. As long as function is not recursive you need not specify the return type of the function. Note : Remember in scala function cannot be defined outside a class or a object. --> In Python lets say you can do that. You also do not need a return statement, last statement is returned. def abs ( x : Double ) = { if ( x >= 0 ) x else - x } you can specify the default value of argument. Looks very-very similar to Python where the un-named must come first and named comes after in the call of the function. A function which takes variable number of arguments. def sum ( args : Int* ) = { var result = 0 for ( element <- args ) result += element result } sum ( 10 , 20 , 30 , 40 , 50 ) // 150 // sum(1 to 10) // error <-- as this is range and not int. sum ( 1 to 10 :_ * ) // This notation : _* tells the compiler to pass each element of arr as its own argument if you omit the = in function definition it becomes a procedure and does not return anything i.e. returns Unit e.g. def x (){ // look no = e.g. no x() =, better stick with x(): Unit = to be explicit } lazy values If lazy is used lazy, the variable/constant initialization is deferred until it is accessed for the first time. lazy val words = xxxx Exceptions Scala exceptions work the same way as in Java or C++. When you throw an exception, for example if ( x >= 0 ) { sqrt ( x ) } else throw new IllegalArgumentException ( \"x should not be negative\" ) syntax for catching exception val url = new URL ( \"http://horstmann.com/fred-tiny.gif\" ) try { process ( url ) } catch { case _: MalformedURLException => println ( s\"Bad URL: $url \" ) case ex : IOException => ex . printStackTrace () } you also have finally which is executed whether or not the process function throws an exception finally { in . close () } Arrays Fixed length array = Array Variable length array = ArrayBuffer val nums = new Array [ Int ]( 10 ) // new is not req if you are supplying initial values for ( i <- 0 until 10 ) nums ( i ) = i for ( i <- 0 until 10 ) println ( nums ( i )) val s = Array ( \"Hello\" , \"World\" ) for ( word <- s ) println ( word ) Lets do a few examples with ArrayBuffer. import scala.collection.mutable.ArrayBuffer val names = ArrayBuffer [ String ]() // new not req because we did not specify the # of elements req in the ArrayBuffer names += \"Sam\" names += ( \"Pam\" , \"Lam\" , \"Tam\" ) for ( name <- names ) println ( name ) removing from end names . trimEnd ( 2 ) for ( name <- names ) println ( name ) // Sam Pam You can use insert and remove function to do operations on arbitraty index but those refactor the entire array. Adding at end or removing from end are better for performance. You can convert from Array to ArrayBuffer and vice versa .toArray or .toBuffer You can create a transformed new array from another array using yeild statement in the loop. val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) for ( element <- arr1 ) yield element * 2 // creates a new array val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) for ( element <- arr1 if element % 2 == 0 ) yield element * 2 common functions val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) arr1 . sum arr1 . max sorting val arr1 = Array ( 1 , 10 , 3 , 106 , 8 , 9 ) arr1 . sorted // retuns a new sorted array, there is also sortInPlace There are a couple of other sorters in import scala.util.Sorting. Display array by making it a string. val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) arr1 . mkString ( \", \" ) // val res0: String = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 Counting # of negetive vals. val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) arr1 . count ( _ < 0 ) // _ acts as a each variable, I tried with other variable names just works with _ or you can do the same as below without _ val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) arr1 . count ( a => a < 0 ) Multi dimension arrays. val matrix = Array . ofDim [ Double ]( 3 , 4 ) // Three rows, four columns matrix ( row )( column ) = 42 Maps val scores = Map ( \"Alice\" -> 10 , \"Bob\" -> 3 , \"Cindy\" -> 8 ) // immutable map val scores = scala . collection . mutable . Map ( \"Alice\" -> 10 , \"Bob\" -> 3 , \"Cindy\" -> 8 ) // mutable map // You can omit -> by use of () val scores = Map (( \"Alice\" , 10 ), ( \"Bob\" , 3 ), ( \"Cindy\" , 8 )) // immutable map // getting values from map - If value is not there exception is thrown val bobsScore = scores ( \"Bob\" ) // To avoid exception use getOrElse val bobsScore = scores . getOrElse ( \"Bob\" , 0 ) // Update values or create new ones --> only in mutable maps scores ( \"Bob\" ) = 10 scores ( \"Fred\" ) = 7 scores += ( \"Bob\" -> 10 , \"Fred\" -> 7 ) // remove values scores -= \"Alice\" Iterating over map val scores = Map ( \"Alice\" -> 10 , \"Bob\" -> 3 , \"Cindy\" -> 8 ) // immutable map for (( k , v ) <- scores ) println ( f\"Name $k - Score $v \" ) There are two common implementation strategies for maps: hash tables and balanced trees. Hash tables use the hash codes of the keys to scramble entries, so iterating over the elements yields them in unpredictable order. By default, Scala gives you a map based on a hash table because it is usually more efficient. If you need to visit the keys in sorted order, use a scala.collection.mutable.SortedMap instead. If you want to visit the keys in insertion order, use a scala.collection.mutable.LinkedHashMap Tuples Creating a tuple val t = ( 1 , 3.14 , \"Fred\" ) println ( t . _2 ) // 3.14 --> index starts from 1 and not 0 Accessing values the better way. val t = ( 1 , 3.14 , \"Fred\" ) val ( id , score , name ) = t // unpacking a tuple. val ( row_id , _ , _ ) = t // use _ if you do not need all components Zipping val data = Array ( Array ( 1 , 3.14 , \"Fred\" ), Array ( 2 , 4.14 , \"Sam\" )) val cols = Array ( \"id\" , \"score\" , \"name\" ) for ( record <- data ){ for (( k , v ) <- cols . zip ( record )) println ( k , v ) } (id,1) (score,3.14) (name,Fred) (id,2) (score,4.14) (name,Sam) Classes A simple Scala class class Counter { // values must be initialized private var value = 0 def increment () { value += 1 } // methods are public by default def current () = value // a function which retuns value of `value` sort of a getter } val counter = new Counter counter . increment () println ( counter . current ) It is considered a good style to use () for mutator methods (a method that chagnes object state) and drop the () for accessor methods (which do not change the object state) Getters and Setters for private variables If you have a private variable and you wish to generate getters and setters this is the recommended way to doing it in scala. class Counter { // values must be initialized private var _value : Int = 0 def value = _value def value_= ( newVal : Int ){ // pay attention to = before () and not after if ( newVal > 0 ) _value = newVal } } var counter = new Counter () counter . value = - 10 println ( counter . value ) object private fields Any private variable or constant can be accessed by the methods of that class, also if you pass another object you can still access the private variable of that object. (E.g. when comparing 2 instances of same class) If you wish that the private variable remains private for only that instance then you can declare it as private[this] . Its sometimes called object private. Bean properties Bean property will auto generate - Scala getter setter - Java getter setter import scala.beans.BeanProperty import scala.beans.BeanProperty class Person { @BeanProperty var name : String = \"\" } Constructors A Scala class can have many. - 1 Primary Constructor. The primary constructor is intervowen in class definition. - Any number of auxiliary constructors called this . Each auxiliary constructor must start with a call to previously defined auxiliary constructor or the primary constructor. class Person { private var name = \"\" private var age = 0 def this ( name : String ){ // auxiliary constructor this () // calls primary constructor this . name = name } def this ( name : String , age : Int ){ // auxiliary constructor this ( name ) // calls the previous constructor this . age = age } } Lets look at some examples of primary constructor. class Person ( private val name : String , var age : Int = 0 ){ // primary // constructor arguments println ( \"I am part of primary constructor\" ) // any statement in the class not in method is part of primary constructor. def birthday () = this . age += 1 } val sam = new Person ( \"Sam\" , 10 ) sam . birthday () println ( sam . age ) println ( sam . name ) // error as name is private If you wish to mark the whole primary constructor as private class Person private ( private val name : String , var age : Int ){} If you do not put var or val in the constructor argument it becomes a field which is private[this] to the primary constructor. It\u2019s just a regular parameter that can be accessed in the code of the primary constructor. Constructor arguments mapping to getters and setters. - no val/var - object private field - private val/var - private field private getter/setter - val/var - private field public getter/setter - @BeanProperty - private field public java and scala getter/setter Nested Class Will cover this later. Objects Scala has no static methods or fields, instead use object. --> Defines a single instance of a class. object Account { private var serial = 0 def newSerial () = { serial += 10 serial } } for ( i <- 1 until 10 ) println ( f\"A new serail # ${ Account . newSerial () } \" ) A new serail # 10 A new serail # 20 A new serail # 30 A new serail # 40 A new serail # 50 A new serail # 60 A new serail # 70 A new serail # 80 A new serail # 90 The constructor of object is executed when its first used. object Account { print ( \"First use\" ) def x () = println ( \"In x\" ) } Account . x () // First Use In x Account . x () // In x They are mostly used for Home for utility functions or constants Single immutable instance can be shared When a single instance is required to co-ordinate some service e.g. reading env config etc... As a companion objects. A companion object has the same name as the class and can access class's private variable and methods, it needs to be placed in the same source file as the class itself and has the same name as the class. class Account ( val holder : String ){ val accountNumber = Account . newNumber () // you cannot directly call newNumber() must do Account.newNumber() println ( accountNumber ) } object Account { private var serial = 10 def newNumber () = { serial += 1 serial } } // Introducing main function object Main { def main ( args : Array [ String ]) { val a1 = new Account ( \"Sam Nelson\" ) val a2 = new Account ( \"Pete Nelson\" ) } } An object can extend a class and/or one or more traits. The result is an object of a class that extends the given class and/or traits, and in addition has all of the features specified in the object definition. Apply method Its similar to __call__ in python. You do not need to say .apply() object Mult { def apply ( x : Int ) = x / 2 } Mult ( 10 ) // 5 Mult . apply ( 10 ) // 5 Apply can be used on Object or on a Class. On Object it will act on the Object. On class it will act on the instance of that class. Lets see an example on the class. class Person { def apply () = println ( \"In apply\" ) } val p1 = new Person () p1 () // calls the apply method Typically, such an apply method returns an object of the companion class. class Person private (){ } object Person { def apply () = { println ( \"In apply\" ) new Person () } } val p1 = Person () // no need of new val p2 = new Person () // cannot do this as constructor of the class is private Application Objects Each Scala program must start with an object's main method of type Array[String] object Hello { def main ( args : Array [ String ] ) { } } Instead of providing main method you can also extend App object main extends App { println ( \"sfd\" ) } In the above case if you need commandline arguments you can get from args property e.g. args(0) etc... Enumerations object TrafficLightColor extends Enumeration { val Red , Yellow , Green = Value } To study in more detail. Packages and import You can import package using the import statement. import java.awt.color.ColorSpace can use wildcard to import eveything as below. import scala.collection._ Imports can be anywhere not just on top of the file. The scope of the import extends will the end of the block in which the import was placed. Import multiple things from the same package. import java.awt.color { ActiveEvent , AWTError } You can rename the class/object the same was you do in Python e.g. import x as y import java.util. { HashMap => JavaHashMap } Creating packages To add your items to a package you can nest them as shown below. package com { package mylastname { package myfirstname { class Person (){} } } } package com { package compname { class Person (){} } } package com.anothercomp { // dont create 2 nested package just chain them class Person {} } Then you can access it as com.mylastname.myfirstname.Person() You can define 1 package in multiple files. You can define multiple packages in 1 file as shown above. Instead of nesting everything in package you can put them on top of the file as shown below but everything in that file will then belong to that pcakge. package com.company.mypackage class Person {} alternatively package com.company package mypackage // breaking it down into 2 instead of just 1 class Person {} A package can contain classes, objects, and traits, but not the definitions of functions or variables. To address this we have package objects. Every package can have one package object. You define it in the parent package, package com.horstmann.impatient package object people { val defaultName = \"John Q. Public\" } Package visibility To control the visibility of a class member package wide you can add a package as shown below private [ impatient ] def description = s\"A person with name $name \" // where impatient is the package name Inheritance Extends keyword inherits the file class A {} class B extends A {} You can declare a class as final and it will not be inheritable. final class c {} // cannot he inherited To override method use the override keyword (where method is not abstract). class A { def x ()={} } class B extends A { override def x () = {} } Invoking a superclass method in Scala works exactly like in Java, with the keyword super class A { def x ()={} } class B extends A { super . x () } Only a primary constructor of the subclass can call the primary constructor of a super class. Here you see a class Employee getting created which extends Person and its primary constructor also passes the values (name, age) to Person class. (As the primary constructor is intertwined with the class itself.) class Employee ( name : String , age : Int , val salary : Double ) extends Person ( name , age ) Overriding fields - A def can override another def - A val can override another val - A var can override an abstract var Anonymous class class Person ( name : String ){} val alien = new Person ( \"Fred\" ) { def greeting = \"Greetings, Earthling! My name is Fred.\" // anonymus class } You can create abstract class in Scala, which cannot be instanciated abstract class Person {} In Scala the eq or equals method checks whether they refer to the same instance. You can override this to get the desired functionality. It will not hurt to read the class chapter again as I skimmed through a some of the material. Files The below examples creates a line iterator to process line 1 at a time. import scala.io.Source object main ext ends App { val source = Source . fromFile ( \"src/main/resources/file.txt\" ) val lineIterator = source . getLines () for ( line <- lineIterator ) println ( line ) } import scala.io.Source object main extends App { val source = Source . fromFile ( \"src/main/resources/file.txt\" ) var lines = source . getLines . toBuffer // or use toArray for ( line <- lines ) { println ( line ) } }","title":"Scala"},{"location":"Data/scala/#scala","text":"","title":"Scala"},{"location":"Data/scala/#the-basics","text":"Declaring variables. In Scala, you are encouraged to use a val unless you really need to change the contents. Perhaps surprisingly for Java or C++ programmers, most programs don\u2019t need many var variables. val a = 10 // val declares a constant val xmax , ymax = 100 var x = 20 // var declares a variable You can specify the type of variable if you wish to. val a : Int = 10 val b : String = \"Hello World\" var greeting , message : String = null Semicolons are optional and required only if multiple statements on the same line. Scala has 7 numeric types, all these are classes though. Thus you can call methods on them. - Byte - Char - Short - Int - Long - Float - Double val a = 10 a . toString You also have - BigInt - BigDecimal Simple math operations a + 10 // This is a shorthand for a.+(10) In general, you can write method b as a shorthand for a.method(b) e.g. 1.to(10) can be written as 1 to 10 If the method has no parameters, you don\u2019t have to use parentheses e.g. \"Bonjour\".sorted and \"Bonjour\".sorted() are pretty much the same. Apply Method s.apply(4) shorthand is s(4) val s = \"Hello\" s ( 4 ) // o Conditionals Single liner if statement if (cond) statement1 else statement2 . This if statement has a value as shown below. scala > if ( x > 1 ) 1 else 0 val res24 : Int = 1 if ( x > 10 ){ println ( \"x is greater than 10\" ) } else if ( x == 10 ){ println ( \"x is equal to 10\" ) } else { println ( \"x is less than 10\" ) } Blocks The value of the block is the value of the last expression. val distance = { val dx = x - x0 ; val dy = y - y0 ; sqrt ( dx * dx + dy * dy ) } A block which ends in assignment has a Unit value which is same as void in java. { r = r * n ; n -= 1 } print print () println () print ( f\"Hello, $name , your age in 6 months will be ${ age + 6 } \" ) // formatted string reading input import scala.io.StdIn val name = StdIn . readLine ( \"Your name : \" ) println ( name ) To read Int, Double etc... you can use readInt , readDouble loops while loop var n = 1 while ( n < 10 ){ n += 1 println ( n ) } for loop for ( i <- 1 to 10 ){ println ( i ) } val name = \"Sam Nelson\" for ( c <- name ) println ( c ) val name = \"Sam Nelson\" for ( i <- 0 until name . length ) println ( name ( i )) for ( i <- 1 to 3 ; j <- 1 to 4 ) println ( f\"i - $i j = $j \" ) generates the output i - 1 j = 1 i - 1 j = 2 i - 1 j = 3 i - 1 j = 4 i - 2 j = 1 i - 2 j = 2 i - 2 j = 3 i - 2 j = 4 i - 3 j = 1 i - 3 j = 2 i - 3 j = 3 i - 3 j = 4 for ( i <- 1 to 3 ; j <- 1 to 4 if i != j ) println ( f\"i - $i j = $j \" ) generates i - 1 j = 2 i - 1 j = 3 i - 1 j = 4 i - 2 j = 1 i - 2 j = 3 i - 2 j = 4 i - 3 j = 1 i - 3 j = 2 i - 3 j = 4 When body of loop starts with yield it constructs a collection of values. for ( i <- 1 to 10 ) yield i % 3 // Yields Vector(1, 2, 0, 1, 2, 0, 1, 2, 0, 1) functions To define a function use the def keyword, you must specify the types of all parameters. As long as function is not recursive you need not specify the return type of the function. Note : Remember in scala function cannot be defined outside a class or a object. --> In Python lets say you can do that. You also do not need a return statement, last statement is returned. def abs ( x : Double ) = { if ( x >= 0 ) x else - x } you can specify the default value of argument. Looks very-very similar to Python where the un-named must come first and named comes after in the call of the function. A function which takes variable number of arguments. def sum ( args : Int* ) = { var result = 0 for ( element <- args ) result += element result } sum ( 10 , 20 , 30 , 40 , 50 ) // 150 // sum(1 to 10) // error <-- as this is range and not int. sum ( 1 to 10 :_ * ) // This notation : _* tells the compiler to pass each element of arr as its own argument if you omit the = in function definition it becomes a procedure and does not return anything i.e. returns Unit e.g. def x (){ // look no = e.g. no x() =, better stick with x(): Unit = to be explicit } lazy values If lazy is used lazy, the variable/constant initialization is deferred until it is accessed for the first time. lazy val words = xxxx Exceptions Scala exceptions work the same way as in Java or C++. When you throw an exception, for example if ( x >= 0 ) { sqrt ( x ) } else throw new IllegalArgumentException ( \"x should not be negative\" ) syntax for catching exception val url = new URL ( \"http://horstmann.com/fred-tiny.gif\" ) try { process ( url ) } catch { case _: MalformedURLException => println ( s\"Bad URL: $url \" ) case ex : IOException => ex . printStackTrace () } you also have finally which is executed whether or not the process function throws an exception finally { in . close () }","title":"The basics"},{"location":"Data/scala/#arrays","text":"Fixed length array = Array Variable length array = ArrayBuffer val nums = new Array [ Int ]( 10 ) // new is not req if you are supplying initial values for ( i <- 0 until 10 ) nums ( i ) = i for ( i <- 0 until 10 ) println ( nums ( i )) val s = Array ( \"Hello\" , \"World\" ) for ( word <- s ) println ( word ) Lets do a few examples with ArrayBuffer. import scala.collection.mutable.ArrayBuffer val names = ArrayBuffer [ String ]() // new not req because we did not specify the # of elements req in the ArrayBuffer names += \"Sam\" names += ( \"Pam\" , \"Lam\" , \"Tam\" ) for ( name <- names ) println ( name ) removing from end names . trimEnd ( 2 ) for ( name <- names ) println ( name ) // Sam Pam You can use insert and remove function to do operations on arbitraty index but those refactor the entire array. Adding at end or removing from end are better for performance. You can convert from Array to ArrayBuffer and vice versa .toArray or .toBuffer You can create a transformed new array from another array using yeild statement in the loop. val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) for ( element <- arr1 ) yield element * 2 // creates a new array val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) for ( element <- arr1 if element % 2 == 0 ) yield element * 2 common functions val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) arr1 . sum arr1 . max sorting val arr1 = Array ( 1 , 10 , 3 , 106 , 8 , 9 ) arr1 . sorted // retuns a new sorted array, there is also sortInPlace There are a couple of other sorters in import scala.util.Sorting. Display array by making it a string. val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) arr1 . mkString ( \", \" ) // val res0: String = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 Counting # of negetive vals. val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) arr1 . count ( _ < 0 ) // _ acts as a each variable, I tried with other variable names just works with _ or you can do the same as below without _ val arr1 = Array ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) arr1 . count ( a => a < 0 ) Multi dimension arrays. val matrix = Array . ofDim [ Double ]( 3 , 4 ) // Three rows, four columns matrix ( row )( column ) = 42","title":"Arrays"},{"location":"Data/scala/#maps","text":"val scores = Map ( \"Alice\" -> 10 , \"Bob\" -> 3 , \"Cindy\" -> 8 ) // immutable map val scores = scala . collection . mutable . Map ( \"Alice\" -> 10 , \"Bob\" -> 3 , \"Cindy\" -> 8 ) // mutable map // You can omit -> by use of () val scores = Map (( \"Alice\" , 10 ), ( \"Bob\" , 3 ), ( \"Cindy\" , 8 )) // immutable map // getting values from map - If value is not there exception is thrown val bobsScore = scores ( \"Bob\" ) // To avoid exception use getOrElse val bobsScore = scores . getOrElse ( \"Bob\" , 0 ) // Update values or create new ones --> only in mutable maps scores ( \"Bob\" ) = 10 scores ( \"Fred\" ) = 7 scores += ( \"Bob\" -> 10 , \"Fred\" -> 7 ) // remove values scores -= \"Alice\" Iterating over map val scores = Map ( \"Alice\" -> 10 , \"Bob\" -> 3 , \"Cindy\" -> 8 ) // immutable map for (( k , v ) <- scores ) println ( f\"Name $k - Score $v \" ) There are two common implementation strategies for maps: hash tables and balanced trees. Hash tables use the hash codes of the keys to scramble entries, so iterating over the elements yields them in unpredictable order. By default, Scala gives you a map based on a hash table because it is usually more efficient. If you need to visit the keys in sorted order, use a scala.collection.mutable.SortedMap instead. If you want to visit the keys in insertion order, use a scala.collection.mutable.LinkedHashMap","title":"Maps"},{"location":"Data/scala/#tuples","text":"Creating a tuple val t = ( 1 , 3.14 , \"Fred\" ) println ( t . _2 ) // 3.14 --> index starts from 1 and not 0 Accessing values the better way. val t = ( 1 , 3.14 , \"Fred\" ) val ( id , score , name ) = t // unpacking a tuple. val ( row_id , _ , _ ) = t // use _ if you do not need all components Zipping val data = Array ( Array ( 1 , 3.14 , \"Fred\" ), Array ( 2 , 4.14 , \"Sam\" )) val cols = Array ( \"id\" , \"score\" , \"name\" ) for ( record <- data ){ for (( k , v ) <- cols . zip ( record )) println ( k , v ) } (id,1) (score,3.14) (name,Fred) (id,2) (score,4.14) (name,Sam)","title":"Tuples"},{"location":"Data/scala/#classes","text":"A simple Scala class class Counter { // values must be initialized private var value = 0 def increment () { value += 1 } // methods are public by default def current () = value // a function which retuns value of `value` sort of a getter } val counter = new Counter counter . increment () println ( counter . current ) It is considered a good style to use () for mutator methods (a method that chagnes object state) and drop the () for accessor methods (which do not change the object state) Getters and Setters for private variables If you have a private variable and you wish to generate getters and setters this is the recommended way to doing it in scala. class Counter { // values must be initialized private var _value : Int = 0 def value = _value def value_= ( newVal : Int ){ // pay attention to = before () and not after if ( newVal > 0 ) _value = newVal } } var counter = new Counter () counter . value = - 10 println ( counter . value ) object private fields Any private variable or constant can be accessed by the methods of that class, also if you pass another object you can still access the private variable of that object. (E.g. when comparing 2 instances of same class) If you wish that the private variable remains private for only that instance then you can declare it as private[this] . Its sometimes called object private. Bean properties Bean property will auto generate - Scala getter setter - Java getter setter import scala.beans.BeanProperty import scala.beans.BeanProperty class Person { @BeanProperty var name : String = \"\" } Constructors A Scala class can have many. - 1 Primary Constructor. The primary constructor is intervowen in class definition. - Any number of auxiliary constructors called this . Each auxiliary constructor must start with a call to previously defined auxiliary constructor or the primary constructor. class Person { private var name = \"\" private var age = 0 def this ( name : String ){ // auxiliary constructor this () // calls primary constructor this . name = name } def this ( name : String , age : Int ){ // auxiliary constructor this ( name ) // calls the previous constructor this . age = age } } Lets look at some examples of primary constructor. class Person ( private val name : String , var age : Int = 0 ){ // primary // constructor arguments println ( \"I am part of primary constructor\" ) // any statement in the class not in method is part of primary constructor. def birthday () = this . age += 1 } val sam = new Person ( \"Sam\" , 10 ) sam . birthday () println ( sam . age ) println ( sam . name ) // error as name is private If you wish to mark the whole primary constructor as private class Person private ( private val name : String , var age : Int ){} If you do not put var or val in the constructor argument it becomes a field which is private[this] to the primary constructor. It\u2019s just a regular parameter that can be accessed in the code of the primary constructor. Constructor arguments mapping to getters and setters. - no val/var - object private field - private val/var - private field private getter/setter - val/var - private field public getter/setter - @BeanProperty - private field public java and scala getter/setter Nested Class Will cover this later.","title":"Classes"},{"location":"Data/scala/#objects","text":"Scala has no static methods or fields, instead use object. --> Defines a single instance of a class. object Account { private var serial = 0 def newSerial () = { serial += 10 serial } } for ( i <- 1 until 10 ) println ( f\"A new serail # ${ Account . newSerial () } \" ) A new serail # 10 A new serail # 20 A new serail # 30 A new serail # 40 A new serail # 50 A new serail # 60 A new serail # 70 A new serail # 80 A new serail # 90 The constructor of object is executed when its first used. object Account { print ( \"First use\" ) def x () = println ( \"In x\" ) } Account . x () // First Use In x Account . x () // In x They are mostly used for Home for utility functions or constants Single immutable instance can be shared When a single instance is required to co-ordinate some service e.g. reading env config etc... As a companion objects. A companion object has the same name as the class and can access class's private variable and methods, it needs to be placed in the same source file as the class itself and has the same name as the class. class Account ( val holder : String ){ val accountNumber = Account . newNumber () // you cannot directly call newNumber() must do Account.newNumber() println ( accountNumber ) } object Account { private var serial = 10 def newNumber () = { serial += 1 serial } } // Introducing main function object Main { def main ( args : Array [ String ]) { val a1 = new Account ( \"Sam Nelson\" ) val a2 = new Account ( \"Pete Nelson\" ) } } An object can extend a class and/or one or more traits. The result is an object of a class that extends the given class and/or traits, and in addition has all of the features specified in the object definition. Apply method Its similar to __call__ in python. You do not need to say .apply() object Mult { def apply ( x : Int ) = x / 2 } Mult ( 10 ) // 5 Mult . apply ( 10 ) // 5 Apply can be used on Object or on a Class. On Object it will act on the Object. On class it will act on the instance of that class. Lets see an example on the class. class Person { def apply () = println ( \"In apply\" ) } val p1 = new Person () p1 () // calls the apply method Typically, such an apply method returns an object of the companion class. class Person private (){ } object Person { def apply () = { println ( \"In apply\" ) new Person () } } val p1 = Person () // no need of new val p2 = new Person () // cannot do this as constructor of the class is private Application Objects Each Scala program must start with an object's main method of type Array[String] object Hello { def main ( args : Array [ String ] ) { } } Instead of providing main method you can also extend App object main extends App { println ( \"sfd\" ) } In the above case if you need commandline arguments you can get from args property e.g. args(0) etc... Enumerations object TrafficLightColor extends Enumeration { val Red , Yellow , Green = Value } To study in more detail.","title":"Objects"},{"location":"Data/scala/#packages-and-import","text":"You can import package using the import statement. import java.awt.color.ColorSpace can use wildcard to import eveything as below. import scala.collection._ Imports can be anywhere not just on top of the file. The scope of the import extends will the end of the block in which the import was placed. Import multiple things from the same package. import java.awt.color { ActiveEvent , AWTError } You can rename the class/object the same was you do in Python e.g. import x as y import java.util. { HashMap => JavaHashMap } Creating packages To add your items to a package you can nest them as shown below. package com { package mylastname { package myfirstname { class Person (){} } } } package com { package compname { class Person (){} } } package com.anothercomp { // dont create 2 nested package just chain them class Person {} } Then you can access it as com.mylastname.myfirstname.Person() You can define 1 package in multiple files. You can define multiple packages in 1 file as shown above. Instead of nesting everything in package you can put them on top of the file as shown below but everything in that file will then belong to that pcakge. package com.company.mypackage class Person {} alternatively package com.company package mypackage // breaking it down into 2 instead of just 1 class Person {} A package can contain classes, objects, and traits, but not the definitions of functions or variables. To address this we have package objects. Every package can have one package object. You define it in the parent package, package com.horstmann.impatient package object people { val defaultName = \"John Q. Public\" } Package visibility To control the visibility of a class member package wide you can add a package as shown below private [ impatient ] def description = s\"A person with name $name \" // where impatient is the package name","title":"Packages and import"},{"location":"Data/scala/#inheritance","text":"Extends keyword inherits the file class A {} class B extends A {} You can declare a class as final and it will not be inheritable. final class c {} // cannot he inherited To override method use the override keyword (where method is not abstract). class A { def x ()={} } class B extends A { override def x () = {} } Invoking a superclass method in Scala works exactly like in Java, with the keyword super class A { def x ()={} } class B extends A { super . x () } Only a primary constructor of the subclass can call the primary constructor of a super class. Here you see a class Employee getting created which extends Person and its primary constructor also passes the values (name, age) to Person class. (As the primary constructor is intertwined with the class itself.) class Employee ( name : String , age : Int , val salary : Double ) extends Person ( name , age ) Overriding fields - A def can override another def - A val can override another val - A var can override an abstract var Anonymous class class Person ( name : String ){} val alien = new Person ( \"Fred\" ) { def greeting = \"Greetings, Earthling! My name is Fred.\" // anonymus class } You can create abstract class in Scala, which cannot be instanciated abstract class Person {} In Scala the eq or equals method checks whether they refer to the same instance. You can override this to get the desired functionality. It will not hurt to read the class chapter again as I skimmed through a some of the material.","title":"Inheritance"},{"location":"Data/scala/#files","text":"The below examples creates a line iterator to process line 1 at a time. import scala.io.Source object main ext ends App { val source = Source . fromFile ( \"src/main/resources/file.txt\" ) val lineIterator = source . getLines () for ( line <- lineIterator ) println ( line ) } import scala.io.Source object main extends App { val source = Source . fromFile ( \"src/main/resources/file.txt\" ) var lines = source . getLines . toBuffer // or use toArray for ( line <- lines ) { println ( line ) } }","title":"Files"},{"location":"Frontend/JS-01-Basics/","text":"JS-01-Basics Language History and Tools JavaScript is based on specification for ECMAScript. JS can execute in browser, server and any device which has the JavaScript engine. Various Engines V8 in Chrome & Opera SpiderMonkey \u2013 in Firefox Points to note Usage 2 ways Use a script tag in html. Include a JS file. < script > alert ( 'Hello, world!' ); </ script > Comments // single line /* multi line comments */ Use Strict \"use strict\" switches the engine to the modern mode. Put this as the first line of the script and its recommended to always have the strict mode enabled. Variables and Data types Variables Are case sensitive. Use let and const . Numbers numbers are both integer and floating point numbers. you can use typeof(a) to check the type of a variable. let billion = 1 e9 ; // declaring large numbers with e notation. Converting to Strings let num = 255 ; alert ( num . toString ( 16 ) ); // ff Rounding Math.floor Math.ceil Math.round Math.trunc Fixed decimal places let num = 12.34 ; alert ( num . toFixed ( 1 ) ); // \"12.3\" parse from a string alert ( parseInt ( '100px' ) ); // 100 alert ( parseFloat ( '12.5em' ) ); // 12.5 Strings enclosed by single or double quotes. using backticks you can use ${<var>} to embed value in the string. You can also put expression inside this e.g. ${1 + 2} Check length using strVariale.length Access specific character using strVariable[index] Strings are immutable 'Interface'.toUpperCase() // INTERFACE Getting index of a substring staring from given pos -> str.indexOf(substr, pos) str.lastIndexOf(substr, position) includes() endsWith() startsWith() Iterating over characters for ( let char of \"Hello\" ) { alert ( char ); // H,e,l,l,o (char becomes \"H\", then \"e\", then \"l\" etc) } SubStrings str . slice ( 0 , 5 ) Conversions String to Number let str = '123' let num = Number ( str ) Boolean Boolean(1) is true Boolean(0) is false Comparison alert ( '' == false ); // true alert ( 0 === false ); // false, because the types are different Prompt / Alert / Confirm prompt Its used to input value. result = prompt ( 'Message' , default ); e.g. result = prompt ( 'Enter your age' , 100 ); confirm Its used to confirm a user action. It returns true or false depending on user clicks OK or Cancel. result = confirm ( 'Do you wish to continue ?' ); Conditionals If condition if ( condition ) { } else if ( condition ) { } else { } let accessAllowed = ( age > 18 ) ? true : false ; || - Or && - And ! - Not Switch statement let a = 2 + 2 ; switch ( a ) { case 3 : alert ( 'Too small' ); break ; case 4 : alert ( 'Exactly!' ); break ; case 5 : alert ( 'Too large' ); break ; default : alert ( \"I don't know such values\" ); } Loops While while ( condition ) { } do { // loop body } while ( condition ); for loop for ( begin ; condition ; step ) { // ... loop body ... } break and continue break statement is used to break the loop and continue is use to continue to next iteration of the loop. break with label name outer : for ( let i = 0 ; i < 3 ; i ++ ) { for ( let j = 0 ; j < 3 ; j ++ ) { let input = prompt ( `Value at coords ( ${ i } , ${ j } )` , '' ); // if an empty string or canceled, then break out of both loops if ( ! input ) break outer ; // (*) // do something with the value... } } alert ( 'Done!' ); Arrays Two ways to create Arrays. let arr = new Array (); let arr = []; Common attributes and functions. - Array.length - From begining of array - shift unshift - End of array - pop push Looping on the array let fruits = [ \"Apple\" , \"Orange\" , \"Plum\" ]; // iterates over array elements for ( let fruit of fruits ) { alert ( fruit ); } // or the 'in' style let arr = [ \"Apple\" , \"Orange\" , \"Pear\" ]; for ( let key in arr ) { alert ( arr [ key ] ); // Apple, Orange, Pear } slice It returns a new array copying to it all items from index start to end (not including end). Both start and end can be negative, in that case position from array end is assumed. arr . slice ([ start ], [ end ]) concat The method arr.concat creates a new array that includes values from other arrays and additional items. arr . concat ( arg1 , arg2 ...) Iterate : forEach arr . forEach ( function ( item , index , array ) { // ... do something with item }); e.g. [ \"Bilbo\" , \"Gandalf\" , \"Nazgul\" ]. forEach (( item , index , array ) => { alert ( ` ${ item } is at index ${ index } in ${ array } ` ); }); e.g. a = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 , 100 ] a . forEach ( function ( item , index , array ){ console . log ( item * 2 )}) Searching in Array IndexOf() lastIndexOf() includes() find and ifndIndex Imagine we have an array of objects. How do we find an object with the specific condition? let users = [ { id : 1 , name : \"John\" }, { id : 2 , name : \"Pete\" }, { id : 3 , name : \"Mary\" } ]; let user = users . find ( item => item . id == 1 ); alert ( user . name ); // John filter Filter does what find does but checks for multiple objects in the array e.g. let users = [ { id : 1 , name : \"John\" }, { id : 2 , name : \"Pete\" }, { id : 3 , name : \"Mary\" } ]; // returns array of the first two users let someUsers = users . filter ( item => item . id < 3 ); alert ( someUsers . length ); // 2","title":"JS-01-Basics"},{"location":"Frontend/JS-01-Basics/#js-01-basics","text":"","title":"JS-01-Basics"},{"location":"Frontend/JS-01-Basics/#language-history-and-tools","text":"JavaScript is based on specification for ECMAScript. JS can execute in browser, server and any device which has the JavaScript engine. Various Engines V8 in Chrome & Opera SpiderMonkey \u2013 in Firefox","title":"Language History and Tools"},{"location":"Frontend/JS-01-Basics/#points-to-note","text":"Usage 2 ways Use a script tag in html. Include a JS file. < script > alert ( 'Hello, world!' ); </ script > Comments // single line /* multi line comments */ Use Strict \"use strict\" switches the engine to the modern mode. Put this as the first line of the script and its recommended to always have the strict mode enabled.","title":"Points to note"},{"location":"Frontend/JS-01-Basics/#variables-and-data-types","text":"Variables Are case sensitive. Use let and const .","title":"Variables and Data types"},{"location":"Frontend/JS-01-Basics/#numbers","text":"numbers are both integer and floating point numbers. you can use typeof(a) to check the type of a variable. let billion = 1 e9 ; // declaring large numbers with e notation. Converting to Strings let num = 255 ; alert ( num . toString ( 16 ) ); // ff Rounding Math.floor Math.ceil Math.round Math.trunc Fixed decimal places let num = 12.34 ; alert ( num . toFixed ( 1 ) ); // \"12.3\" parse from a string alert ( parseInt ( '100px' ) ); // 100 alert ( parseFloat ( '12.5em' ) ); // 12.5","title":"Numbers"},{"location":"Frontend/JS-01-Basics/#strings","text":"enclosed by single or double quotes. using backticks you can use ${<var>} to embed value in the string. You can also put expression inside this e.g. ${1 + 2} Check length using strVariale.length Access specific character using strVariable[index] Strings are immutable 'Interface'.toUpperCase() // INTERFACE Getting index of a substring staring from given pos -> str.indexOf(substr, pos) str.lastIndexOf(substr, position) includes() endsWith() startsWith() Iterating over characters for ( let char of \"Hello\" ) { alert ( char ); // H,e,l,l,o (char becomes \"H\", then \"e\", then \"l\" etc) } SubStrings str . slice ( 0 , 5 ) Conversions String to Number let str = '123' let num = Number ( str ) Boolean Boolean(1) is true Boolean(0) is false","title":"Strings"},{"location":"Frontend/JS-01-Basics/#comparison","text":"alert ( '' == false ); // true alert ( 0 === false ); // false, because the types are different","title":"Comparison"},{"location":"Frontend/JS-01-Basics/#prompt-alert-confirm","text":"prompt Its used to input value. result = prompt ( 'Message' , default ); e.g. result = prompt ( 'Enter your age' , 100 ); confirm Its used to confirm a user action. It returns true or false depending on user clicks OK or Cancel. result = confirm ( 'Do you wish to continue ?' );","title":"Prompt / Alert / Confirm"},{"location":"Frontend/JS-01-Basics/#conditionals","text":"","title":"Conditionals"},{"location":"Frontend/JS-01-Basics/#if-condition","text":"if ( condition ) { } else if ( condition ) { } else { } let accessAllowed = ( age > 18 ) ? true : false ; || - Or && - And ! - Not","title":"If condition"},{"location":"Frontend/JS-01-Basics/#switch-statement","text":"let a = 2 + 2 ; switch ( a ) { case 3 : alert ( 'Too small' ); break ; case 4 : alert ( 'Exactly!' ); break ; case 5 : alert ( 'Too large' ); break ; default : alert ( \"I don't know such values\" ); }","title":"Switch statement"},{"location":"Frontend/JS-01-Basics/#loops","text":"","title":"Loops"},{"location":"Frontend/JS-01-Basics/#while","text":"while ( condition ) { } do { // loop body } while ( condition );","title":"While"},{"location":"Frontend/JS-01-Basics/#for-loop","text":"for ( begin ; condition ; step ) { // ... loop body ... } break and continue break statement is used to break the loop and continue is use to continue to next iteration of the loop. break with label name outer : for ( let i = 0 ; i < 3 ; i ++ ) { for ( let j = 0 ; j < 3 ; j ++ ) { let input = prompt ( `Value at coords ( ${ i } , ${ j } )` , '' ); // if an empty string or canceled, then break out of both loops if ( ! input ) break outer ; // (*) // do something with the value... } } alert ( 'Done!' );","title":"for loop"},{"location":"Frontend/JS-01-Basics/#arrays","text":"Two ways to create Arrays. let arr = new Array (); let arr = []; Common attributes and functions. - Array.length - From begining of array - shift unshift - End of array - pop push Looping on the array let fruits = [ \"Apple\" , \"Orange\" , \"Plum\" ]; // iterates over array elements for ( let fruit of fruits ) { alert ( fruit ); } // or the 'in' style let arr = [ \"Apple\" , \"Orange\" , \"Pear\" ]; for ( let key in arr ) { alert ( arr [ key ] ); // Apple, Orange, Pear } slice It returns a new array copying to it all items from index start to end (not including end). Both start and end can be negative, in that case position from array end is assumed. arr . slice ([ start ], [ end ]) concat The method arr.concat creates a new array that includes values from other arrays and additional items. arr . concat ( arg1 , arg2 ...) Iterate : forEach arr . forEach ( function ( item , index , array ) { // ... do something with item }); e.g. [ \"Bilbo\" , \"Gandalf\" , \"Nazgul\" ]. forEach (( item , index , array ) => { alert ( ` ${ item } is at index ${ index } in ${ array } ` ); }); e.g. a = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 , 100 ] a . forEach ( function ( item , index , array ){ console . log ( item * 2 )}) Searching in Array IndexOf() lastIndexOf() includes() find and ifndIndex Imagine we have an array of objects. How do we find an object with the specific condition? let users = [ { id : 1 , name : \"John\" }, { id : 2 , name : \"Pete\" }, { id : 3 , name : \"Mary\" } ]; let user = users . find ( item => item . id == 1 ); alert ( user . name ); // John filter Filter does what find does but checks for multiple objects in the array e.g. let users = [ { id : 1 , name : \"John\" }, { id : 2 , name : \"Pete\" }, { id : 3 , name : \"Mary\" } ]; // returns array of the first two users let someUsers = users . filter ( item => item . id < 3 ); alert ( someUsers . length ); // 2","title":"Arrays"},{"location":"Frontend/JS-02-Functions/","text":"JS-02-Functions Declaration function < funcName > ( < arg1 > , < arg2 > = < default > , ... ){ // code return < result > } A function with an empty return or without it returns undefined primitive values are pass by value. objects and arrays are pass by reference. Naming Conventions \"get\u2026\" \u2013 return a value, \"calc\u2026\" \u2013 calculate something, \"create\u2026\" \u2013 create something, \"check\u2026\" \u2013 check something and return a boolean, etc. Function as Expression Function can be assinged as values. let sayHi = function ( name ){ alert ( \"Hello \" + name ) } // later executed. sayHi ( 'Sam' ) // also assign to another variable let func = sayHi func ( 'Peter' ) Callback functions These function expressions can be passed as arguments which are then 'called back'. The arguments showOk and showCancel of ask are called callback functions or just callbacks . function ask ( question , yes , no ) { if ( confirm ( question )) yes () else no (); } function showOk () { alert ( \"You agreed.\" ); } function showCancel () { alert ( \"You canceled the execution.\" ); } // usage: functions showOk, showCancel are passed as arguments to ask ask ( \"Do you agree?\" , showOk , showCancel ); Arrow Functions let func = (arg1, arg2, ...argN) => expression so left side of => is arguments and right side is the function code. let age = prompt ( \"What is your age?\" , 18 ); let welcome = ( age < 18 ) ? () => alert ( 'Hello' ) : () => alert ( \"Greetings!\" ); welcome (); // ok now","title":"JS-02-Functions"},{"location":"Frontend/JS-02-Functions/#js-02-functions","text":"","title":"JS-02-Functions"},{"location":"Frontend/JS-02-Functions/#declaration","text":"function < funcName > ( < arg1 > , < arg2 > = < default > , ... ){ // code return < result > } A function with an empty return or without it returns undefined primitive values are pass by value. objects and arrays are pass by reference. Naming Conventions \"get\u2026\" \u2013 return a value, \"calc\u2026\" \u2013 calculate something, \"create\u2026\" \u2013 create something, \"check\u2026\" \u2013 check something and return a boolean, etc.","title":"Declaration"},{"location":"Frontend/JS-02-Functions/#function-as-expression","text":"Function can be assinged as values. let sayHi = function ( name ){ alert ( \"Hello \" + name ) } // later executed. sayHi ( 'Sam' ) // also assign to another variable let func = sayHi func ( 'Peter' )","title":"Function as Expression"},{"location":"Frontend/JS-02-Functions/#callback-functions","text":"These function expressions can be passed as arguments which are then 'called back'. The arguments showOk and showCancel of ask are called callback functions or just callbacks . function ask ( question , yes , no ) { if ( confirm ( question )) yes () else no (); } function showOk () { alert ( \"You agreed.\" ); } function showCancel () { alert ( \"You canceled the execution.\" ); } // usage: functions showOk, showCancel are passed as arguments to ask ask ( \"Do you agree?\" , showOk , showCancel );","title":"Callback functions"},{"location":"Frontend/JS-02-Functions/#arrow-functions","text":"let func = (arg1, arg2, ...argN) => expression so left side of => is arguments and right side is the function code. let age = prompt ( \"What is your age?\" , 18 ); let welcome = ( age < 18 ) ? () => alert ( 'Hello' ) : () => alert ( \"Greetings!\" ); welcome (); // ok now","title":"Arrow Functions"},{"location":"Frontend/JS-03-Objects/","text":"JS-03-Objects Plain Objects Objects are stored and copied by reference. Equality test is true if both are same object. Objects declared as const can be changed but the variable cannot point to other object or value. To clone the object use .cloneDeep(obj) let user = new Object (); // \"object constructor\" syntax let user = {}; // \"object literal\" syntax There are 2 notations to use with objects. . (dot) [] (square brackets notation) Add/Delete user = { \"likes bird\" : true , // enclose in quotes if separated by space isManager : false , } user . name = \"Sam\" user [ \"age\" ] = 30 delete user . age Computed Properties let fruit = prompt ( \"Enter Fruit?\" , \"apple\" ); let bag = { [ fruit ] : 5 , // the name of the property is taken from the variable fruit }; short hand function makeUser ( name , age ) { return { name , // same as name: name age // same as age: age // ... }; } existance check - use keyword in let user = { age : 30 } console . log ( \"age\" in user ) // note - property in quotes loop over for ( let key in user ){ } Other objects {} objects (or what we have seen till now) are called plain object, there are other kinds of objects in JS e.g. Array, Date, Error and so on. Symbols Object properties can be - Strings - Symbols Properties of Symbols - They do not participate in for .. in loop - They are gaureenteed to be unique even if the text is the same. - They are used to create hidden properties inside an object. (Think object exhange between 2 scripts, one script wants to add something to the object without notifiying other script, they can use Symbol() because its hidden. ) let id1 = Symbol ( \"id\" ) let id2 = Symbol ( \"id\" ) let user = { fname : \"Sam\" , [ id1 ] : 1 , // when adding symbols use [] [ id2 ] : 2 , } Global Symbols In order to read (create if absent) a symbol from the registry, use Symbol.for(key) // get symbol by name let sym = Symbol . for ( \"name\" ); let sym2 = Symbol . for ( \"id\" ); // get name by symbol alert ( Symbol . keyFor ( sym ) ); // name alert ( Symbol . keyFor ( sym2 ) ); // id There exist many \u201csystem\u201d symbols that JavaScript uses internally, and we can use them to fine-tune various aspects of our objects. this this is a keyword used to refer to the current object. When we write our code using objects to represent entities, that\u2019s called object-oriented programming, in short: \u201cOOP\u201d. this is not bound The value of this is evaluated during the run-time, depending on the context. let user = { name : \"John\" }; let admin = { name : \"Admin\" }; function sayHi () { alert ( this . name ); } // use the same function in two objects user . f = sayHi ; admin . f = sayHi ; // these calls have different this // \"this\" inside the function is the object \"before the dot\" user . f (); // John (this == user) admin . f (); // Admin (this == admin) admin [ 'f' ](); // Admin (dot or square brackets access the method \u2013 doesn't matter) Constructor and new The main purpose of constructors \u2013 to implement reusable object creation code. The \u201ccapital letter first\u201d is a common agreement, to make it clear that a function is to be run with new. // Approach1 -> Constructor Function function Person ( name ){ this . name = name ; this . isAdmin = false ; } let user1 = new Person ( \"Jack\" ) // Person {name: \"Jack\", isAdmin: false} // Approach2 -> Literals. let user2 = { name : \"Jack\" , isAdmin : false , } // {name: \"Jack\", isAdmin: false} Usually, constructors do not have a return statement. Their task is to write all necessary stuff into this, and it automatically becomes the result. Technically you can call any function with new as shown below. however in this case the return statement when returns any primitive is ignored and it returns this i.e. the object which was created by new. function sum ( a , b ){ return a + b } console . log ( new sum ( 10 , 20 )) // sum {} Methods in constructor functions. A normal object can have methods as well as constructor objects can have methods as shown below. literalUser = { fname : \"John\" , lname : \"Nelson\" , getFullName : function (){ return this . fname + \" \" + this . lname } } console . log ( literalUser ) console . log ( literalUser . getFullName ()) function ConstructorUser ( fname , lname ){ this . fname = fname ; this . lname = lname ; this . getFullName = function (){ return this . fname + \" \" + this . lname ; } } let cuser = new ConstructorUser ( \"John\" , \"Nelson\" ); console . log ( cuser ); console . log ( cuser . getFullName ()); Map and Set Map is a collection of keyed data items, just like an Object. But the main difference is that Map allows keys of any type. new Map() \u2013 creates the map. map.set(key, value) \u2013 stores the value by the key. map.get(key) \u2013 returns the value by the key, undefined if key doesn\u2019t exist in map. map.has(key) \u2013 returns true if the key exists, false otherwise. map.delete(key) \u2013 removes the value by the key. map.clear() \u2013 removes everything from the map. map.size \u2013 returns the current element count. let john = { name : \"John\" }; // for every user, let's store their visits count let visitsCountMap = new Map (); // john is the key for the map visitsCountMap . set ( john , 123 ); alert ( visitsCountMap . get ( john ) ); // 123 Looping over maps For looping over a map, there are 3 methods: map.keys() \u2013 returns an iterable for keys, map.values() \u2013 returns an iterable for values, map.entries() \u2013 returns an iterable for entries [key, value], it\u2019s used by default in for..of. A Set is a special type collection \u2013 \u201cset of values\u201d (without keys), where each value may occur only once. let set = new Set (); let john = { name : \"John\" }; let pete = { name : \"Pete\" }; let mary = { name : \"Mary\" }; // visits, some users come multiple times set . add ( john ); set . add ( pete ); set . add ( mary ); set . add ( john ); set . add ( mary ); // set keeps only unique values alert ( set . size ); // 3 WeakMap The first difference from Map is that WeakMap keys must be objects, not primitive values. The main area of application for WeakMap is an additional data storage. If we\u2019re working with an object that \u201cbelongs\u201d to another code, maybe even a third-party library, and would like to store some data associated with it, that should only exist while the object is alive \u2013 then WeakMap is exactly what\u2019s needed. We put the data to a WeakMap, using the object as the key, and when the object is garbage collected, that data will automatically disappear as well. weakMap . set ( john , \"secret documents\" ); // if john dies, secret documents will be destroyed automatically Another common example is caching: when a function result should be remembered (\u201ccached\u201d), so that future calls on the same object reuse it. WeakSet WeakSet behaves similarly: It is analogous to Set, but we may only add objects to WeakSet (not primitives). An object exists in the set while it is reachable from somewhere else. Like Set, it supports add, has and delete, but not size, keys() and no iterations. Destructuring assignment Destructuring assignment is a special syntax that allows us to \u201cunpack\u201d arrays or objects into a bunch of variables, as sometimes that\u2019s more convenient. Destructuring also works great with complex functions that have a lot of parameters, default values, and so on. let arr = [ \"Ilya\" , \"Kantor\" ] let [ firstName , surname ] = arr ; Ignore elements using commas let [ firstName , , title ] = [ \"Julius\" , \"Caesar\" , \"Consul\" , \"of the Roman Republic\" ]; The Rest let [ name1 , name2 , ... rest ] = [ \"Julius\" , \"Caesar\" , \"Consul\" , \"of the Roman Republic\" ]; Working with objects let { var1 , var2 } = { var1 : \u2026 , var2 \u2026 } Example let options = { size : { width : 100 , height : 200 }, items : [ \"Cake\" , \"Donut\" ], extra : true }; // destructuring assignment split in multiple lines for clarity let { size : { // put size here width , height }, items : [ item1 , item2 ], // assign items here title = \"Menu\" // not present in the object (default value is used) } = options ; alert ( title ); // Menu alert ( width ); // 100 alert ( height ); // 200 alert ( item1 ); // Cake alert ( item2 ); // Donut","title":"JS-03-Objects"},{"location":"Frontend/JS-03-Objects/#js-03-objects","text":"","title":"JS-03-Objects"},{"location":"Frontend/JS-03-Objects/#plain-objects","text":"Objects are stored and copied by reference. Equality test is true if both are same object. Objects declared as const can be changed but the variable cannot point to other object or value. To clone the object use .cloneDeep(obj) let user = new Object (); // \"object constructor\" syntax let user = {}; // \"object literal\" syntax There are 2 notations to use with objects. . (dot) [] (square brackets notation) Add/Delete user = { \"likes bird\" : true , // enclose in quotes if separated by space isManager : false , } user . name = \"Sam\" user [ \"age\" ] = 30 delete user . age Computed Properties let fruit = prompt ( \"Enter Fruit?\" , \"apple\" ); let bag = { [ fruit ] : 5 , // the name of the property is taken from the variable fruit }; short hand function makeUser ( name , age ) { return { name , // same as name: name age // same as age: age // ... }; } existance check - use keyword in let user = { age : 30 } console . log ( \"age\" in user ) // note - property in quotes loop over for ( let key in user ){ } Other objects {} objects (or what we have seen till now) are called plain object, there are other kinds of objects in JS e.g. Array, Date, Error and so on.","title":"Plain Objects"},{"location":"Frontend/JS-03-Objects/#symbols","text":"Object properties can be - Strings - Symbols Properties of Symbols - They do not participate in for .. in loop - They are gaureenteed to be unique even if the text is the same. - They are used to create hidden properties inside an object. (Think object exhange between 2 scripts, one script wants to add something to the object without notifiying other script, they can use Symbol() because its hidden. ) let id1 = Symbol ( \"id\" ) let id2 = Symbol ( \"id\" ) let user = { fname : \"Sam\" , [ id1 ] : 1 , // when adding symbols use [] [ id2 ] : 2 , } Global Symbols In order to read (create if absent) a symbol from the registry, use Symbol.for(key) // get symbol by name let sym = Symbol . for ( \"name\" ); let sym2 = Symbol . for ( \"id\" ); // get name by symbol alert ( Symbol . keyFor ( sym ) ); // name alert ( Symbol . keyFor ( sym2 ) ); // id There exist many \u201csystem\u201d symbols that JavaScript uses internally, and we can use them to fine-tune various aspects of our objects.","title":"Symbols"},{"location":"Frontend/JS-03-Objects/#this","text":"this is a keyword used to refer to the current object. When we write our code using objects to represent entities, that\u2019s called object-oriented programming, in short: \u201cOOP\u201d. this is not bound The value of this is evaluated during the run-time, depending on the context. let user = { name : \"John\" }; let admin = { name : \"Admin\" }; function sayHi () { alert ( this . name ); } // use the same function in two objects user . f = sayHi ; admin . f = sayHi ; // these calls have different this // \"this\" inside the function is the object \"before the dot\" user . f (); // John (this == user) admin . f (); // Admin (this == admin) admin [ 'f' ](); // Admin (dot or square brackets access the method \u2013 doesn't matter)","title":"this"},{"location":"Frontend/JS-03-Objects/#constructor-and-new","text":"The main purpose of constructors \u2013 to implement reusable object creation code. The \u201ccapital letter first\u201d is a common agreement, to make it clear that a function is to be run with new. // Approach1 -> Constructor Function function Person ( name ){ this . name = name ; this . isAdmin = false ; } let user1 = new Person ( \"Jack\" ) // Person {name: \"Jack\", isAdmin: false} // Approach2 -> Literals. let user2 = { name : \"Jack\" , isAdmin : false , } // {name: \"Jack\", isAdmin: false} Usually, constructors do not have a return statement. Their task is to write all necessary stuff into this, and it automatically becomes the result. Technically you can call any function with new as shown below. however in this case the return statement when returns any primitive is ignored and it returns this i.e. the object which was created by new. function sum ( a , b ){ return a + b } console . log ( new sum ( 10 , 20 )) // sum {}","title":"Constructor and new"},{"location":"Frontend/JS-03-Objects/#methods-in-constructor-functions","text":"A normal object can have methods as well as constructor objects can have methods as shown below. literalUser = { fname : \"John\" , lname : \"Nelson\" , getFullName : function (){ return this . fname + \" \" + this . lname } } console . log ( literalUser ) console . log ( literalUser . getFullName ()) function ConstructorUser ( fname , lname ){ this . fname = fname ; this . lname = lname ; this . getFullName = function (){ return this . fname + \" \" + this . lname ; } } let cuser = new ConstructorUser ( \"John\" , \"Nelson\" ); console . log ( cuser ); console . log ( cuser . getFullName ());","title":"Methods in constructor functions."},{"location":"Frontend/JS-03-Objects/#map-and-set","text":"Map is a collection of keyed data items, just like an Object. But the main difference is that Map allows keys of any type. new Map() \u2013 creates the map. map.set(key, value) \u2013 stores the value by the key. map.get(key) \u2013 returns the value by the key, undefined if key doesn\u2019t exist in map. map.has(key) \u2013 returns true if the key exists, false otherwise. map.delete(key) \u2013 removes the value by the key. map.clear() \u2013 removes everything from the map. map.size \u2013 returns the current element count. let john = { name : \"John\" }; // for every user, let's store their visits count let visitsCountMap = new Map (); // john is the key for the map visitsCountMap . set ( john , 123 ); alert ( visitsCountMap . get ( john ) ); // 123 Looping over maps For looping over a map, there are 3 methods: map.keys() \u2013 returns an iterable for keys, map.values() \u2013 returns an iterable for values, map.entries() \u2013 returns an iterable for entries [key, value], it\u2019s used by default in for..of. A Set is a special type collection \u2013 \u201cset of values\u201d (without keys), where each value may occur only once. let set = new Set (); let john = { name : \"John\" }; let pete = { name : \"Pete\" }; let mary = { name : \"Mary\" }; // visits, some users come multiple times set . add ( john ); set . add ( pete ); set . add ( mary ); set . add ( john ); set . add ( mary ); // set keeps only unique values alert ( set . size ); // 3","title":"Map and Set"},{"location":"Frontend/JS-03-Objects/#weakmap","text":"The first difference from Map is that WeakMap keys must be objects, not primitive values. The main area of application for WeakMap is an additional data storage. If we\u2019re working with an object that \u201cbelongs\u201d to another code, maybe even a third-party library, and would like to store some data associated with it, that should only exist while the object is alive \u2013 then WeakMap is exactly what\u2019s needed. We put the data to a WeakMap, using the object as the key, and when the object is garbage collected, that data will automatically disappear as well. weakMap . set ( john , \"secret documents\" ); // if john dies, secret documents will be destroyed automatically Another common example is caching: when a function result should be remembered (\u201ccached\u201d), so that future calls on the same object reuse it.","title":"WeakMap"},{"location":"Frontend/JS-03-Objects/#weakset","text":"WeakSet behaves similarly: It is analogous to Set, but we may only add objects to WeakSet (not primitives). An object exists in the set while it is reachable from somewhere else. Like Set, it supports add, has and delete, but not size, keys() and no iterations.","title":"WeakSet"},{"location":"Frontend/JS-03-Objects/#destructuring-assignment","text":"Destructuring assignment is a special syntax that allows us to \u201cunpack\u201d arrays or objects into a bunch of variables, as sometimes that\u2019s more convenient. Destructuring also works great with complex functions that have a lot of parameters, default values, and so on. let arr = [ \"Ilya\" , \"Kantor\" ] let [ firstName , surname ] = arr ; Ignore elements using commas let [ firstName , , title ] = [ \"Julius\" , \"Caesar\" , \"Consul\" , \"of the Roman Republic\" ]; The Rest let [ name1 , name2 , ... rest ] = [ \"Julius\" , \"Caesar\" , \"Consul\" , \"of the Roman Republic\" ]; Working with objects let { var1 , var2 } = { var1 : \u2026 , var2 \u2026 } Example let options = { size : { width : 100 , height : 200 }, items : [ \"Cake\" , \"Donut\" ], extra : true }; // destructuring assignment split in multiple lines for clarity let { size : { // put size here width , height }, items : [ item1 , item2 ], // assign items here title = \"Menu\" // not present in the object (default value is used) } = options ; alert ( title ); // Menu alert ( width ); // 100 alert ( height ); // 200 alert ( item1 ); // Cake alert ( item2 ); // Donut","title":"Destructuring assignment"},{"location":"Frontend/JS-04-Arrays/","text":"JS-04-Arrays Array Functions Map The arr.map method is one of the most useful and often used. It calls the function for each element of the array and returns the array of results. a = [ 10 , 20 , 30 , 40 , 50 ] r = a . map ( function ( item ){ return item * 2 } ) console . log ( r ); Sort This does not sort numberically but converts to String and then sorts it. To sort numerically or by any means you can put in your own method like showen below. arr . sort ( ( a , b ) => a - b ); Split and Join let str = \"test\" ; alert ( str . split ( '' ) ); // t,e,s,t let arr = [ 'Bilbo' , 'Gandalf' , 'Nazgul' ]; let str = arr . join ( ';' ); // glue the array into a string using ; alert ( str ); // Bilbo;Gandalf;Nazgul Reduce and reduceRight They are used to calculate a single value based on the array. Syntax : - let value = arr . reduce ( function ( previousValue , item , index , array ) { // ... }, [ initial ]); Example : - let arr = [ 1 , 2 , 3 , 4 , 5 ]; let value = arr . reduce ( function ( p , item ){ return p + item ; } ) console . log ( value );","title":"JS-04-Arrays"},{"location":"Frontend/JS-04-Arrays/#js-04-arrays","text":"","title":"JS-04-Arrays"},{"location":"Frontend/JS-04-Arrays/#array-functions","text":"","title":"Array Functions"},{"location":"Frontend/JS-04-Arrays/#map","text":"The arr.map method is one of the most useful and often used. It calls the function for each element of the array and returns the array of results. a = [ 10 , 20 , 30 , 40 , 50 ] r = a . map ( function ( item ){ return item * 2 } ) console . log ( r );","title":"Map"},{"location":"Frontend/JS-04-Arrays/#sort","text":"This does not sort numberically but converts to String and then sorts it. To sort numerically or by any means you can put in your own method like showen below. arr . sort ( ( a , b ) => a - b );","title":"Sort"},{"location":"Frontend/JS-04-Arrays/#split-and-join","text":"let str = \"test\" ; alert ( str . split ( '' ) ); // t,e,s,t let arr = [ 'Bilbo' , 'Gandalf' , 'Nazgul' ]; let str = arr . join ( ';' ); // glue the array into a string using ; alert ( str ); // Bilbo;Gandalf;Nazgul","title":"Split and Join"},{"location":"Frontend/JS-04-Arrays/#reduce-and-reduceright","text":"They are used to calculate a single value based on the array. Syntax : - let value = arr . reduce ( function ( previousValue , item , index , array ) { // ... }, [ initial ]); Example : - let arr = [ 1 , 2 , 3 , 4 , 5 ]; let value = arr . reduce ( function ( p , item ){ return p + item ; } ) console . log ( value );","title":"Reduce and reduceRight"},{"location":"Frontend/Tailwind%20CSS/","text":"Tailwind CSS Setting up Tailwind with Parcel.js Check my gitlab project","title":"Tailwind CSS"},{"location":"Frontend/Tailwind%20CSS/#tailwind-css","text":"","title":"Tailwind CSS"},{"location":"Frontend/Tailwind%20CSS/#setting-up-tailwind-with-parceljs","text":"Check my gitlab project","title":"Setting up Tailwind with Parcel.js"},{"location":"Frontend/Web%20Technologies/","text":"Web Technologies Web Technologies to learn BackEnd - Django - DRF - Payments/Subscriptions - Notifications/Emails/Calls/Text Front End - JS - Charting - VueJS - Webpack - Parcel - TailwindCSS - SCSS - Bootstrap Deployment - Heruko vs Linode - Updates - Postgress - Security - Django env variables JS What to learn before jumping into a framework. Basics - Basic Syntax - Arrays Objects - Events - Loops, conditionals Modules - ES6 Modules - Parcel, Webpack - Export and Export Default Classes - Structuring a class - Constructors - Methods - Properties - Instantiation - Extending Classes - Inheritance Others - Arrow Functions - Promises and ASync - Http requests - Standard .then() and .catch() - Destructuring","title":"Web Technologies"},{"location":"Frontend/Web%20Technologies/#web-technologies","text":"","title":"Web Technologies"},{"location":"Frontend/Web%20Technologies/#web-technologies-to-learn","text":"BackEnd - Django - DRF - Payments/Subscriptions - Notifications/Emails/Calls/Text Front End - JS - Charting - VueJS - Webpack - Parcel - TailwindCSS - SCSS - Bootstrap Deployment - Heruko vs Linode - Updates - Postgress - Security - Django env variables","title":"Web Technologies to learn"},{"location":"Frontend/Web%20Technologies/#js","text":"What to learn before jumping into a framework. Basics - Basic Syntax - Arrays Objects - Events - Loops, conditionals Modules - ES6 Modules - Parcel, Webpack - Export and Export Default Classes - Structuring a class - Constructors - Methods - Properties - Instantiation - Extending Classes - Inheritance Others - Arrow Functions - Promises and ASync - Http requests - Standard .then() and .catch() - Destructuring","title":"JS"},{"location":"Frontend/the-circle-of-ds/","text":"The circle of Data Science.","title":"The circle of Data Science."},{"location":"Frontend/the-circle-of-ds/#the-circle-of-data-science","text":"","title":"The circle of Data Science."},{"location":"Java/1.%20Overview%20of%20the%20JVM%20World/","text":"Java Platform Java now belongs to Oracle. Java Platform is made up of: - Java Programming Language Java Runtime Env (JVM) Standard Library All of the above are combined in the JDK Java code compiles to application bytecode. Java Bytecode can then be translated to machine code by JVM or the Java runtime envirounment. Steps in compliing a Java File Create a java file using the .java extention. The source code is compiled to bytecode using javac . This then creates a file of extention .class Then you can use the java command which starts up the JVM and executes the .class file on the JVM. Adopting Java Philosophy Portability WORA - Write once and Run Anywhere JVM is designed for each operating system and architecture, so you just have to swap the JVM on different machine. The same byte code can then run on different machines. Conservative New features are considered but Java is conservative. Backward Compatibility Huge focus on backward compatability. Java is open Java Community Process (JCP) have say in how it develops. Oracle is not the only Java implementation. IBM, Eclipse have their own Java implementations. Open JDK file:openjdk.java.net Why adopt Java Web Applications Backend applications Suited for large appliaitons Wealth of libraries4 Strongly typed language e.g. organizing the java code Classes > Packages > Modules Productivity : Managed Rutime By using JVM you get these benefits automatically which you may not get in lower level languages. Automatic memory management Garbage collection Multi-threading When not to use Java Real time systems (self driving cars) when you need low level access to system e.g. device drivers Quick prototyping For developers who want cutting-edge languages C# vs Java C# also now open source Its a more faster moving lanaguge As its only recently cross-platform its lacking in tools and libraries which target across platform Still dominated by Microsoft Java vs Python - Like Java its a high-level managed language - Like Java its open ecosystem - Its interpreted language thus a bit slower - Its not statically typed JavaScript vs Java - With NodeJS JS can now come out of browser and became and managed runtime. - Interpreted lanaguge and statically typed - Single-threaded Types of applications in Java Desktop Java Here we are talking about GUI applications AWT - GUI toolkit (less used now its old) Swing (Better option than AWT) MVC based JavaFX (Most recent and modern looking) Offers advanced components Can use CSS 3D Graphics Enerprise Java or Java EE Enterprise have complex needs like batch jobs, monitoring, database interactions etc... With Java SE you will have to use 3rd party tools for a lot of this funcitonality. Java EE is a set of API\\'s which solve common enterprise applications. e.g. Data persistance Web applications Security Messaging XML/JSON For Java EE you would need Java Application Server which runs on top of JVM. e.g. - Java Persistence Architecture - Enterprise Java Beans - Java Server Faces - ... and others There are multiple Java application servers available Wildfly (Red Hat) WebShpere (IBM) WebLogic (Oracle) Tomcat (Apache) Note : - Java EE will become Jakarta EE Java for the cloud Java EE applications are almost always monolithic in nature. When developing for cloud you should go for microservices in the cloud. Popular microframework in Sprint Boot which is built on Spring framework and other Netflix libraries . (Java SE and not EE) Other microframework MicroProfile (Java EE) Vert.x (Open source by Redhat) Play Framework as of now Spring Boot is the king. Java on Mobile Java ! Andriod= Very different skillset. Popular Java Libraries Spring Framework Main notion is Dependency Injection . It servers as Depencency Injection Container . It also brings other technologies to the table. It decopules the helper and utility classes from the main application class because the application class does not instanciate the other classes. Thus decoupling is achieved. Spring WebFlux Other libraries Utility Libraries Google Guava --> collections, Cachching, IO helpers Apache Commons --> Extend core java func, collections, csv, IO (Broader than google guava) Apache Log4J --> Application Logging Distributed Systems Netty - High performance but low level (Networking) Akka - (High level) concurrency, clustering and distribution RxJava - Reactive programming, Async and event-based application Apache Camel - Enterprise application integration Database JDBC (Too low level) MySQL Postgres Oracle H2 ORM\\'s Hibernate EclipseLink SQL DSL\\'s jOOQ QueryDSL Data processing (Big Data) Apache Hadoop Apache Spark (More scalable) DL4J - Deep learning for Java Stuff written in Java Cassandra (No SQL database) Neo4J - Graph Database in Java ElasticSearch Hadoop Distributed File System Practices and Common Tools Code IDEs Eclipse (Open Source) IntelliJ Build (Runnable application) Uses They manage multiple modules Manage dependency Tools Maven (Most used) Gradle Uses Groovy scripts Supports incremental scripts Test JUnit Mockito (Supports mocking) Static Analysis Checkstyle Spotbugs PMD SobarQube : Continours Inspection Maven central is like pypi for java. Continuous Integration Server Write Code Push to CI server It builds code Runs tests Analyse code Also can deploy automatically and out comes a working application E.g. Jenkins (Open Source) Alternative JVM languages The JVM can run btye code but it does not matter where this byte code comes from e.g. it can come from a java file or any other file. E.g. Scala, Kotlin can compile to Java byte code and can run on JVM. Why choose alternative JVM languages Productivity Familiarity Different paradigms e.g. Functional programming Goovy Dynamic scripting language. Interpreted or compilied Opt-in type system Scala Combines OO and Functional Programming Compiled Extensive type system Akka, Spart written in Scala Kotlin Designed as a better java by WebStorm seamless java interop also andriod dev can be done in Java also compiles to JS and can run in browser JRE vs JDK JRE - Java runtime env End users will install it JDK - Java development Kit Developers will install it. It contains the JRE Java code will compile to platform independent bytecode which can be run on specific JRE for that platform.","title":"1. Overview of the JVM World"},{"location":"Java/1.%20Overview%20of%20the%20JVM%20World/#java-platform","text":"Java now belongs to Oracle. Java Platform is made up of: - Java Programming Language Java Runtime Env (JVM) Standard Library All of the above are combined in the JDK Java code compiles to application bytecode. Java Bytecode can then be translated to machine code by JVM or the Java runtime envirounment. Steps in compliing a Java File Create a java file using the .java extention. The source code is compiled to bytecode using javac . This then creates a file of extention .class Then you can use the java command which starts up the JVM and executes the .class file on the JVM.","title":"Java Platform"},{"location":"Java/1.%20Overview%20of%20the%20JVM%20World/#adopting-java","text":"Philosophy Portability WORA - Write once and Run Anywhere JVM is designed for each operating system and architecture, so you just have to swap the JVM on different machine. The same byte code can then run on different machines. Conservative New features are considered but Java is conservative. Backward Compatibility Huge focus on backward compatability. Java is open Java Community Process (JCP) have say in how it develops. Oracle is not the only Java implementation. IBM, Eclipse have their own Java implementations. Open JDK file:openjdk.java.net Why adopt Java Web Applications Backend applications Suited for large appliaitons Wealth of libraries4 Strongly typed language e.g. organizing the java code Classes > Packages > Modules Productivity : Managed Rutime By using JVM you get these benefits automatically which you may not get in lower level languages. Automatic memory management Garbage collection Multi-threading When not to use Java Real time systems (self driving cars) when you need low level access to system e.g. device drivers Quick prototyping For developers who want cutting-edge languages C# vs Java C# also now open source Its a more faster moving lanaguge As its only recently cross-platform its lacking in tools and libraries which target across platform Still dominated by Microsoft Java vs Python - Like Java its a high-level managed language - Like Java its open ecosystem - Its interpreted language thus a bit slower - Its not statically typed JavaScript vs Java - With NodeJS JS can now come out of browser and became and managed runtime. - Interpreted lanaguge and statically typed - Single-threaded","title":"Adopting Java"},{"location":"Java/1.%20Overview%20of%20the%20JVM%20World/#types-of-applications-in-java","text":"Desktop Java Here we are talking about GUI applications AWT - GUI toolkit (less used now its old) Swing (Better option than AWT) MVC based JavaFX (Most recent and modern looking) Offers advanced components Can use CSS 3D Graphics Enerprise Java or Java EE Enterprise have complex needs like batch jobs, monitoring, database interactions etc... With Java SE you will have to use 3rd party tools for a lot of this funcitonality. Java EE is a set of API\\'s which solve common enterprise applications. e.g. Data persistance Web applications Security Messaging XML/JSON For Java EE you would need Java Application Server which runs on top of JVM. e.g. - Java Persistence Architecture - Enterprise Java Beans - Java Server Faces - ... and others There are multiple Java application servers available Wildfly (Red Hat) WebShpere (IBM) WebLogic (Oracle) Tomcat (Apache) Note : - Java EE will become Jakarta EE Java for the cloud Java EE applications are almost always monolithic in nature. When developing for cloud you should go for microservices in the cloud. Popular microframework in Sprint Boot which is built on Spring framework and other Netflix libraries . (Java SE and not EE) Other microframework MicroProfile (Java EE) Vert.x (Open source by Redhat) Play Framework as of now Spring Boot is the king. Java on Mobile Java ! Andriod= Very different skillset.","title":"Types of applications in Java"},{"location":"Java/1.%20Overview%20of%20the%20JVM%20World/#popular-java-libraries","text":"","title":"Popular Java Libraries"},{"location":"Java/1.%20Overview%20of%20the%20JVM%20World/#spring-framework","text":"Main notion is Dependency Injection . It servers as Depencency Injection Container . It also brings other technologies to the table. It decopules the helper and utility classes from the main application class because the application class does not instanciate the other classes. Thus decoupling is achieved. Spring WebFlux","title":"Spring Framework"},{"location":"Java/1.%20Overview%20of%20the%20JVM%20World/#other-libraries","text":"Utility Libraries Google Guava --> collections, Cachching, IO helpers Apache Commons --> Extend core java func, collections, csv, IO (Broader than google guava) Apache Log4J --> Application Logging Distributed Systems Netty - High performance but low level (Networking) Akka - (High level) concurrency, clustering and distribution RxJava - Reactive programming, Async and event-based application Apache Camel - Enterprise application integration Database JDBC (Too low level) MySQL Postgres Oracle H2 ORM\\'s Hibernate EclipseLink SQL DSL\\'s jOOQ QueryDSL Data processing (Big Data) Apache Hadoop Apache Spark (More scalable) DL4J - Deep learning for Java Stuff written in Java Cassandra (No SQL database) Neo4J - Graph Database in Java ElasticSearch Hadoop Distributed File System","title":"Other libraries"},{"location":"Java/1.%20Overview%20of%20the%20JVM%20World/#practices-and-common-tools","text":"Code IDEs Eclipse (Open Source) IntelliJ Build (Runnable application) Uses They manage multiple modules Manage dependency Tools Maven (Most used) Gradle Uses Groovy scripts Supports incremental scripts Test JUnit Mockito (Supports mocking) Static Analysis Checkstyle Spotbugs PMD SobarQube : Continours Inspection Maven central is like pypi for java. Continuous Integration Server Write Code Push to CI server It builds code Runs tests Analyse code Also can deploy automatically and out comes a working application E.g. Jenkins (Open Source)","title":"Practices and Common Tools"},{"location":"Java/1.%20Overview%20of%20the%20JVM%20World/#alternative-jvm-languages","text":"The JVM can run btye code but it does not matter where this byte code comes from e.g. it can come from a java file or any other file. E.g. Scala, Kotlin can compile to Java byte code and can run on JVM. Why choose alternative JVM languages Productivity Familiarity Different paradigms e.g. Functional programming Goovy Dynamic scripting language. Interpreted or compilied Opt-in type system Scala Combines OO and Functional Programming Compiled Extensive type system Akka, Spart written in Scala Kotlin Designed as a better java by WebStorm seamless java interop also andriod dev can be done in Java also compiles to JS and can run in browser","title":"Alternative JVM languages"},{"location":"Java/1.%20Overview%20of%20the%20JVM%20World/#jre-vs-jdk","text":"JRE - Java runtime env End users will install it JDK - Java development Kit Developers will install it. It contains the JRE Java code will compile to platform independent bytecode which can be run on specific JRE for that platform.","title":"JRE vs JDK"},{"location":"Java/2.%20Core%20Java/","text":"Basics Comments // Line Comments /* Block Comments */ /** Java Doc Comments */ Introducing Packages Packages provide organization Naming conventions Packages follow standard naming convention all lower case (Use reversed domain name) e.g. packages created by rajatsethi.ca will be package ca.rajatsethi packages created by pluralsight.com will be package com.pluralsight You can then further subdivide by group within a company etc... e.g. =package com.pluralsight.projecta;= For larger organization you can further divide into package com.pluralsight.accounting.projecta; and package com.pluralsight.it.projecta; Affect source code file structure Each . will create a subfolder inside a src directory. All members become part of that package. In the example given below the Main class becomes part of the package com.company and becomes com.company.Main package com.company ; public class Main { public static void main ( String [] args ) { // write your code here } } Primitive Data Types By convention we follow \\\"Camel Casing\\\" in java. First letter is lower case Start of each word is upper case rest all lower case e.g. bankAccountBalance, levelTwoTraining // Primitive Data Types // Int byte short int long // Float float double // Character char regularU = '\\u00DA' ; // Boolean boolean x = true ; Java primitive types are stored by value. int x = 10 ; int y = x ; // The is distinct sepereate memory location for y Operators Basic Math operators + - / * % Postfix and prefix ++ -- Compound assignment operators + -= *= %= /== Operator Precedence Postfix x++ x-- Prefix ++x --x Multiplicative * / % Additive + - Type conversions There are 2 types of type conversions - implicit - explicit // Implicit conversion int iVal = 50 ; long lVal = iVal ; // Java is doing int to long implicit conversion // Explicit conversion long lVal = 50 ; int iVal = ( int ) lVal ; Implict conversions Usually the widening conversion are implicit i.e. automatic e.g. int to long Mixed - Java will use largest integer in equation. Mixed int and float - Will cast to largest floating point in the equation Explicit conversions Both widening and narrowing Float to int --> fraction will be dropped Int to float --> can loose precesion // Example short shortVal = 10 ; long longVal = 10 ; // This will give you a error because implicit cannot be narrowing short result = shortVal - longVal ; So do explicit type cast. short shortVal = 10 ; long longVal = 10 ; short result = ( short ) ( shortVal - longVal ); Conditional Logic Relational Operators > < > \\<= == !== Conditional Assignment // result = condition ? true_val : false_val int v1 = 7 ; int v2 = 5 ; int result = v1 > v2 ? v1 : v2 ; If Else if Else Nested If Block Statement A variable declared with the block statement is only visible inside the block statement. Where the variable is visible is its scope . Locical Operators AND & Or | Exclusive Or (XOR) ^ Negation ! Conditional Logical Operators These only execute the right-side if needed to determine the result. Conditional AND && Conditional OR || & vs && & int rooms = 0 ; int students = 150 ; if ( rooms > 0 & students / rooms > 30 ){ // This will give divide by 0 error. System . out . println ( \"Crowded\" ); } && int rooms = 0 ; int students = 150 ; if ( rooms > 0 && students / rooms > 30 ){ // This will not give divide by 0 error because it evaluates right side only when left side is true. System . out . println ( \"Crowded\" ); } While Loop while ( condition ){ } While Loop do { } while ( condition ) For Loop for ( initialize ; condition ; update ){ } Arrays Provide an ordered collection of elements of same type. float [] theVals = new float [ 3 ] ; // Array for ( int i = 0 ; i < theVals . length ; i ++ ){ } float [] theVals = { 10.0f , 20.0f , 30.0f }; // alternate way of declaring array For Each For each loop executes the code once for each memeber of the array. It automatically handles getting the collection length and accessing each value. float [] theVals = { 10.0 , 20.0 , 30.0 }; float sum = 0.0 ; for ( float currentVal : theVals ){ sum += currentVal ; } System . out . println ( sum ); Switch Only primitive supported with switch are char and int switch ( test - value ){ case value - 1 : statements case value - 2 : statements default : statements } Example of switch. Note always put a break at end of ** int iVal = 10 ; switch ( iVal % 2 ){ case 0 : System . out . print ( iVal ); System . out . println ( \" is even\" ); break ; case 1 : System . out . print ( iVal ); System . out . println ( \" is odd\" ); break ; default : System . out . println ( \"Oops it broke\" ); break ; } Object Oriented Java Java is object oriented language Objects encapsulate data, operations and usage semantics Allow storage and manipulation detail to be hidden When creating classes the source file name is same as the name of the class. (For public class its mandatory) Classes are reference types. When we create 2 objects of the same class and we say object2 = object1 , it means that object1 and object2 point to the same memory address. Encapsulation and Access Modifiers This concept is also often used to hide the internal representation, or state, of an object from the outside. No access modifier --> Only within own package public - Everywhere private - Only within its own class Naming Classes Follow \\\"Pascal Case\\\" All first char in words are capital e.g. =BankAccount= Use simple, descriptive names Methods void no return value A method can return a single value a primitive value a reference to an object a reference to an arary (array are objects) The below example demonstrates different return types. // Class Flight public class Flight { private int passengers ; private int seats ; // Constructors and other methods public boolean hasRoom ( Flight f2 ){ int total = passengers + f2 . passengers ; return total <= seats ; } public Flight createNewWithBoth ( Flight f2 ){ // returns a new object of the class. Flight newFlight = new Flight (); newFlight . seats = seats ; newFlight . passengers = passengers + f2 . passengers ; return newFlight ; } } // Main Function Flight lax1 = new Flight (); Flight lax2 = new Flight (); // add passengers to both flights Flight lax3 ; if ( lax1 . hasRoom ( lax2 )){ lax3 = lax1 . createNewWithBoth ( lax2 ); } Special References this - implicit reference to the current object. useful for reducing ambiguity allows an object to pass itself as a parameter public Flight createNewWithBoth ( Flight f2 ){ // returns a new object of the class. Flight newFlight = new Flight (); newFlight . seats = seats ; newFlight . passengers = this . passengers + f2 . passengers ; return newFlight ; } null - is a reference literal represents an uncreated object can be assigned to any reference variable // Main Function ... Flight lax3 = null ; // uncreated object is assigned null. ... Field Encapsulation We use getters and setters instead of exposing the fields of the class. Establishing Initial State of the fields There are 3 ways to do this Field Initial State Constructor Initialization Blocks 1. Field Initial State The variables have to be initialized before you can use them. e.g. the below will give you an error. public static void main ( String [] args ) { int x ; System . out . print ( x ); // Error - x is not initialized } However fields i.e. class variables receive \\\"zero\\\" value by default. int --> defaults to 0 float --> defualts to 0.0 char --> defaults to \\u000 boolean --> defualts to false reference types --> defaults to null or you can initialize them yourself public class Earth { long circum = 24901 ; // initializing manually } 2. Constructors Constructor has no return type. Every class has at least 1 constructor. If there are no explict constructor Java provides one in the background. A class can have multiple constructors with different parameter list 2.1 Chaining Constructors You can call another constructor from within an other constructor. (This is called constructor chaining). You can do that by using this() Call to other constructor must be the first line of the current constructor. e.g. In this class we have 4 constructors. Not all the constructors need to be public . public class Passenger { public Passenger (){} public Passenger ( int freeBags ){ this ( freeBags > 1 25.0 : 50.0 ); this . freeBags = freeBags } public Passenger ( int freeBags , int checkedBags ){ this ( freeBags ); this . checkedBags = checkedBags ; } private Passenger ( double perBagFee ){ this . perBagFee = perBagFee ; } } // Main Passenger jane = new Passenger ( 2 , 3 ); public Passenger(int freeBags, int checkedBags) then this(freeBags) is called from the above constructor Which calls public Passenger(int freeBags) Which in turn calls the private Passenger(double perBagFee) for setting the perBagFee ... 3. Initialization Blocks Initialization blocks are share across all constructors Executed as if the code was placed at start of each constructor. There can be multiple initialization blocks and they are executed in top down fashion public class Flight { private int seats ; { // Start of initialization block } // end of initialization block public Flight (){ } } Order of Execution Java follows the below mentioned order for field initialization and constructor. Field Initialization (Field initial state) Initialization Block Constructor Overloading A class can have the same method name multiple times. Signature needs to be different e.g. Number of parameters Type of each parameter Any number of parameters A method can be declared to accept variable number of parameters. - Place ... after parameter type - It can be done only for the last parameter public class Flight { public void addPassenger ( Passenger ... list ){ // same as *args in Python for ( Passenger p : list ){ // Code here } } } Inheritance Use extends keyword. public class CargoFlight extends Flight { } One not commonly known phoenomenon is objects of the derived class can be created using the Base type e.g. Flight f = new CargoFlight (); now in f we can use Flight class methods and capabilities but not CargoFlight class capabilities. This is uselful in grouping the objects. Object Class The object class is the root of the java class hierarchy. So we can reference the object class as well. Object [] stuff = new Object [ 3 ] ; stuff [ 0 ] = new Flight (); stuff [ 1 ] = new Passenger ( 2 , 4 ); stuff [ 2 ] = new CargoFlight (); Another example Object o = new Passenger (); o = new CargoFlight (); // as of yet o will only be able to access functionality of 'Object' class in Java // When you need o to access functionality of CargoFlight you need to do the following CargoFlight cf = ( CargoFlight ) o ; // Typecast o to CargoFlight and cf will point to save memory address of o but will be able to access methods of CargoFlight class. cf . addPassenger (); Methods of Object class clone hashCode getClass finalize toString equals Equality == For reference types it checks if the objects point to the same instance. You can override the default equals implementations. class Flight { private int flightNumber ; private int flightClass ; @Override public boolean equals ( Object o ){ // As we are getting object o and we need data from the Flight class we need to cast it. if ( ! ( o instanceof Flight ){) return false ; Flight other = ( Flight ) o ; return this . flightNumber == other . flightNumber ; } } Super Super treats the object as if it was an instance of its base class useful for accessing base class members that have been overridden class Flight extends object { @Override public boolean equals ( Object o ) { super . equals ( o ); // Calling the super class method } } Final and Abstract By default all classes can be extended Creating a final class. A final class cannot be extended or inherited from. public final class Passenger { } You can also make a particular method as final and not the whole class thus that method cannot be overriden. public class CargoFlight { public void methodA (){} public final void methodB (){} } Abstract Abstract will require that the class will be inherited or a method will be overriden. If any method in a class is abstract you need to mark the whole class as abstract. public abstract class Pilot { public void methodA (){ } public abstract boolean canAccept ( Flight f ); // this is an abstract method. } Abstract class cannot be instanciated. The below code will give you an error. abstract class Pilot { Pilot (){} } public class Main { public static void main ( String [] args ) { Pilot p1 = new Pilot (); // Trying to instanciate a abstract class. } } Inheritance and Constructor Constructors are not inherited A base class constructor must always be called. If you do not do this explicitly, Java will call the base class no argument constructor automatically. If you call manually make sure you call the base class constructor in the first line of the current class constructor. // TODO example (added a note on the video) Interfaces What is interface? Do not provide implementation. Only the contract is provided. Although in new versions of Java this has changed --> a default method can have implementation as well. You can also add fields to an interface in new versions of Java. Although a lot of java community was not happy about the above and this point. e.g. an existing interface is java.lang.Comparable . It says that we should implement a compareTo method. This method returns negetive value (we should come before), positive value (we should come after) and 0 (if equal). Now lets use this in our flight class. public class Passenger implements Comparable { private int memberLevel ; private int memberDays ; public int compareTo ( object o ){ Passenger p = ( passenger ) o ; if ( this . memberLevel > p . memberLevel ) return - 1 ; else if ( this . memberLevel < p . memberLvel ) return 1 ; return 0 ; } } Because our class conforms to the Comparable interface we can create a array of passenger objecs and sort it. Passenger [] passengers = { bob , jane , steve , lisa }; Arrays . sort ( passengers ); // because the Passenger class conforms to the Comparable interface. Some interfaces require another types generics . e.g. public interface Comparable < T > { } So you can specify the <T> type as shown below. Then you can specify which object will be compared thus getting rid of the type casting from object to Flight or Passenger object. public class Flight implements Comparable < Flight > { private int flightTime ; public int compareTo ( Flight f ){ // notice the type parameter return this . flightTime - f . flightTime ; } } Classes are free to implement multiple interfaces but can only extend 1 class. e.g. public class Flight implements Comparable < flight > , Iterable < Person > { } Another comparable example /* Comparable Interface. */ import java.util.ArrayList ; import java.util.Collection ; import java.util.Collections ; import java.util.List ; class Employee implements Comparable < Employee > { int id ; String name ; public Employee ( int id , String name ) { this . id = id ; this . name = name ; } @Override public int compareTo ( Employee o ) { return id - o . id ; } @Override public String toString () { return \"Employee{\" + \"id=\" + id + \", name='\" + name + '\\'' + '}' ; } } public class Main13 { public static void main ( String [] args ) { List < Employee > employees = new ArrayList <> (); employees . add ( new Employee ( 109 , \"sam nelson\" )); employees . add ( new Employee ( 10 , \"pete nelson\" )); employees . add ( new Employee ( 11 , \"becky nelson\" )); System . out . println ( employees . get ( 0 ). compareTo ( employees . get ( 1 ))); Collections . sort ( employees ); // Sorting employees array. for ( Employee employee : employees ){ System . out . println ( employee ); } } } Declaring an Interface Can have methods but no implementations. Can have constants. Can extend other interfaces. Static Static Members Static members are shared class wide. Declared using the static keyword. Can be accessed using the class name. You can also access them directly without putting class name in front of them if you did a static import. e.g. public class Flight { static int allPassengers ; public static void resetAllPassengers (){ allPassengers = 0 ; } } import static com.company.travel.Flight.resetAllPassengers ; // static import public class main { public static main ( String args [] ){ resetAllPassengers (); // do not need to do Flight.resetAllPassengers(); } } Static initialization blocks One time Execute automatically before the static type is used. Must handle all checked exceptions, no concept of throws keyword. public class Flight { static { // static initilization block } } Nested types Classes within classes and interfaces Interface within classes and interfaces The nested type is treated as a type of the enclosing type and has access to its private members Why use them Structure and scoping (Static inner class) - ** Inner classes (each instance of nested class is associated with the instance of enclosing class) (non static inner class) Inner Class Example of inner class provided below. In this example the inner class is creating an Iterator check CustomIterator below. import java.util.Iterator ; class GenericList implements Iterable < Integer > { int [] mylist = new int [ 10 ] ; int position = 0 ; void push ( int value ){ mylist [ position ++] = value ; } int pop (){ return mylist [-- position ] ; } @Override public Iterator < Integer > iterator () { return new CustomIterator ( this ); } public class CustomIterator implements Iterator < Integer > { GenericList list ; private int pointer = 0 ; CustomIterator ( GenericList list ){ this . list = list ; } @Override public boolean hasNext () { return pointer < list . position ; } @Override public Integer next () { return list . mylist [ pointer ++] ; } } } public class Main10 { public static void main ( String [] args ) { GenericList lst = new GenericList (); lst . push ( 10 ); lst . push ( 20 ); // System.out.println(lst.pop()); for ( int value : lst ){ System . out . println ( value ); } } } Anonymous Classes - Useful for extend interface/classes to solve a particular problem which is not code wide. --> Check lambda's as well. More Data Types Strings Java string has UTF-16 encoding Use double quotes \\\"\\\" contatenate using + String objects are immutable. (They cannot be changed but new value can be assigned to them). This can be inefficient Methods length valueOf - convert non string value to a string concat replace toLowerCase trim split format chatAt substring contains startsWith equals equalsIgnoreCase ... Documentation on String Methods String Equality Check out apache commons library for string operations. It takes care of the null pointer exception in most cases. String s1 = \"I Love\" ; s1 += \"Java\" ; String s2 = \"I\" ; s1 += \" Love Java\" ; if ( s1 == s2 ) {} // Its False The above example returns false because they are not the exact same instance of the String even though they have the same value in them. if ( s1 . equals ( s2 )){} // This will return True .equals does a char by char comparison. As the char based comparisons are quiet expensive we use the intern method to compare strings. intern method returns a cannonical form of the string based on its value. s1 . intern () == s2 . intern (); // Will return true intern does have its overhead so use it only if you are doing comparisons over and over again. So lets say you have multiple strings which are master data and you want to search (multiple times) a new string exists in this array of strings or not. In this case turn the array of strings into intern and comapre using the == operator which will be inexpensive. Convert to String int iVal = 100 ; String sVal = String . valueOf ( iVal ); // sVal = \"100\" Remember that object class provides some standard methods that all classes will have. Such a method is toString which is used to get the string representation of various objects e.g. an object of a class. public class Flight { int flightNumber ; @Override public String toString (){ return \"I fly to\" + flightNumber ; } } String Builder Remember strings are immutable but sometimes we wish to manipulate them. StringBuilder provides mutable string buffer General recomendation --> pre-size buffer StringBuilder sb = new StringBuilder ( 40 ); // Sample methods // append // insert sb . append ( \"I flew to \" ); sb . append ( \"Florida\" ); sb . insert ( 4 , \"at\" ); // convert back to String String message = sb . toString (); Classes vs Primitives !!! note \\\"\\\" You may not use this very often. Classes Provide convenience incurs an overhead Primitives Provide efficiency So we sometimes we use Primitive Wrapper Class . The standard class hierarchy for primitive type looks like this. Classes shown below: - Object Boolean Number Byte Short Integer Long Float Double Character All primitive wrapper class instances are immutable. When you create an int variable it is an instance/object of the Integer class shown above. The conversion is done automatically. Java also provides methods for explicit conversions. Primitive to wrapper -> valueOf . This is known as boxing. Wrapper to primitive > =xxxValue . This is known as unboxing. String to primitive -> parseXxx String to wrapper -> valueOf Using this you can treat the primitive type as an object. Example 1 - Treat as object Object [] stuff = new Object [ 3 ] ; stuff [ 0 ] = new Flight (); stuff [ 1 ] = new Passenger ( 0 , 2 ); stuff [ 2 ] = 100 ; Example 2 - Null References public class Flight { Integer flightNumber ; // note we are not creating int but Integer which creates it as an object Character flightClass ; // same as above @Override public String toString (){ if ( flightNumber != null ){ // we can now compare int to null because its an object. Else as soon as you create an object of flight class the int will get value of 0 and if there is any flight number with value 0 the comparison will not work e.g. flightNumber != 0 is leaving 1 case out where as using int as Integer i.e. as object we can also cover the case of 0. } else if ( flightClass != null ){ } } } Sample documentation for Interger Class Refer the same for other primitive wrapper classes. Wrapper Class Equality Did not read a lot on this becase seemed I may never use it. (Check slides) Final Fields Final Static - Cannot be set by an object instance. public class Flight { static final int MAX_FAA_SEATS = 500 ; } Enumeration types Its useful for defining a type with a finite list of valid values. Declare with keyword enum and provide a comma separated value list of types. public enum FlightCrewJob { Pilot , CoPilot , FlightAttendant } public class CrewMember { private FlightCrewJob job ; } // How to create CrewMember judy = newCrewMember ( FlightCrewJob . CoPilot ); Exceptions There are in total 3 types of exceptions - Error - Checked - Detected by compiler. - Unchecked - Happening at runtime. try/catch/finally Try block will run the normal code Catch block is called only if matching exception is thrown runs in all cases when try block or catch block finishes. It usually contains clean up code. A single try can have multiple catch public class test { public static void main ( String [] args ) { int i = 1 ; int j = 0 ; try { System . out . println ( i / j ); } catch ( Exception e ){ System . out . println ( \"Error : \" + e . getMessage ()); System . out . println ( e . getStackTrace ()); } finally { System . out . println ( \"Program continues...\" ); } } } Example below : Reading file in JAVA using exception handling. import java.io.BufferedReader ; import java.io.FileReader ; public class test { public static void main ( String [] args ) { BufferedReader reader = null ; int total = 0 ; try { // try reading the file reader = new BufferedReader ( new FileReader ( \"C:\\\\a.txt\" )); String line = null ; while (( line = reader . readLine ()) != null ){ // read line by line System . out . println ( line ); total += Integer . valueOf ( line ); System . out . println ( \"Total = \" + total ); } } catch ( Exception e ){ // print any exception when reading the file System . out . println ( e . getMessage ()); } finally { // here we close the file using nested try and catch try { if ( reader != null ) reader . close (); } catch ( Exception e ){ System . out . println ( e . getMessage ()); } } } } Exception Class Hierarchy (Check slide) Object Throwable Error Linkage Error ... Exception -Runtime Exception - Null Pointer Exception ... Some of the excpetions are checked exceptions and some are unchecked exceptions. Its mandatory to handle Checked Exceptions Exception are handled by Type Each type of exception can have a seperate catch block. each catch is tested from top to bottom first assignable catch is selected You should start with Specific exceptions at the top and then get general as you go down. Exception and methods In some cases a method does a processing of say a file open. But the file name is being passed by another method. The method which is opening the file will get the exception if the file name is not correct but it should be the method which sends the file name which should be made aware of this exception so we can use throws Exception on the method which is opening the file and this will propogate up the call stack. public class Flight { public void addPassengers ( String filename ) throws IOException { // ... try { // open file } finally { // close file } } } As you see in the above example the method which is receiving the file name is not catching the exception its just throws IOException to the caller method. The throws clause of an overriding method must be compatable with the throws clause of the overriden method. Throwing Exception Exceptions are objects, they have to be created before they are thrown. Put meaning full information in it. When caused by another exception, include orignal originating exception by using initCause method. You can also create your own excpetion types and throw them however in most of the cases you will use the existing exception types. Inherit from Exception class. Make them checked excpetions. Constructors are often their only members Packages A package is a group of related types It creates a namespace, useful in naming collisions. Usually use reverse domain naming. It provides access boundaries It acts as a unit of distribution e.g. package com.examplesite.travel ; public class Flight { } to use this you can fully qualify the type like we do below com . examplesite . travel . Flight lax178 = ...; Whenever you are working in package you don\\'t have to fully qualify standard ones need not be fully qualify for others use type imports Type imports we do this using import statement. e.g. import com.pluralsight.travel.Flight ; import com.xyzcompany.bar ; Flight = ; Wine = ; Package can serve as an access boundary. No access modifier is by default a package private. Others are public, private and protected. Jar Files You can distribute your entire package or library of code using single jar file which maintains the folder structure in that file. In addition to that it can also contains a manifest file which provides information about that Jar file. Its usually in named value format. You can find more information about it here You can create - Normal Jar - Runnable Jar","title":"2. Core Java"},{"location":"Java/2.%20Core%20Java/#basics","text":"Comments // Line Comments /* Block Comments */ /** Java Doc Comments */ Introducing Packages Packages provide organization Naming conventions Packages follow standard naming convention all lower case (Use reversed domain name) e.g. packages created by rajatsethi.ca will be package ca.rajatsethi packages created by pluralsight.com will be package com.pluralsight You can then further subdivide by group within a company etc... e.g. =package com.pluralsight.projecta;= For larger organization you can further divide into package com.pluralsight.accounting.projecta; and package com.pluralsight.it.projecta; Affect source code file structure Each . will create a subfolder inside a src directory. All members become part of that package. In the example given below the Main class becomes part of the package com.company and becomes com.company.Main package com.company ; public class Main { public static void main ( String [] args ) { // write your code here } } Primitive Data Types By convention we follow \\\"Camel Casing\\\" in java. First letter is lower case Start of each word is upper case rest all lower case e.g. bankAccountBalance, levelTwoTraining // Primitive Data Types // Int byte short int long // Float float double // Character char regularU = '\\u00DA' ; // Boolean boolean x = true ; Java primitive types are stored by value. int x = 10 ; int y = x ; // The is distinct sepereate memory location for y Operators Basic Math operators + - / * % Postfix and prefix ++ -- Compound assignment operators + -= *= %= /== Operator Precedence Postfix x++ x-- Prefix ++x --x Multiplicative * / % Additive + - Type conversions There are 2 types of type conversions - implicit - explicit // Implicit conversion int iVal = 50 ; long lVal = iVal ; // Java is doing int to long implicit conversion // Explicit conversion long lVal = 50 ; int iVal = ( int ) lVal ; Implict conversions Usually the widening conversion are implicit i.e. automatic e.g. int to long Mixed - Java will use largest integer in equation. Mixed int and float - Will cast to largest floating point in the equation Explicit conversions Both widening and narrowing Float to int --> fraction will be dropped Int to float --> can loose precesion // Example short shortVal = 10 ; long longVal = 10 ; // This will give you a error because implicit cannot be narrowing short result = shortVal - longVal ; So do explicit type cast. short shortVal = 10 ; long longVal = 10 ; short result = ( short ) ( shortVal - longVal ); Conditional Logic Relational Operators > < > \\<= == !== Conditional Assignment // result = condition ? true_val : false_val int v1 = 7 ; int v2 = 5 ; int result = v1 > v2 ? v1 : v2 ; If Else if Else Nested If Block Statement A variable declared with the block statement is only visible inside the block statement. Where the variable is visible is its scope . Locical Operators AND & Or | Exclusive Or (XOR) ^ Negation ! Conditional Logical Operators These only execute the right-side if needed to determine the result. Conditional AND && Conditional OR || & vs && & int rooms = 0 ; int students = 150 ; if ( rooms > 0 & students / rooms > 30 ){ // This will give divide by 0 error. System . out . println ( \"Crowded\" ); } && int rooms = 0 ; int students = 150 ; if ( rooms > 0 && students / rooms > 30 ){ // This will not give divide by 0 error because it evaluates right side only when left side is true. System . out . println ( \"Crowded\" ); } While Loop while ( condition ){ } While Loop do { } while ( condition ) For Loop for ( initialize ; condition ; update ){ } Arrays Provide an ordered collection of elements of same type. float [] theVals = new float [ 3 ] ; // Array for ( int i = 0 ; i < theVals . length ; i ++ ){ } float [] theVals = { 10.0f , 20.0f , 30.0f }; // alternate way of declaring array For Each For each loop executes the code once for each memeber of the array. It automatically handles getting the collection length and accessing each value. float [] theVals = { 10.0 , 20.0 , 30.0 }; float sum = 0.0 ; for ( float currentVal : theVals ){ sum += currentVal ; } System . out . println ( sum ); Switch Only primitive supported with switch are char and int switch ( test - value ){ case value - 1 : statements case value - 2 : statements default : statements } Example of switch. Note always put a break at end of ** int iVal = 10 ; switch ( iVal % 2 ){ case 0 : System . out . print ( iVal ); System . out . println ( \" is even\" ); break ; case 1 : System . out . print ( iVal ); System . out . println ( \" is odd\" ); break ; default : System . out . println ( \"Oops it broke\" ); break ; }","title":"Basics"},{"location":"Java/2.%20Core%20Java/#object-oriented-java","text":"Java is object oriented language Objects encapsulate data, operations and usage semantics Allow storage and manipulation detail to be hidden When creating classes the source file name is same as the name of the class. (For public class its mandatory) Classes are reference types. When we create 2 objects of the same class and we say object2 = object1 , it means that object1 and object2 point to the same memory address. Encapsulation and Access Modifiers This concept is also often used to hide the internal representation, or state, of an object from the outside. No access modifier --> Only within own package public - Everywhere private - Only within its own class Naming Classes Follow \\\"Pascal Case\\\" All first char in words are capital e.g. =BankAccount= Use simple, descriptive names Methods void no return value A method can return a single value a primitive value a reference to an object a reference to an arary (array are objects) The below example demonstrates different return types. // Class Flight public class Flight { private int passengers ; private int seats ; // Constructors and other methods public boolean hasRoom ( Flight f2 ){ int total = passengers + f2 . passengers ; return total <= seats ; } public Flight createNewWithBoth ( Flight f2 ){ // returns a new object of the class. Flight newFlight = new Flight (); newFlight . seats = seats ; newFlight . passengers = passengers + f2 . passengers ; return newFlight ; } } // Main Function Flight lax1 = new Flight (); Flight lax2 = new Flight (); // add passengers to both flights Flight lax3 ; if ( lax1 . hasRoom ( lax2 )){ lax3 = lax1 . createNewWithBoth ( lax2 ); } Special References this - implicit reference to the current object. useful for reducing ambiguity allows an object to pass itself as a parameter public Flight createNewWithBoth ( Flight f2 ){ // returns a new object of the class. Flight newFlight = new Flight (); newFlight . seats = seats ; newFlight . passengers = this . passengers + f2 . passengers ; return newFlight ; } null - is a reference literal represents an uncreated object can be assigned to any reference variable // Main Function ... Flight lax3 = null ; // uncreated object is assigned null. ... Field Encapsulation We use getters and setters instead of exposing the fields of the class. Establishing Initial State of the fields There are 3 ways to do this Field Initial State Constructor Initialization Blocks 1. Field Initial State The variables have to be initialized before you can use them. e.g. the below will give you an error. public static void main ( String [] args ) { int x ; System . out . print ( x ); // Error - x is not initialized } However fields i.e. class variables receive \\\"zero\\\" value by default. int --> defaults to 0 float --> defualts to 0.0 char --> defaults to \\u000 boolean --> defualts to false reference types --> defaults to null or you can initialize them yourself public class Earth { long circum = 24901 ; // initializing manually } 2. Constructors Constructor has no return type. Every class has at least 1 constructor. If there are no explict constructor Java provides one in the background. A class can have multiple constructors with different parameter list 2.1 Chaining Constructors You can call another constructor from within an other constructor. (This is called constructor chaining). You can do that by using this() Call to other constructor must be the first line of the current constructor. e.g. In this class we have 4 constructors. Not all the constructors need to be public . public class Passenger { public Passenger (){} public Passenger ( int freeBags ){ this ( freeBags > 1 25.0 : 50.0 ); this . freeBags = freeBags } public Passenger ( int freeBags , int checkedBags ){ this ( freeBags ); this . checkedBags = checkedBags ; } private Passenger ( double perBagFee ){ this . perBagFee = perBagFee ; } } // Main Passenger jane = new Passenger ( 2 , 3 ); public Passenger(int freeBags, int checkedBags) then this(freeBags) is called from the above constructor Which calls public Passenger(int freeBags) Which in turn calls the private Passenger(double perBagFee) for setting the perBagFee ... 3. Initialization Blocks Initialization blocks are share across all constructors Executed as if the code was placed at start of each constructor. There can be multiple initialization blocks and they are executed in top down fashion public class Flight { private int seats ; { // Start of initialization block } // end of initialization block public Flight (){ } } Order of Execution Java follows the below mentioned order for field initialization and constructor. Field Initialization (Field initial state) Initialization Block Constructor Overloading A class can have the same method name multiple times. Signature needs to be different e.g. Number of parameters Type of each parameter Any number of parameters A method can be declared to accept variable number of parameters. - Place ... after parameter type - It can be done only for the last parameter public class Flight { public void addPassenger ( Passenger ... list ){ // same as *args in Python for ( Passenger p : list ){ // Code here } } }","title":"Object Oriented Java"},{"location":"Java/2.%20Core%20Java/#inheritance","text":"Use extends keyword. public class CargoFlight extends Flight { } One not commonly known phoenomenon is objects of the derived class can be created using the Base type e.g. Flight f = new CargoFlight (); now in f we can use Flight class methods and capabilities but not CargoFlight class capabilities. This is uselful in grouping the objects. Object Class The object class is the root of the java class hierarchy. So we can reference the object class as well. Object [] stuff = new Object [ 3 ] ; stuff [ 0 ] = new Flight (); stuff [ 1 ] = new Passenger ( 2 , 4 ); stuff [ 2 ] = new CargoFlight (); Another example Object o = new Passenger (); o = new CargoFlight (); // as of yet o will only be able to access functionality of 'Object' class in Java // When you need o to access functionality of CargoFlight you need to do the following CargoFlight cf = ( CargoFlight ) o ; // Typecast o to CargoFlight and cf will point to save memory address of o but will be able to access methods of CargoFlight class. cf . addPassenger (); Methods of Object class clone hashCode getClass finalize toString equals Equality == For reference types it checks if the objects point to the same instance. You can override the default equals implementations. class Flight { private int flightNumber ; private int flightClass ; @Override public boolean equals ( Object o ){ // As we are getting object o and we need data from the Flight class we need to cast it. if ( ! ( o instanceof Flight ){) return false ; Flight other = ( Flight ) o ; return this . flightNumber == other . flightNumber ; } } Super Super treats the object as if it was an instance of its base class useful for accessing base class members that have been overridden class Flight extends object { @Override public boolean equals ( Object o ) { super . equals ( o ); // Calling the super class method } } Final and Abstract By default all classes can be extended Creating a final class. A final class cannot be extended or inherited from. public final class Passenger { } You can also make a particular method as final and not the whole class thus that method cannot be overriden. public class CargoFlight { public void methodA (){} public final void methodB (){} } Abstract Abstract will require that the class will be inherited or a method will be overriden. If any method in a class is abstract you need to mark the whole class as abstract. public abstract class Pilot { public void methodA (){ } public abstract boolean canAccept ( Flight f ); // this is an abstract method. } Abstract class cannot be instanciated. The below code will give you an error. abstract class Pilot { Pilot (){} } public class Main { public static void main ( String [] args ) { Pilot p1 = new Pilot (); // Trying to instanciate a abstract class. } } Inheritance and Constructor Constructors are not inherited A base class constructor must always be called. If you do not do this explicitly, Java will call the base class no argument constructor automatically. If you call manually make sure you call the base class constructor in the first line of the current class constructor. // TODO example (added a note on the video)","title":"Inheritance"},{"location":"Java/2.%20Core%20Java/#interfaces","text":"What is interface? Do not provide implementation. Only the contract is provided. Although in new versions of Java this has changed --> a default method can have implementation as well. You can also add fields to an interface in new versions of Java. Although a lot of java community was not happy about the above and this point. e.g. an existing interface is java.lang.Comparable . It says that we should implement a compareTo method. This method returns negetive value (we should come before), positive value (we should come after) and 0 (if equal). Now lets use this in our flight class. public class Passenger implements Comparable { private int memberLevel ; private int memberDays ; public int compareTo ( object o ){ Passenger p = ( passenger ) o ; if ( this . memberLevel > p . memberLevel ) return - 1 ; else if ( this . memberLevel < p . memberLvel ) return 1 ; return 0 ; } } Because our class conforms to the Comparable interface we can create a array of passenger objecs and sort it. Passenger [] passengers = { bob , jane , steve , lisa }; Arrays . sort ( passengers ); // because the Passenger class conforms to the Comparable interface. Some interfaces require another types generics . e.g. public interface Comparable < T > { } So you can specify the <T> type as shown below. Then you can specify which object will be compared thus getting rid of the type casting from object to Flight or Passenger object. public class Flight implements Comparable < Flight > { private int flightTime ; public int compareTo ( Flight f ){ // notice the type parameter return this . flightTime - f . flightTime ; } } Classes are free to implement multiple interfaces but can only extend 1 class. e.g. public class Flight implements Comparable < flight > , Iterable < Person > { } Another comparable example /* Comparable Interface. */ import java.util.ArrayList ; import java.util.Collection ; import java.util.Collections ; import java.util.List ; class Employee implements Comparable < Employee > { int id ; String name ; public Employee ( int id , String name ) { this . id = id ; this . name = name ; } @Override public int compareTo ( Employee o ) { return id - o . id ; } @Override public String toString () { return \"Employee{\" + \"id=\" + id + \", name='\" + name + '\\'' + '}' ; } } public class Main13 { public static void main ( String [] args ) { List < Employee > employees = new ArrayList <> (); employees . add ( new Employee ( 109 , \"sam nelson\" )); employees . add ( new Employee ( 10 , \"pete nelson\" )); employees . add ( new Employee ( 11 , \"becky nelson\" )); System . out . println ( employees . get ( 0 ). compareTo ( employees . get ( 1 ))); Collections . sort ( employees ); // Sorting employees array. for ( Employee employee : employees ){ System . out . println ( employee ); } } } Declaring an Interface Can have methods but no implementations. Can have constants. Can extend other interfaces.","title":"Interfaces"},{"location":"Java/2.%20Core%20Java/#static","text":"Static Members Static members are shared class wide. Declared using the static keyword. Can be accessed using the class name. You can also access them directly without putting class name in front of them if you did a static import. e.g. public class Flight { static int allPassengers ; public static void resetAllPassengers (){ allPassengers = 0 ; } } import static com.company.travel.Flight.resetAllPassengers ; // static import public class main { public static main ( String args [] ){ resetAllPassengers (); // do not need to do Flight.resetAllPassengers(); } } Static initialization blocks One time Execute automatically before the static type is used. Must handle all checked exceptions, no concept of throws keyword. public class Flight { static { // static initilization block } }","title":"Static"},{"location":"Java/2.%20Core%20Java/#nested-types","text":"Classes within classes and interfaces Interface within classes and interfaces The nested type is treated as a type of the enclosing type and has access to its private members Why use them Structure and scoping (Static inner class) - ** Inner classes (each instance of nested class is associated with the instance of enclosing class) (non static inner class) Inner Class Example of inner class provided below. In this example the inner class is creating an Iterator check CustomIterator below. import java.util.Iterator ; class GenericList implements Iterable < Integer > { int [] mylist = new int [ 10 ] ; int position = 0 ; void push ( int value ){ mylist [ position ++] = value ; } int pop (){ return mylist [-- position ] ; } @Override public Iterator < Integer > iterator () { return new CustomIterator ( this ); } public class CustomIterator implements Iterator < Integer > { GenericList list ; private int pointer = 0 ; CustomIterator ( GenericList list ){ this . list = list ; } @Override public boolean hasNext () { return pointer < list . position ; } @Override public Integer next () { return list . mylist [ pointer ++] ; } } } public class Main10 { public static void main ( String [] args ) { GenericList lst = new GenericList (); lst . push ( 10 ); lst . push ( 20 ); // System.out.println(lst.pop()); for ( int value : lst ){ System . out . println ( value ); } } } Anonymous Classes - Useful for extend interface/classes to solve a particular problem which is not code wide. --> Check lambda's as well.","title":"Nested types"},{"location":"Java/2.%20Core%20Java/#more-data-types","text":"Strings Java string has UTF-16 encoding Use double quotes \\\"\\\" contatenate using + String objects are immutable. (They cannot be changed but new value can be assigned to them). This can be inefficient Methods length valueOf - convert non string value to a string concat replace toLowerCase trim split format chatAt substring contains startsWith equals equalsIgnoreCase ... Documentation on String Methods String Equality Check out apache commons library for string operations. It takes care of the null pointer exception in most cases. String s1 = \"I Love\" ; s1 += \"Java\" ; String s2 = \"I\" ; s1 += \" Love Java\" ; if ( s1 == s2 ) {} // Its False The above example returns false because they are not the exact same instance of the String even though they have the same value in them. if ( s1 . equals ( s2 )){} // This will return True .equals does a char by char comparison. As the char based comparisons are quiet expensive we use the intern method to compare strings. intern method returns a cannonical form of the string based on its value. s1 . intern () == s2 . intern (); // Will return true intern does have its overhead so use it only if you are doing comparisons over and over again. So lets say you have multiple strings which are master data and you want to search (multiple times) a new string exists in this array of strings or not. In this case turn the array of strings into intern and comapre using the == operator which will be inexpensive. Convert to String int iVal = 100 ; String sVal = String . valueOf ( iVal ); // sVal = \"100\" Remember that object class provides some standard methods that all classes will have. Such a method is toString which is used to get the string representation of various objects e.g. an object of a class. public class Flight { int flightNumber ; @Override public String toString (){ return \"I fly to\" + flightNumber ; } } String Builder Remember strings are immutable but sometimes we wish to manipulate them. StringBuilder provides mutable string buffer General recomendation --> pre-size buffer StringBuilder sb = new StringBuilder ( 40 ); // Sample methods // append // insert sb . append ( \"I flew to \" ); sb . append ( \"Florida\" ); sb . insert ( 4 , \"at\" ); // convert back to String String message = sb . toString (); Classes vs Primitives !!! note \\\"\\\" You may not use this very often. Classes Provide convenience incurs an overhead Primitives Provide efficiency So we sometimes we use Primitive Wrapper Class . The standard class hierarchy for primitive type looks like this. Classes shown below: - Object Boolean Number Byte Short Integer Long Float Double Character All primitive wrapper class instances are immutable. When you create an int variable it is an instance/object of the Integer class shown above. The conversion is done automatically. Java also provides methods for explicit conversions. Primitive to wrapper -> valueOf . This is known as boxing. Wrapper to primitive > =xxxValue . This is known as unboxing. String to primitive -> parseXxx String to wrapper -> valueOf Using this you can treat the primitive type as an object. Example 1 - Treat as object Object [] stuff = new Object [ 3 ] ; stuff [ 0 ] = new Flight (); stuff [ 1 ] = new Passenger ( 0 , 2 ); stuff [ 2 ] = 100 ; Example 2 - Null References public class Flight { Integer flightNumber ; // note we are not creating int but Integer which creates it as an object Character flightClass ; // same as above @Override public String toString (){ if ( flightNumber != null ){ // we can now compare int to null because its an object. Else as soon as you create an object of flight class the int will get value of 0 and if there is any flight number with value 0 the comparison will not work e.g. flightNumber != 0 is leaving 1 case out where as using int as Integer i.e. as object we can also cover the case of 0. } else if ( flightClass != null ){ } } } Sample documentation for Interger Class Refer the same for other primitive wrapper classes. Wrapper Class Equality Did not read a lot on this becase seemed I may never use it. (Check slides) Final Fields Final Static - Cannot be set by an object instance. public class Flight { static final int MAX_FAA_SEATS = 500 ; } Enumeration types Its useful for defining a type with a finite list of valid values. Declare with keyword enum and provide a comma separated value list of types. public enum FlightCrewJob { Pilot , CoPilot , FlightAttendant } public class CrewMember { private FlightCrewJob job ; } // How to create CrewMember judy = newCrewMember ( FlightCrewJob . CoPilot );","title":"More Data Types"},{"location":"Java/2.%20Core%20Java/#exceptions","text":"There are in total 3 types of exceptions - Error - Checked - Detected by compiler. - Unchecked - Happening at runtime. try/catch/finally Try block will run the normal code Catch block is called only if matching exception is thrown runs in all cases when try block or catch block finishes. It usually contains clean up code. A single try can have multiple catch public class test { public static void main ( String [] args ) { int i = 1 ; int j = 0 ; try { System . out . println ( i / j ); } catch ( Exception e ){ System . out . println ( \"Error : \" + e . getMessage ()); System . out . println ( e . getStackTrace ()); } finally { System . out . println ( \"Program continues...\" ); } } } Example below : Reading file in JAVA using exception handling. import java.io.BufferedReader ; import java.io.FileReader ; public class test { public static void main ( String [] args ) { BufferedReader reader = null ; int total = 0 ; try { // try reading the file reader = new BufferedReader ( new FileReader ( \"C:\\\\a.txt\" )); String line = null ; while (( line = reader . readLine ()) != null ){ // read line by line System . out . println ( line ); total += Integer . valueOf ( line ); System . out . println ( \"Total = \" + total ); } } catch ( Exception e ){ // print any exception when reading the file System . out . println ( e . getMessage ()); } finally { // here we close the file using nested try and catch try { if ( reader != null ) reader . close (); } catch ( Exception e ){ System . out . println ( e . getMessage ()); } } } } Exception Class Hierarchy (Check slide) Object Throwable Error Linkage Error ... Exception -Runtime Exception - Null Pointer Exception ... Some of the excpetions are checked exceptions and some are unchecked exceptions. Its mandatory to handle Checked Exceptions Exception are handled by Type Each type of exception can have a seperate catch block. each catch is tested from top to bottom first assignable catch is selected You should start with Specific exceptions at the top and then get general as you go down. Exception and methods In some cases a method does a processing of say a file open. But the file name is being passed by another method. The method which is opening the file will get the exception if the file name is not correct but it should be the method which sends the file name which should be made aware of this exception so we can use throws Exception on the method which is opening the file and this will propogate up the call stack. public class Flight { public void addPassengers ( String filename ) throws IOException { // ... try { // open file } finally { // close file } } } As you see in the above example the method which is receiving the file name is not catching the exception its just throws IOException to the caller method. The throws clause of an overriding method must be compatable with the throws clause of the overriden method. Throwing Exception Exceptions are objects, they have to be created before they are thrown. Put meaning full information in it. When caused by another exception, include orignal originating exception by using initCause method. You can also create your own excpetion types and throw them however in most of the cases you will use the existing exception types. Inherit from Exception class. Make them checked excpetions. Constructors are often their only members","title":"Exceptions"},{"location":"Java/2.%20Core%20Java/#packages","text":"A package is a group of related types It creates a namespace, useful in naming collisions. Usually use reverse domain naming. It provides access boundaries It acts as a unit of distribution e.g. package com.examplesite.travel ; public class Flight { } to use this you can fully qualify the type like we do below com . examplesite . travel . Flight lax178 = ...; Whenever you are working in package you don\\'t have to fully qualify standard ones need not be fully qualify for others use type imports Type imports we do this using import statement. e.g. import com.pluralsight.travel.Flight ; import com.xyzcompany.bar ; Flight = ; Wine = ; Package can serve as an access boundary. No access modifier is by default a package private. Others are public, private and protected. Jar Files You can distribute your entire package or library of code using single jar file which maintains the folder structure in that file. In addition to that it can also contains a manifest file which provides information about that Jar file. Its usually in named value format. You can find more information about it here You can create - Normal Jar - Runnable Jar","title":"Packages"},{"location":"Java/20.%20Design%20patterns%20-%20creational/","text":"Design Patterns : Creational Singleton Concept Gaurentees only 1 instance is going to be created Gaurentees the control of the resource Usually lazily loaded Examples Runtime Logger Spring Beans Graphics Managers Design Static in nature but not static class (because it needs to be thread safe) private instance private constructor no params, if params req its a factory pattern and violates rules of singleton // A very basic singletom class DBSingleton { private static DBSingleton instance = new DBSingleton (); // only a single instance private DBSingleton (){} // private constructor // the only public method will return that single instance which was created above. public static DBSingleton getInstance (){ return instance ; } } Now you can call the above singleton class using main method public class SingletonDemo { public static void main ( String [] args ) { DBSingleton instance = DBSingleton . getInstance (); System . out . println ( instance ); // DBSingleton@4554617c } } Lazy loaded As of the above demo the singleton is always created whether you use it or not. Lets make it lazily loaded. class DBSingleton { private static DBSingleton instance = null ; private DBSingleton (){} // private constructor // the only public method will return that single instance which was created above. public static DBSingleton getInstance (){ if ( instance == null ){ instance = new DBSingleton (); } return instance ; } } Thread Safe I am not there yet :) Pitfalls overused diffcult to unit test if not careful, not threadsafe sometimes confused for factory Contrast (check slide) mostly it will be contrasted with Factory method Builder Pattern Concepts Handles complex constructors Large no of parameters Immutability Examples StringBuilder DocumentBuilder Locale.Builder Design The builder pattern solves a common problem where we have permutations of parameters when a object is being created. So in this case you will create multiple constructors. The builder pattern simplifies this. Flexibility over telescoping constructors Static inner class Calls appropriate constructors Negates needs of exposed setters Demo class LunchMenu { public String getBread () { return bread ; } public String getCheese () { return cheese ; } public String getVegges () { return vegges ; } public String getMeat () { return meat ; } // builder class within the actual lunchmenu class. public static class Builder { private String bread ; private String cheese ; private String vegges ; private String meat ; Builder ( String bread ){ // bread is mandatory in sandwich this . bread = bread ; } // rest of these are optionals public Builder cheese ( String cheese ){ this . cheese = cheese ; return this ; } public Builder vegees ( String vegges ){ this . vegges = vegges ; return this ; } public Builder meat ( String meat ){ this . meat = meat ; return this ; } public LunchMenu build (){ return new LunchMenu ( this ); } } private String bread ; private String cheese ; private String vegges ; private String meat ; private LunchMenu ( Builder builder ){ this . bread = builder . bread ; this . cheese = builder . cheese ; this . meat = builder . meat ; this . vegges = builder . vegges ; } } public class BuilderPatternDemo { public static void main ( String [] args ) { LunchMenu . Builder builder = new LunchMenu . Builder ( \"Herbs and cheese\" ); builder . cheese ( \"swis\" ). meat ( \"chicken\" ); LunchMenu sandwhich1 = builder . build (); System . out . println ( sandwhich1 . getBread ()); System . out . println ( sandwhich1 . getCheese ()); System . out . println ( sandwhich1 . getMeat ()); System . out . println ( sandwhich1 . getVegges ()); } } Pitfalls Doesn\\'t have a lot of negetives, just some things to consider. objects created are generally made to be immutable inner static class is requried little bit more complex Prototype Design pattern Generally used to create a new instance of the same flavor. Concept When trying to avoid costly creation this will be created when you are refactoring your application. (based on performance considerations) avoids subclassing typically dont use the keyword new often utilizes an Interface usually implemented with some type of registry when another object is needed we create a clone of that with the registry Example java.lang.Object --> clone() method If the example is expensive to create then we just copy the memeber variables and create the object. It typically implements the Cloneable interface with method clone. Can utilize parameters for construction but you typicall dont. There are shallow vs deep copy. // TODO Factory Method Pattern Concept This does not expose instanciation logic It defers the instanciation to its subclass A common interface is exposed often implemented by framework, where the user of the framwork uses this and customizes it Examples Calendar Resource Bundle Number format","title":"20. Design patterns   creational"},{"location":"Java/20.%20Design%20patterns%20-%20creational/#design-patterns-creational","text":"","title":"Design Patterns : Creational"},{"location":"Java/20.%20Design%20patterns%20-%20creational/#singleton","text":"Concept Gaurentees only 1 instance is going to be created Gaurentees the control of the resource Usually lazily loaded Examples Runtime Logger Spring Beans Graphics Managers Design Static in nature but not static class (because it needs to be thread safe) private instance private constructor no params, if params req its a factory pattern and violates rules of singleton // A very basic singletom class DBSingleton { private static DBSingleton instance = new DBSingleton (); // only a single instance private DBSingleton (){} // private constructor // the only public method will return that single instance which was created above. public static DBSingleton getInstance (){ return instance ; } } Now you can call the above singleton class using main method public class SingletonDemo { public static void main ( String [] args ) { DBSingleton instance = DBSingleton . getInstance (); System . out . println ( instance ); // DBSingleton@4554617c } } Lazy loaded As of the above demo the singleton is always created whether you use it or not. Lets make it lazily loaded. class DBSingleton { private static DBSingleton instance = null ; private DBSingleton (){} // private constructor // the only public method will return that single instance which was created above. public static DBSingleton getInstance (){ if ( instance == null ){ instance = new DBSingleton (); } return instance ; } } Thread Safe I am not there yet :) Pitfalls overused diffcult to unit test if not careful, not threadsafe sometimes confused for factory Contrast (check slide) mostly it will be contrasted with Factory method","title":"Singleton"},{"location":"Java/20.%20Design%20patterns%20-%20creational/#builder-pattern","text":"Concepts Handles complex constructors Large no of parameters Immutability Examples StringBuilder DocumentBuilder Locale.Builder Design The builder pattern solves a common problem where we have permutations of parameters when a object is being created. So in this case you will create multiple constructors. The builder pattern simplifies this. Flexibility over telescoping constructors Static inner class Calls appropriate constructors Negates needs of exposed setters Demo class LunchMenu { public String getBread () { return bread ; } public String getCheese () { return cheese ; } public String getVegges () { return vegges ; } public String getMeat () { return meat ; } // builder class within the actual lunchmenu class. public static class Builder { private String bread ; private String cheese ; private String vegges ; private String meat ; Builder ( String bread ){ // bread is mandatory in sandwich this . bread = bread ; } // rest of these are optionals public Builder cheese ( String cheese ){ this . cheese = cheese ; return this ; } public Builder vegees ( String vegges ){ this . vegges = vegges ; return this ; } public Builder meat ( String meat ){ this . meat = meat ; return this ; } public LunchMenu build (){ return new LunchMenu ( this ); } } private String bread ; private String cheese ; private String vegges ; private String meat ; private LunchMenu ( Builder builder ){ this . bread = builder . bread ; this . cheese = builder . cheese ; this . meat = builder . meat ; this . vegges = builder . vegges ; } } public class BuilderPatternDemo { public static void main ( String [] args ) { LunchMenu . Builder builder = new LunchMenu . Builder ( \"Herbs and cheese\" ); builder . cheese ( \"swis\" ). meat ( \"chicken\" ); LunchMenu sandwhich1 = builder . build (); System . out . println ( sandwhich1 . getBread ()); System . out . println ( sandwhich1 . getCheese ()); System . out . println ( sandwhich1 . getMeat ()); System . out . println ( sandwhich1 . getVegges ()); } } Pitfalls Doesn\\'t have a lot of negetives, just some things to consider. objects created are generally made to be immutable inner static class is requried little bit more complex","title":"Builder Pattern"},{"location":"Java/20.%20Design%20patterns%20-%20creational/#prototype-design-pattern","text":"Generally used to create a new instance of the same flavor. Concept When trying to avoid costly creation this will be created when you are refactoring your application. (based on performance considerations) avoids subclassing typically dont use the keyword new often utilizes an Interface usually implemented with some type of registry when another object is needed we create a clone of that with the registry Example java.lang.Object --> clone() method If the example is expensive to create then we just copy the memeber variables and create the object. It typically implements the Cloneable interface with method clone. Can utilize parameters for construction but you typicall dont. There are shallow vs deep copy. // TODO","title":"Prototype Design pattern"},{"location":"Java/20.%20Design%20patterns%20-%20creational/#factory-method-pattern","text":"Concept This does not expose instanciation logic It defers the instanciation to its subclass A common interface is exposed often implemented by framework, where the user of the framwork uses this and customizes it Examples Calendar Resource Bundle Number format","title":"Factory Method Pattern"},{"location":"Java/3.%20Collections/","text":"Problems with arrays Cannot resize very low level concept do not provide many functionality like add, duplicacy check etc... With below example I try to show the problems with arrays. lets say we have a simple class Product and we are doing some operations on it in the main program below // Product.java package ca.rajatsethi.programming ; public class Product { private String name ; private int price ; public Product ( String name , int price ) { this . name = name ; this . price = price ; } @Override public String toString () { return \"Product{\" + this . name + \",\" + this . price + \"}\" ; } } // main.java package ca.rajatsethi.programming ; import java.lang.reflect.Array ; import java.util.Arrays ; public class Main { public static void main ( String [] args ) { Product door = new Product ( \"Wooden Door\" , 35 ); Product window = new Product ( \"Wooden Window\" , 15 ); Product [] products = { door , window }; System . out . println ( Arrays . toString ( products )); // print out in human legible format. // https://docs.oracle.com/javase/9/docs/api/java/util/Arrays.html#toString-java.lang.Object:A- // just to add we have to implement another function as arrays do not resize. Product handle = new Product ( \"Handle\" , 4 ); products = add ( products , handle ); // arrays also do not check for duplicate, you can add handle again. products = add ( products , handle ); System . out . println ( Arrays . toString ( products )); } // we wish to add another product to the products array. The problem is that the array cannot be resized. // so lets create another method which adds functionality to add another product to the array. public static Product [] add ( Product [] array , Product product ){ int length = array . length ; Product [] newArray = Arrays . copyOf ( array , length + 1 ); newArray [ length ] = product ; return newArray ; } } // Console output [ Product { Wooden Door , 35 }, Product { Wooden Window , 15 } ] [ Product { Wooden Door , 35 }, Product { Wooden Window , 15 }, Product { Handle , 4 }, Product { Handle , 4 } ] Java Collections Java collections ships with the JDK Any application in Java (which is not very basic) will use the collections frameworks. The data structures are diverse Some of them provide ordered access. Some of them provide uniqueness. Some of them provide ability for pairs. Defining collections Collection of Collections All the java interfaces which deal with collections extend the Collection . Types of collections Lists (Array list and linked list) Sets (Hash Set) Sorted Set (Tree set) Queue (Priority queue) and Deque or double ended queue (Linked list and Array Deque) Map (Hash map) and Sorted Map (Treemap) Lists is interface its implementation is array list and linked list. Same as others. Outside brackets --> interfaces, inside brackets --> their specific implementations. The interface will drive their charactersistics, but for 1 interface there can be multiple implementations e.g. for list we have 2. Collection Behaviour All collections extend the iterable interface. Some of the other methods in the collection interface are shown below. size() isEmpty() add(element) addAll(collection) remove(element) removeAll(collection) retainall(collection) contains(element) containsAll(collection) clear() Example of working with collections (in this case we are using ArrayList ) is given below. // Product.java public class Product { private String name ; private int price ; public Product ( String name , int price ) { this . name = name ; this . price = price ; } @Override public String toString () { return \"Product{\" + this . name + \",\" + this . price + \"}\" ; } public int getPrice () { return price ; } } // main.java import java.util.ArrayList ; import java.util.Collection ; import java.util.Iterator ; public class Main { public static void main ( String [] args ) { Product door = new Product ( \"Door\" , 15 ); Product floorPannel = new Product ( \"Floor Pannel\" , 60 ); Product window = new Product ( \"Window\" , 30 ); Collection < Product > products = new ArrayList <> (); // creating a collection of products <T> -> <Product> // now that our collection is created we can add our products to the `products` collection. products . add ( door ); products . add ( floorPannel ); products . add ( window ); // printing out the whole collection System . out . println ( products ); // iterating on collection using for loop. for ( Product product : products ){ System . out . println ( product ); } // iterating on collection the long way. // this way is useful if you wish to modify the collection while looping on it // e.g. if you wish to remove product from the products collection while looping on it as shown below. final Iterator < Product > productIterator = products . iterator (); while ( productIterator . hasNext ()){ Product product = productIterator . next (); if ( product . getPrice () > 20 ) { System . out . println ( product ); } else { productIterator . remove (); // if price is less than that then remove from the collection. } } // printing out the whole collection to confirm its removed. System . out . println ( products ); // trying other methods System . out . println ( \"Is collection empty : \" + products . isEmpty ()); System . out . println ( \"Collection size : \" + products . size ()); System . out . println ( \"Contains floorPannel : \" + products . contains ( floorPannel )); } } Lists There are 2 types of lists. Arraylist Linked List In the example below we are using ArrayList and wrapping it in out Shipment class which represents something in our domain. (This is a common practice). The shipment class will have functions which make sense in the real world and in the background it will use Arraylist. // Product.Java package ca.rajatsethi.programming ; import java.util.Comparator ; public class Product { // Attributes of the Product Class private String name ; private int weight ; // Constructor public Product ( String name , int weight ) { this . name = name ; this . weight = weight ; } // Getters public int getWeight () { return weight ; } // String representation of product @Override public String toString () { return \"Product{ \" + this . name + \" , \" + this . weight + \"}\" ; } // implementing the comparator public static final Comparator < Product > BY_WEIGHT = Comparator . comparing ( Product :: getWeight ); } // Shipment.Java package ca.rajatsethi.programming ; import java.util.ArrayList ; import java.util.Iterator ; import java.util.List ; public class Shipment implements Iterable < Product > { private static final int PRODUCT_NOT_PRESENT = - 1 ; private static final int SMALL_VAN_MAX_ITEM_WEIGHT = 20 ; private List < Product > products = new ArrayList <> (); // products arraylist private List < Product > small_van_products = new ArrayList <> (); private List < Product > large_van_products = new ArrayList <> (); // Getters public List < Product > getSmall_van_products () { return small_van_products ; } public List < Product > getLarge_van_products () { return large_van_products ; } // Iterator @Override public Iterator < Product > iterator () { return products . iterator (); // uses the built in list iterator. } // Adding product public void add ( Product p ) { products . add ( p ); } // replacing the product public void replace ( Product oldProduct , Product newProduct ) { int oldProductIndex = products . indexOf ( oldProduct ); if ( oldProductIndex != PRODUCT_NOT_PRESENT ) { products . set ( oldProductIndex , newProduct ); } } // Seperating products into different Vans public void prepare () { products . sort ( Product . BY_WEIGHT ); // sort by weight ascending. int splitPoint = findSplitPoint (); small_van_products = products . subList ( 0 , splitPoint ); large_van_products = products . subList ( splitPoint , products . size ()); } // Helper product private int findSplitPoint () { for ( Product p : products ){ if ( p . getWeight () > SMALL_VAN_MAX_ITEM_WEIGHT ){ return products . indexOf ( p ); } } return 0 ; } } // main.java package ca.rajatsethi.programming ; public class Main { public static void main ( String [] args ) { Shipment ship = new Shipment (); Product floorPannel = new Product ( \"Floor Pannel\" , 30 ); Product window = new Product ( \"Window\" , 10 ); Product door = new Product ( \"Door\" , 45 ); ship . add ( floorPannel ); ship . add ( window ); ship . add ( door ); // Because we implemented the iterable on the Product for the // Shipment class we can iterate over it. for ( Product p : ship ){ System . out . println ( p ); } ship . prepare (); System . out . println ( \"Small Van products = \" + ship . getSmall_van_products ()); System . out . println ( \"Large Van products = \" + ship . getLarge_van_products ()); } } Another example below which shows how to create your own list on top of Array (with creating own version of iteration and not using an existing iterator like ArrayList iterator) capability. /* Java Iterable and Iterator Example. */ import java.util.Iterator ; class GenericList implements Iterable < Integer > { int [] mylist = new int [ 10 ] ; int position = 0 ; void push ( int value ){ mylist [ position ++] = value ; } int pop (){ return mylist [-- position ] ; } @Override public Iterator < Integer > iterator () { return new CustomIterator ( this ); } public class CustomIterator implements Iterator < Integer > { GenericList list ; private int pointer = 0 ; CustomIterator ( GenericList list ){ this . list = list ; } @Override public boolean hasNext () { return pointer < list . position ; } @Override public Integer next () { return list . mylist [ pointer ++] ; } } } public class Main10 { public static void main ( String [] args ) { GenericList lst = new GenericList (); lst . push ( 10 ); lst . push ( 20 ); for ( int value : lst ){ System . out . println ( value ); } } } Sets There are the following types of set implementations which are avaiable to us. HashSet TreeSet EnumSet (Designed to be efficient when dealing with enum types) Hash Set These are based on HashMap. (Calls hashCode() on element and looks up the location). Hash Sets are good general purpose implementations. They resize when run out of space. How equals works Standard java checks the equals using the hash code. If 2 objects hashcode value is same then they are equals. i.e. they are the same object. Your implementation of equals can differ and you will have to override it. object . hashCode () == object . hashCode () Tree Set Tree set is based on tree map. Similarly hash set was based on hash map. Uses sorted binary tree. It keeps the elements in specified order. (it implements SortedSet and NavigableSet ) Enum Set Only allow to store enum objects. Uses a bitset based on ordinal of the enum. Two other interfaces mentioned below which extend the behaviour of Sets. They talk about enforcing orders. SortedSet NavigableSet SortedSet E first() E last() SortedSet tailSet(E fromElement) SortedSet headSet(E toElement) SortedSet subSet(E fromElement, E toElement) NavigableSet This extends sortedSet and is implemented by Treeset. E lower(E e) E higher(E e) E floor(E e) E cieling(E e) E pollFirst() E pollLast() Queues Deque and Stacks Queue First In First Out Methods offer() --> Use offer method instead of add() when adding to queue. As some of the queues are bounded (max in queue) and if you use add() method to add something to queue when its full it throws an exception. offer() will just return false. poll() --> remove and return value. The remove() method throws exception when queue is empty and you wish to remove something so instead of using it use poll() method. peek() --> use peek, element() throws exception when empty, peek returns null. Basic example of Queue provided below. The below queue has been implemented using the LinkedList implementation. import java.util.LinkedList ; import java.util.Queue ; public class Main { public static void main ( String [] args ) { // Instanciating a new queue of type linked list Queue < Integer > q = new LinkedList <> (); // adding elements to queue q . add ( 10 ); q . add ( 20 ); // printing out the queue System . out . println ( q ); // get first element System . out . println ( q . element ()); System . out . println ( q ); // iterating over queue for ( int i : q ) { System . out . println ( i ); } // remove element from the queue System . out . println ( \"Remove element from queue : \" + q . remove ()); System . out . println ( q ); } } A more real world example of helpdesk implemented with queue is provided below. // Category.java public enum Category { PRINTER , COMPUTER , PHONE , TABLET } //Customer.java public class Customer { private final String name ; public Customer ( String name ) { this . name = name ; } public void reply ( final String message ) { System . out . println ( this . name + \" : \" + message ); } public static final Customer JACK = new Customer ( \"Jack\" ); public static final Customer JILL = new Customer ( \"Jill\" ); public static final Customer MARY = new Customer ( \"Mary\" ); } // Enquiry.java public class Enquiry { private final Customer customer ; private final Category category ; public Enquiry ( Customer customer , Category category ) { this . customer = customer ; this . category = category ; } public Customer getCustomer () { return customer ; } public Category getCategory () { return category ; } @Override public String toString () { return \"Enquiry{customer = \" + customer + \", category = \" + category + \"}\" ; } } //Helpdesk.java import java.util.ArrayDeque ; import java.util.Queue ; public class HelpDesk { private final Queue < Enquiry > enquiries = new ArrayDeque <> (); public void eqnuire ( final Customer customer , Category category ) { enquiries . offer ( new Enquiry ( customer , category )); } public void processAllEnquiries () { Enquiry enquiry ; while (( enquiry = enquiries . poll ()) != null ) { enquiry . getCustomer (). reply ( \"Have you tried turning if off and on again?\" ); } } public static void main ( String [] args ) { HelpDesk helpDesk = new HelpDesk (); helpDesk . eqnuire ( Customer . JACK , Category . PHONE ); helpDesk . eqnuire ( Customer . JILL , Category . PRINTER ); helpDesk . processAllEnquiries (); } } Using Priority Queue We can actually use priority queue to sort the enquiry using some priority. In the Helpdesk.java class we will now implement priorityQueue. Helpdesk . java import java.util.ArrayDeque ; import java.util.Comparator ; import java.util.PriorityQueue ; import java.util.Queue ; /** * Created by sethir on 2019/02/01. */ public class HelpDesk { private final Queue < Enquiry > enquiries = new PriorityQueue <> ( BY_CATEGORY ); // creating a priority queue by using category comparator. public void eqnuire ( final Customer customer , Category category ) { enquiries . offer ( new Enquiry ( customer , category )); } public static final Comparator < Enquiry > BY_CATEGORY = new Comparator < Enquiry > () { // implementing comparator. @Override public int compare ( Enquiry o1 , Enquiry o2 ) { return o1 . getCategory (). compareTo ( o2 . getCategory ()); } }; public void processAllEnquiries () { Enquiry enquiry ; while (( enquiry = enquiries . poll ()) != null ) { enquiry . getCustomer (). reply ( \"Have you tried turning if off and on again?\" ); } } public static void main ( String [] args ) { HelpDesk helpDesk = new HelpDesk (); helpDesk . eqnuire ( Customer . JACK , Category . TABLET ); helpDesk . eqnuire ( Customer . JILL , Category . PRINTER ); helpDesk . eqnuire ( Customer . MARY , Category . PHONE ); helpDesk . processAllEnquiries (); } } The result is sorted based on the enum ordering. // output Jill : Have you tried turning if off and on again ? Mary : Have you tried turning if off and on again ? Jack : Have you tried turning if off and on again ? Stack and Deque Stacks are Last In - First Out Java.util.stack is deprecated and should not be used. You should be using Deque, they are the correct way to use stacks. Deque (double ended queue) --> Use 2 ends. Below are the methods which we can use for Deque to implement stack. boolean offerFirst(E e) boolean offerLast(E e) void addFirst(E e) void addLast(E e) E removeFirst() E removeLast() E pollFirst() E pollLast() E getFirst() E getLast() E peekFirst() E peekLast() If the above methods are confuing . void Push(E e) void pop() Example of Calculator provided below. // Calculator.Java package ca.rajatsethi.programming ; import java.util.ArrayDeque ; import java.util.Deque ; public class Calculator { public int evaluate ( final String input ) { final Deque < String > stack = new ArrayDeque <> (); final String [] tokens = input . split ( \" \" ); for ( String token : tokens ) { stack . push ( token ); } while ( stack . size () > 1 ) { int left = Integer . parseInt ( stack . pop ()); String operator = stack . pop (); int right = Integer . parseInt ( stack . pop ()); int result = 0 ; switch ( operator ) { case \"+\" : result = left + right ; break ; case \"-\" : result = left - right ; break ; } stack . push ( String . valueOf ( result )); } return Integer . parseInt ( stack . pop ()); } } // Main.java package ca.rajatsethi.programming ; public class Main { public static void main ( String [] args ) { Calculator calculator = new Calculator (); System . out . println ( calculator . evaluate ( \"1 + 2\" )); System . out . println ( calculator . evaluate ( \"1 + 2 - 11 - 12 - 18 + 109\" )); System . out . println ( calculator . evaluate ( \"1 + 6\" )); } } // output 3 103 7 Maps Maps are key value pairs like dictionaries in Python. A quick example with maps is given below. // product.java package ca.rajatsethi.programming ; import java.util.Comparator ; public class Product { private int id ; private String name ; private int weight ; public Product ( int id , String name , int weight ) { this . id = id ; this . name = name ; this . weight = weight ; } public int getId () { return id ; } public String getName () { return name ; } public int getWeight () { return weight ; } // Implementing comparator public static final Comparator < Product > BY_WEIGHT = Comparator . comparing ( Product :: getWeight ); public static final Comparator < Product > BY_NAME = Comparator . comparing ( Product :: getName ); } // ProductLookupTable.java --> Interface package ca.rajatsethi.programming ; public interface ProductLookupTable { Product lookupByID ( int id ); void addProduct ( Product productToAdd ); void clear (); } We first see how we do things without a map. The below is using lists. // NaiveProductLookupTable.Java package ca.rajatsethi.programming ; import java.util.ArrayList ; import java.util.List ; public class NaiveProductLookupTable implements ProductLookupTable { private List < Product > products = new ArrayList <> (); @Override public Product lookupByID ( int id ) { for ( Product product : products ){ if ( product . getId () == id ){ return product ; } } return null ; } @Override public void addProduct ( Product productToAdd ) { for ( Product product : products ){ if ( product . getId () == productToAdd . getId ()){ throw new IllegalArgumentException ( \"Unable to add : duplicate id : \" + product . getId ()); // throwing exception because we found a duplicate id } } products . add ( productToAdd ); } @Override public void clear () { products . clear (); } } Now that we have seen list, we do the same using maps below. The code is cleaner and performs much better. // MapProductLookupTable.Java package ca.rajatsethi.programming ; import java.util.HashMap ; import java.util.Map ; public class MapProductLookupTable implements ProductLookupTable { private final Map < Integer , Product > products = new HashMap <> (); @Override public Product lookupByID ( int id ) { return products . get ( id ); } @Override public void addProduct ( Product productToAdd ) { if ( products . containsKey ( productToAdd . getId ())){ throw new IllegalArgumentException ( \"Unable to add product, id already exists : \" + productToAdd . getId ()); } products . put ( productToAdd . getId (), productToAdd ); } @Override public void clear () { products . clear (); } } Methods for Maps put(K key, V value) --> if you add some key which is already there, it will update the existing value. putAll(Map\\<>) get(Object key) boolean containsKey(key) booleans containsValue(value) remove(key) clear() int size() boolean isEmpty() !!!note Map is the only collections that don\\'t extend or implement the Collection interface. Views Over Maps Similar to lists which have views over them i.e. created by subList . Even maps have views. The methods are given below. keySet() values() entrySet() The above methods are demonstrated in the below program. // ViewsOverMaps.java import java.util.Collection ; import java.util.HashMap ; import java.util.Map ; import java.util.Set ; public class ViewsOverMaps { public static void main ( String [] args ) { final Map < Integer , Product > products = new HashMap <> (); products . put ( 1 , new Product ( 1 , \"Door\" , 35 )); products . put ( 2 , new Product ( 2 , \"Window\" , 55 )); products . put ( 3 , new Product ( 3 , \"Frame\" , 75 )); System . out . println ( products ); System . out . println (); Set < Integer > ids = products . keySet (); System . out . println ( ids ); System . out . println (); // if you remove something from this set of ids now it also gets removed from the products map ids . remove ( 1 ); System . out . println ( ids ); System . out . println ( products ); System . out . println (); // values() Collection < Product > values = products . values (); System . out . println ( values ); System . out . println (); Set < Map . Entry < Integer , Product >> entries = products . entrySet (); System . out . println ( entries ); System . out . println (); for ( Map . Entry < Integer , Product > entry : entries ){ System . out . println ( entry ); if ( entry . getKey () == 2 ){ entry . setValue ( new Product ( entry . getKey (), \"Pipe\" , 10 )); // you can update the value for the entry } } System . out . println ( entries ); System . out . println (); } } Sorted and Navigable Maps SortedMap is supersceded by NavigableMap These will enforce order (usually by key in ascending order). Some of the methods of SortedMap are provided below : - firstKey() lastKey() This also has views over the Map tailMap(key) headMap(key) subMap(from key, to key) The key should be comparable or we need to provide a comparator. The NavigableMap add more features to sorted map. firstEntry() lastEntry() pollFirstEntry() --> removes and returns the first entry pollLastEntry() --> removes the last entry lowerEntry() higherEntry() lowerKey() higherKey() floorEntry(k) --> previous entry ceilingEntry(k) --> next entry for the provided key floorKey(k) ceilingKey(k) There are a lot of methods which were added by Java-8 to the Maps for ease of use. Below is examples of how to use them. // Java8Enhacements.Java package ca.rajatsethi.programming ; import java.util.HashMap ; import java.util.Map ; public class Java8Enhacements { public static void main ( String [] args ) { final Map < Integer , Product > products = new HashMap <> (); products . put ( 1 , new Product ( 1 , \"Door\" , 20 )); products . put ( 2 , new Product ( 2 , \"Window\" , 25 )); products . put ( 3 , new Product ( 3 , \"Frame\" , 30 )); Product defualtProduct = new Product ( - 1 , \"Default\" , 0 ); // getOrDefault --> If something is not there in the map it will get you default value. System . out . println ( products . getOrDefault ( 10 , defualtProduct )); // replace System . out . println ( products . replace ( 1 , new Product ( 1 , \"Big Door\" , 50 ))); // replaceAll --> with new products of weight 10 products . replaceAll (( id , oldProduct ) -> new Product ( id , oldProduct . getName (), oldProduct . getWeight () + 10 ) ); System . out . println ( products ); //computeIfAbsent -> creates new entry in map if the key is missing. Product result = products . computeIfAbsent ( 10 , ( id ) -> new Product ( id , \"Custom Product\" , 25 )); System . out . println ( result ); System . out . println ( products ); // with java-8 you can loop on the map itself using lambda expressions products . forEach (( key , value ) -> { System . out . println ( key + \" -> \" + value ); }); } } There are 3 general purpose maps HashMap LinkedHashMap TreeMap --> kind of balanced binary tree and is using red-black tree under the hood. There are 3 special purpose maps x x x HashMap General purpose Uses the .hashcode() Collections Operations This section contains details about the common operations provided by the Java collections class across all data structure types. Algorithms rotate() shuffle() -> rearrange randomly sort() Factories These are static methods on the collections class which will create a collection with some properties. Singletons singleton --> only contain single value. They are immutable. Empty Collections return immutable empty set, or list or map e.g. =Collections.emptySet(0)= or Collections.emptyList() These empty collections are useful when you want to pass no values to a method which takes in a collection. Unmodifyable collections Lets consider a code where we have a list. It returns the shopping list but then someone from the main function adds something to it thus modifying the shopping list as shown below. package ca.rajatsethi.programming ; import java.util.ArrayList ; import java.util.List ; public class ShoppingBasket { private final List < Product > products = new ArrayList <> (); private int total_weight = 0 ; public void add ( Product product ){ products . add ( product ); total_weight += product . getWeight (); } public List < Product > getItems (){ return products ; } @Override public String toString () { return \"Shopping basket of \" + products + \"with weight of \" + total_weight + \" kg\" ; } public static void main ( String [] args ) { ShoppingBasket s1 = new ShoppingBasket (); s1 . add ( new Product ( 1 , \"Apple\" , 10 )); System . out . println ( s1 ); s1 . getItems (). add ( new Product ( 2 , \"Banana\" , 2 )); System . out . println ( s1 ); } } You will see that because the item is added in the main function the weight is not updated. //output Shopping basket of [{ 1, Apple, 10 }]with weight of 10 kg Shopping basket of [{ 1, Apple, 10 }, { 2, Banana, 2 }] with weight of 10 kg In this case its better to return a collection which is unmodifyable. So that no one can add to the list in the main funciton. public List < Product > getItems (){ return Collections . unmodifiableList ( products ); } Utility Methods of Collection Methods Instead of adding products one by one you should use the below : - Collections . addAll ( products , door , window , frame ); Collections.min() Collections.max() Misc Converting between collections and arrays Sometimes you need to convert collection to array. Use toArray() method. If you do not provide any parameters it will return an array of type object . Another version of the toArray(T[] array) accepts an array of type specified, this will return the type which you specified. Arrays can also be retrived as collection. Use Array class asList method. // Collection to array Myclass [] a1 = list . toArray ( new MyClass [ 0 ] ); // Array to collection Collection < MyClass > list = Arrays . asList ( myArray );","title":"3. Collections"},{"location":"Java/3.%20Collections/#problems-with-arrays","text":"Cannot resize very low level concept do not provide many functionality like add, duplicacy check etc... With below example I try to show the problems with arrays. lets say we have a simple class Product and we are doing some operations on it in the main program below // Product.java package ca.rajatsethi.programming ; public class Product { private String name ; private int price ; public Product ( String name , int price ) { this . name = name ; this . price = price ; } @Override public String toString () { return \"Product{\" + this . name + \",\" + this . price + \"}\" ; } } // main.java package ca.rajatsethi.programming ; import java.lang.reflect.Array ; import java.util.Arrays ; public class Main { public static void main ( String [] args ) { Product door = new Product ( \"Wooden Door\" , 35 ); Product window = new Product ( \"Wooden Window\" , 15 ); Product [] products = { door , window }; System . out . println ( Arrays . toString ( products )); // print out in human legible format. // https://docs.oracle.com/javase/9/docs/api/java/util/Arrays.html#toString-java.lang.Object:A- // just to add we have to implement another function as arrays do not resize. Product handle = new Product ( \"Handle\" , 4 ); products = add ( products , handle ); // arrays also do not check for duplicate, you can add handle again. products = add ( products , handle ); System . out . println ( Arrays . toString ( products )); } // we wish to add another product to the products array. The problem is that the array cannot be resized. // so lets create another method which adds functionality to add another product to the array. public static Product [] add ( Product [] array , Product product ){ int length = array . length ; Product [] newArray = Arrays . copyOf ( array , length + 1 ); newArray [ length ] = product ; return newArray ; } } // Console output [ Product { Wooden Door , 35 }, Product { Wooden Window , 15 } ] [ Product { Wooden Door , 35 }, Product { Wooden Window , 15 }, Product { Handle , 4 }, Product { Handle , 4 } ] Java Collections Java collections ships with the JDK Any application in Java (which is not very basic) will use the collections frameworks. The data structures are diverse Some of them provide ordered access. Some of them provide uniqueness. Some of them provide ability for pairs.","title":"Problems with arrays"},{"location":"Java/3.%20Collections/#defining-collections","text":"Collection of Collections All the java interfaces which deal with collections extend the Collection . Types of collections Lists (Array list and linked list) Sets (Hash Set) Sorted Set (Tree set) Queue (Priority queue) and Deque or double ended queue (Linked list and Array Deque) Map (Hash map) and Sorted Map (Treemap) Lists is interface its implementation is array list and linked list. Same as others. Outside brackets --> interfaces, inside brackets --> their specific implementations. The interface will drive their charactersistics, but for 1 interface there can be multiple implementations e.g. for list we have 2. Collection Behaviour All collections extend the iterable interface. Some of the other methods in the collection interface are shown below. size() isEmpty() add(element) addAll(collection) remove(element) removeAll(collection) retainall(collection) contains(element) containsAll(collection) clear() Example of working with collections (in this case we are using ArrayList ) is given below. // Product.java public class Product { private String name ; private int price ; public Product ( String name , int price ) { this . name = name ; this . price = price ; } @Override public String toString () { return \"Product{\" + this . name + \",\" + this . price + \"}\" ; } public int getPrice () { return price ; } } // main.java import java.util.ArrayList ; import java.util.Collection ; import java.util.Iterator ; public class Main { public static void main ( String [] args ) { Product door = new Product ( \"Door\" , 15 ); Product floorPannel = new Product ( \"Floor Pannel\" , 60 ); Product window = new Product ( \"Window\" , 30 ); Collection < Product > products = new ArrayList <> (); // creating a collection of products <T> -> <Product> // now that our collection is created we can add our products to the `products` collection. products . add ( door ); products . add ( floorPannel ); products . add ( window ); // printing out the whole collection System . out . println ( products ); // iterating on collection using for loop. for ( Product product : products ){ System . out . println ( product ); } // iterating on collection the long way. // this way is useful if you wish to modify the collection while looping on it // e.g. if you wish to remove product from the products collection while looping on it as shown below. final Iterator < Product > productIterator = products . iterator (); while ( productIterator . hasNext ()){ Product product = productIterator . next (); if ( product . getPrice () > 20 ) { System . out . println ( product ); } else { productIterator . remove (); // if price is less than that then remove from the collection. } } // printing out the whole collection to confirm its removed. System . out . println ( products ); // trying other methods System . out . println ( \"Is collection empty : \" + products . isEmpty ()); System . out . println ( \"Collection size : \" + products . size ()); System . out . println ( \"Contains floorPannel : \" + products . contains ( floorPannel )); } }","title":"Defining collections"},{"location":"Java/3.%20Collections/#lists","text":"There are 2 types of lists. Arraylist Linked List In the example below we are using ArrayList and wrapping it in out Shipment class which represents something in our domain. (This is a common practice). The shipment class will have functions which make sense in the real world and in the background it will use Arraylist. // Product.Java package ca.rajatsethi.programming ; import java.util.Comparator ; public class Product { // Attributes of the Product Class private String name ; private int weight ; // Constructor public Product ( String name , int weight ) { this . name = name ; this . weight = weight ; } // Getters public int getWeight () { return weight ; } // String representation of product @Override public String toString () { return \"Product{ \" + this . name + \" , \" + this . weight + \"}\" ; } // implementing the comparator public static final Comparator < Product > BY_WEIGHT = Comparator . comparing ( Product :: getWeight ); } // Shipment.Java package ca.rajatsethi.programming ; import java.util.ArrayList ; import java.util.Iterator ; import java.util.List ; public class Shipment implements Iterable < Product > { private static final int PRODUCT_NOT_PRESENT = - 1 ; private static final int SMALL_VAN_MAX_ITEM_WEIGHT = 20 ; private List < Product > products = new ArrayList <> (); // products arraylist private List < Product > small_van_products = new ArrayList <> (); private List < Product > large_van_products = new ArrayList <> (); // Getters public List < Product > getSmall_van_products () { return small_van_products ; } public List < Product > getLarge_van_products () { return large_van_products ; } // Iterator @Override public Iterator < Product > iterator () { return products . iterator (); // uses the built in list iterator. } // Adding product public void add ( Product p ) { products . add ( p ); } // replacing the product public void replace ( Product oldProduct , Product newProduct ) { int oldProductIndex = products . indexOf ( oldProduct ); if ( oldProductIndex != PRODUCT_NOT_PRESENT ) { products . set ( oldProductIndex , newProduct ); } } // Seperating products into different Vans public void prepare () { products . sort ( Product . BY_WEIGHT ); // sort by weight ascending. int splitPoint = findSplitPoint (); small_van_products = products . subList ( 0 , splitPoint ); large_van_products = products . subList ( splitPoint , products . size ()); } // Helper product private int findSplitPoint () { for ( Product p : products ){ if ( p . getWeight () > SMALL_VAN_MAX_ITEM_WEIGHT ){ return products . indexOf ( p ); } } return 0 ; } } // main.java package ca.rajatsethi.programming ; public class Main { public static void main ( String [] args ) { Shipment ship = new Shipment (); Product floorPannel = new Product ( \"Floor Pannel\" , 30 ); Product window = new Product ( \"Window\" , 10 ); Product door = new Product ( \"Door\" , 45 ); ship . add ( floorPannel ); ship . add ( window ); ship . add ( door ); // Because we implemented the iterable on the Product for the // Shipment class we can iterate over it. for ( Product p : ship ){ System . out . println ( p ); } ship . prepare (); System . out . println ( \"Small Van products = \" + ship . getSmall_van_products ()); System . out . println ( \"Large Van products = \" + ship . getLarge_van_products ()); } } Another example below which shows how to create your own list on top of Array (with creating own version of iteration and not using an existing iterator like ArrayList iterator) capability. /* Java Iterable and Iterator Example. */ import java.util.Iterator ; class GenericList implements Iterable < Integer > { int [] mylist = new int [ 10 ] ; int position = 0 ; void push ( int value ){ mylist [ position ++] = value ; } int pop (){ return mylist [-- position ] ; } @Override public Iterator < Integer > iterator () { return new CustomIterator ( this ); } public class CustomIterator implements Iterator < Integer > { GenericList list ; private int pointer = 0 ; CustomIterator ( GenericList list ){ this . list = list ; } @Override public boolean hasNext () { return pointer < list . position ; } @Override public Integer next () { return list . mylist [ pointer ++] ; } } } public class Main10 { public static void main ( String [] args ) { GenericList lst = new GenericList (); lst . push ( 10 ); lst . push ( 20 ); for ( int value : lst ){ System . out . println ( value ); } } }","title":"Lists"},{"location":"Java/3.%20Collections/#sets","text":"There are the following types of set implementations which are avaiable to us. HashSet TreeSet EnumSet (Designed to be efficient when dealing with enum types) Hash Set These are based on HashMap. (Calls hashCode() on element and looks up the location). Hash Sets are good general purpose implementations. They resize when run out of space. How equals works Standard java checks the equals using the hash code. If 2 objects hashcode value is same then they are equals. i.e. they are the same object. Your implementation of equals can differ and you will have to override it. object . hashCode () == object . hashCode () Tree Set Tree set is based on tree map. Similarly hash set was based on hash map. Uses sorted binary tree. It keeps the elements in specified order. (it implements SortedSet and NavigableSet ) Enum Set Only allow to store enum objects. Uses a bitset based on ordinal of the enum. Two other interfaces mentioned below which extend the behaviour of Sets. They talk about enforcing orders. SortedSet NavigableSet SortedSet E first() E last() SortedSet tailSet(E fromElement) SortedSet headSet(E toElement) SortedSet subSet(E fromElement, E toElement) NavigableSet This extends sortedSet and is implemented by Treeset. E lower(E e) E higher(E e) E floor(E e) E cieling(E e) E pollFirst() E pollLast()","title":"Sets"},{"location":"Java/3.%20Collections/#queues-deque-and-stacks","text":"Queue First In First Out Methods offer() --> Use offer method instead of add() when adding to queue. As some of the queues are bounded (max in queue) and if you use add() method to add something to queue when its full it throws an exception. offer() will just return false. poll() --> remove and return value. The remove() method throws exception when queue is empty and you wish to remove something so instead of using it use poll() method. peek() --> use peek, element() throws exception when empty, peek returns null. Basic example of Queue provided below. The below queue has been implemented using the LinkedList implementation. import java.util.LinkedList ; import java.util.Queue ; public class Main { public static void main ( String [] args ) { // Instanciating a new queue of type linked list Queue < Integer > q = new LinkedList <> (); // adding elements to queue q . add ( 10 ); q . add ( 20 ); // printing out the queue System . out . println ( q ); // get first element System . out . println ( q . element ()); System . out . println ( q ); // iterating over queue for ( int i : q ) { System . out . println ( i ); } // remove element from the queue System . out . println ( \"Remove element from queue : \" + q . remove ()); System . out . println ( q ); } } A more real world example of helpdesk implemented with queue is provided below. // Category.java public enum Category { PRINTER , COMPUTER , PHONE , TABLET } //Customer.java public class Customer { private final String name ; public Customer ( String name ) { this . name = name ; } public void reply ( final String message ) { System . out . println ( this . name + \" : \" + message ); } public static final Customer JACK = new Customer ( \"Jack\" ); public static final Customer JILL = new Customer ( \"Jill\" ); public static final Customer MARY = new Customer ( \"Mary\" ); } // Enquiry.java public class Enquiry { private final Customer customer ; private final Category category ; public Enquiry ( Customer customer , Category category ) { this . customer = customer ; this . category = category ; } public Customer getCustomer () { return customer ; } public Category getCategory () { return category ; } @Override public String toString () { return \"Enquiry{customer = \" + customer + \", category = \" + category + \"}\" ; } } //Helpdesk.java import java.util.ArrayDeque ; import java.util.Queue ; public class HelpDesk { private final Queue < Enquiry > enquiries = new ArrayDeque <> (); public void eqnuire ( final Customer customer , Category category ) { enquiries . offer ( new Enquiry ( customer , category )); } public void processAllEnquiries () { Enquiry enquiry ; while (( enquiry = enquiries . poll ()) != null ) { enquiry . getCustomer (). reply ( \"Have you tried turning if off and on again?\" ); } } public static void main ( String [] args ) { HelpDesk helpDesk = new HelpDesk (); helpDesk . eqnuire ( Customer . JACK , Category . PHONE ); helpDesk . eqnuire ( Customer . JILL , Category . PRINTER ); helpDesk . processAllEnquiries (); } } Using Priority Queue We can actually use priority queue to sort the enquiry using some priority. In the Helpdesk.java class we will now implement priorityQueue. Helpdesk . java import java.util.ArrayDeque ; import java.util.Comparator ; import java.util.PriorityQueue ; import java.util.Queue ; /** * Created by sethir on 2019/02/01. */ public class HelpDesk { private final Queue < Enquiry > enquiries = new PriorityQueue <> ( BY_CATEGORY ); // creating a priority queue by using category comparator. public void eqnuire ( final Customer customer , Category category ) { enquiries . offer ( new Enquiry ( customer , category )); } public static final Comparator < Enquiry > BY_CATEGORY = new Comparator < Enquiry > () { // implementing comparator. @Override public int compare ( Enquiry o1 , Enquiry o2 ) { return o1 . getCategory (). compareTo ( o2 . getCategory ()); } }; public void processAllEnquiries () { Enquiry enquiry ; while (( enquiry = enquiries . poll ()) != null ) { enquiry . getCustomer (). reply ( \"Have you tried turning if off and on again?\" ); } } public static void main ( String [] args ) { HelpDesk helpDesk = new HelpDesk (); helpDesk . eqnuire ( Customer . JACK , Category . TABLET ); helpDesk . eqnuire ( Customer . JILL , Category . PRINTER ); helpDesk . eqnuire ( Customer . MARY , Category . PHONE ); helpDesk . processAllEnquiries (); } } The result is sorted based on the enum ordering. // output Jill : Have you tried turning if off and on again ? Mary : Have you tried turning if off and on again ? Jack : Have you tried turning if off and on again ? Stack and Deque Stacks are Last In - First Out Java.util.stack is deprecated and should not be used. You should be using Deque, they are the correct way to use stacks. Deque (double ended queue) --> Use 2 ends. Below are the methods which we can use for Deque to implement stack. boolean offerFirst(E e) boolean offerLast(E e) void addFirst(E e) void addLast(E e) E removeFirst() E removeLast() E pollFirst() E pollLast() E getFirst() E getLast() E peekFirst() E peekLast() If the above methods are confuing . void Push(E e) void pop() Example of Calculator provided below. // Calculator.Java package ca.rajatsethi.programming ; import java.util.ArrayDeque ; import java.util.Deque ; public class Calculator { public int evaluate ( final String input ) { final Deque < String > stack = new ArrayDeque <> (); final String [] tokens = input . split ( \" \" ); for ( String token : tokens ) { stack . push ( token ); } while ( stack . size () > 1 ) { int left = Integer . parseInt ( stack . pop ()); String operator = stack . pop (); int right = Integer . parseInt ( stack . pop ()); int result = 0 ; switch ( operator ) { case \"+\" : result = left + right ; break ; case \"-\" : result = left - right ; break ; } stack . push ( String . valueOf ( result )); } return Integer . parseInt ( stack . pop ()); } } // Main.java package ca.rajatsethi.programming ; public class Main { public static void main ( String [] args ) { Calculator calculator = new Calculator (); System . out . println ( calculator . evaluate ( \"1 + 2\" )); System . out . println ( calculator . evaluate ( \"1 + 2 - 11 - 12 - 18 + 109\" )); System . out . println ( calculator . evaluate ( \"1 + 6\" )); } } // output 3 103 7","title":"Queues Deque and Stacks"},{"location":"Java/3.%20Collections/#maps","text":"Maps are key value pairs like dictionaries in Python. A quick example with maps is given below. // product.java package ca.rajatsethi.programming ; import java.util.Comparator ; public class Product { private int id ; private String name ; private int weight ; public Product ( int id , String name , int weight ) { this . id = id ; this . name = name ; this . weight = weight ; } public int getId () { return id ; } public String getName () { return name ; } public int getWeight () { return weight ; } // Implementing comparator public static final Comparator < Product > BY_WEIGHT = Comparator . comparing ( Product :: getWeight ); public static final Comparator < Product > BY_NAME = Comparator . comparing ( Product :: getName ); } // ProductLookupTable.java --> Interface package ca.rajatsethi.programming ; public interface ProductLookupTable { Product lookupByID ( int id ); void addProduct ( Product productToAdd ); void clear (); } We first see how we do things without a map. The below is using lists. // NaiveProductLookupTable.Java package ca.rajatsethi.programming ; import java.util.ArrayList ; import java.util.List ; public class NaiveProductLookupTable implements ProductLookupTable { private List < Product > products = new ArrayList <> (); @Override public Product lookupByID ( int id ) { for ( Product product : products ){ if ( product . getId () == id ){ return product ; } } return null ; } @Override public void addProduct ( Product productToAdd ) { for ( Product product : products ){ if ( product . getId () == productToAdd . getId ()){ throw new IllegalArgumentException ( \"Unable to add : duplicate id : \" + product . getId ()); // throwing exception because we found a duplicate id } } products . add ( productToAdd ); } @Override public void clear () { products . clear (); } } Now that we have seen list, we do the same using maps below. The code is cleaner and performs much better. // MapProductLookupTable.Java package ca.rajatsethi.programming ; import java.util.HashMap ; import java.util.Map ; public class MapProductLookupTable implements ProductLookupTable { private final Map < Integer , Product > products = new HashMap <> (); @Override public Product lookupByID ( int id ) { return products . get ( id ); } @Override public void addProduct ( Product productToAdd ) { if ( products . containsKey ( productToAdd . getId ())){ throw new IllegalArgumentException ( \"Unable to add product, id already exists : \" + productToAdd . getId ()); } products . put ( productToAdd . getId (), productToAdd ); } @Override public void clear () { products . clear (); } } Methods for Maps put(K key, V value) --> if you add some key which is already there, it will update the existing value. putAll(Map\\<>) get(Object key) boolean containsKey(key) booleans containsValue(value) remove(key) clear() int size() boolean isEmpty() !!!note Map is the only collections that don\\'t extend or implement the Collection interface. Views Over Maps Similar to lists which have views over them i.e. created by subList . Even maps have views. The methods are given below. keySet() values() entrySet() The above methods are demonstrated in the below program. // ViewsOverMaps.java import java.util.Collection ; import java.util.HashMap ; import java.util.Map ; import java.util.Set ; public class ViewsOverMaps { public static void main ( String [] args ) { final Map < Integer , Product > products = new HashMap <> (); products . put ( 1 , new Product ( 1 , \"Door\" , 35 )); products . put ( 2 , new Product ( 2 , \"Window\" , 55 )); products . put ( 3 , new Product ( 3 , \"Frame\" , 75 )); System . out . println ( products ); System . out . println (); Set < Integer > ids = products . keySet (); System . out . println ( ids ); System . out . println (); // if you remove something from this set of ids now it also gets removed from the products map ids . remove ( 1 ); System . out . println ( ids ); System . out . println ( products ); System . out . println (); // values() Collection < Product > values = products . values (); System . out . println ( values ); System . out . println (); Set < Map . Entry < Integer , Product >> entries = products . entrySet (); System . out . println ( entries ); System . out . println (); for ( Map . Entry < Integer , Product > entry : entries ){ System . out . println ( entry ); if ( entry . getKey () == 2 ){ entry . setValue ( new Product ( entry . getKey (), \"Pipe\" , 10 )); // you can update the value for the entry } } System . out . println ( entries ); System . out . println (); } } Sorted and Navigable Maps SortedMap is supersceded by NavigableMap These will enforce order (usually by key in ascending order). Some of the methods of SortedMap are provided below : - firstKey() lastKey() This also has views over the Map tailMap(key) headMap(key) subMap(from key, to key) The key should be comparable or we need to provide a comparator. The NavigableMap add more features to sorted map. firstEntry() lastEntry() pollFirstEntry() --> removes and returns the first entry pollLastEntry() --> removes the last entry lowerEntry() higherEntry() lowerKey() higherKey() floorEntry(k) --> previous entry ceilingEntry(k) --> next entry for the provided key floorKey(k) ceilingKey(k) There are a lot of methods which were added by Java-8 to the Maps for ease of use. Below is examples of how to use them. // Java8Enhacements.Java package ca.rajatsethi.programming ; import java.util.HashMap ; import java.util.Map ; public class Java8Enhacements { public static void main ( String [] args ) { final Map < Integer , Product > products = new HashMap <> (); products . put ( 1 , new Product ( 1 , \"Door\" , 20 )); products . put ( 2 , new Product ( 2 , \"Window\" , 25 )); products . put ( 3 , new Product ( 3 , \"Frame\" , 30 )); Product defualtProduct = new Product ( - 1 , \"Default\" , 0 ); // getOrDefault --> If something is not there in the map it will get you default value. System . out . println ( products . getOrDefault ( 10 , defualtProduct )); // replace System . out . println ( products . replace ( 1 , new Product ( 1 , \"Big Door\" , 50 ))); // replaceAll --> with new products of weight 10 products . replaceAll (( id , oldProduct ) -> new Product ( id , oldProduct . getName (), oldProduct . getWeight () + 10 ) ); System . out . println ( products ); //computeIfAbsent -> creates new entry in map if the key is missing. Product result = products . computeIfAbsent ( 10 , ( id ) -> new Product ( id , \"Custom Product\" , 25 )); System . out . println ( result ); System . out . println ( products ); // with java-8 you can loop on the map itself using lambda expressions products . forEach (( key , value ) -> { System . out . println ( key + \" -> \" + value ); }); } } There are 3 general purpose maps HashMap LinkedHashMap TreeMap --> kind of balanced binary tree and is using red-black tree under the hood. There are 3 special purpose maps x x x HashMap General purpose Uses the .hashcode()","title":"Maps"},{"location":"Java/3.%20Collections/#collections-operations","text":"This section contains details about the common operations provided by the Java collections class across all data structure types. Algorithms rotate() shuffle() -> rearrange randomly sort() Factories These are static methods on the collections class which will create a collection with some properties. Singletons singleton --> only contain single value. They are immutable. Empty Collections return immutable empty set, or list or map e.g. =Collections.emptySet(0)= or Collections.emptyList() These empty collections are useful when you want to pass no values to a method which takes in a collection. Unmodifyable collections Lets consider a code where we have a list. It returns the shopping list but then someone from the main function adds something to it thus modifying the shopping list as shown below. package ca.rajatsethi.programming ; import java.util.ArrayList ; import java.util.List ; public class ShoppingBasket { private final List < Product > products = new ArrayList <> (); private int total_weight = 0 ; public void add ( Product product ){ products . add ( product ); total_weight += product . getWeight (); } public List < Product > getItems (){ return products ; } @Override public String toString () { return \"Shopping basket of \" + products + \"with weight of \" + total_weight + \" kg\" ; } public static void main ( String [] args ) { ShoppingBasket s1 = new ShoppingBasket (); s1 . add ( new Product ( 1 , \"Apple\" , 10 )); System . out . println ( s1 ); s1 . getItems (). add ( new Product ( 2 , \"Banana\" , 2 )); System . out . println ( s1 ); } } You will see that because the item is added in the main function the weight is not updated. //output Shopping basket of [{ 1, Apple, 10 }]with weight of 10 kg Shopping basket of [{ 1, Apple, 10 }, { 2, Banana, 2 }] with weight of 10 kg In this case its better to return a collection which is unmodifyable. So that no one can add to the list in the main funciton. public List < Product > getItems (){ return Collections . unmodifiableList ( products ); } Utility Methods of Collection Methods Instead of adding products one by one you should use the below : - Collections . addAll ( products , door , window , frame ); Collections.min() Collections.max()","title":"Collections Operations"},{"location":"Java/3.%20Collections/#misc","text":"Converting between collections and arrays Sometimes you need to convert collection to array. Use toArray() method. If you do not provide any parameters it will return an array of type object . Another version of the toArray(T[] array) accepts an array of type specified, this will return the type which you specified. Arrays can also be retrived as collection. Use Array class asList method. // Collection to array Myclass [] a1 = list . toArray ( new MyClass [ 0 ] ); // Array to collection Collection < MyClass > list = Arrays . asList ( myArray );","title":"Misc"},{"location":"Java/4.%20The%20core%20platform/","text":"Core platform and libraries Streams Streams in java are ordered input and output data It provides a common input and output model Streams are unidirectional Abstracts away the underline source of destination of the data (abstracts storange or where data is coming from) You can have input stream you can have output stream stream can be either a binary stream or character stream Reading streams Binary (The base class for this is InputStream) Text (The base class for this is Reader) There are various methods which are used when reading stream data read() --> reads 1 byte at a time read(byte[] buff) or read(char[] buff) --> read the data which can fit into buff If you see the below examples it may look like we are creating instances of InputStream, OutputStream, Reader or Writer. These are however abstract classes and cannot be instanciated. There are various subclasses of these which will be inherited based on the type of stream e.g. CharArrayWriter, StringWriter, PipedWriter, InputStreamReader, OutputStreamWriter etc... // Reading one byte(binary) at a time. InputStream input = // create input stream int intVal while (( intVal = input . read ()) >= 0 ){ // end of stream is indicated by .read() method returning -1 byte byteVal = ( byte ) intVal ; // now you can do something with byte value. } // Reading one character at a time. Reader input = // create input stream int intVal while (( intVal = input . read ()) >= 0 ){ // end of stream is indicated by .read() method returning -1 byte byteVal = ( chat ) intVal ; // now you can do something with chat value. } // Reading multiple byte(binary) at a time. InputStream input = // create input stream int length ; byte [] byteBuff = new byte [ 10 ] ; while (( length = input . read ()) >= 0 ){ // end of stream is indicated by .read() method returning -1 for ( int i = 0 ; i < length ; i ++ ){ // now we have array of byte, so loop on array to get single byte and do something with it. byte byteVal = byteBuff [ i ] ; // do something with byteval. } } Writing values OutputSteam output = // create stream byte value = 100 ; output . write ( value ); byte [] byteBuff = { 0 , 63 , 127 }; output . write ( byteBuff ); Writer output = //create stream char a = 'A' ; // can write single char output . write ( a ); char [] b = { 'a' , 'b' }; // can write array of char output . write ( b ); String c = \"HEllo\" ; // can write a string output . write ( c ); Usually when you are dealing with streams you will be using try and catch or try with resources and then closing down the stream as shown below. try { reader = // open reader; } catch ( IOException e ){ } finally (){ if ( reader != null ){ try { reader . close (); } catch (){} } } Use try with resources The above close gets complicated (its still simplified on top) so use try with resources. Try with resources handles the following exceptions Automate cleans ups of 1 or more resources Handle try body Also handles close method calls and exceptions try ( reader = //open reader ){} catch {} DEMO Check stream --> slides and demos. The interface AutoCloseable has a method close. The try and catch block will auto call this method close() even if its not called explicitly so the resources are freed up. Also when you use try with resources you may get only 1 exception however there may be other exceptions which are not printed out. So in that case use method getSuppressed and you can loop on this and print out all the suppressed exceptions. Chaining Streams You can chain streams together. Java provdes FilterReader, FilterWriter, FilterInputSteam and FilterOutputStream abstract classes which provide easier customization for chaining streams. Interacting with Files java.io classes are depricated use java.nio.file for file handling. Use Paths class to locate something in the file system or directory. Use Files static method to interacting with files. Create, copy delete etc... Open file streams like newBufferedReader, newBufferedWriter, newInputStream, newOutputStream Read/write file contents using readAllLines, write (If you see in the pluralSight course this below example was not accepted by Java, so modified it a little and it works) import java.io.BufferedReader ; import java.io.IOException ; import java.nio.file.Files ; import java.nio.file.Paths ; public class ReadFiles { public static void main ( String [] args ) throws IOException { try ( BufferedReader br = Files . newBufferedReader ( Paths . get ( \"file.txt\" )); ){ String data ; while (( data = br . readLine ()) != null ){ System . out . println ( data ); } } } } Working with File Systems When we work with file systems we usually work with computers default file system. java also supports specialized file systems like Zip file system. So we use the File System Type which represents the file system. Class --> FileSystem Check slide and programs for additional references. (Code demos are avaiable there) Strings Basic concatenation and Stringbuilder are not powerful enough to create complicated strings. so we will be looking at more powerful ways. StringJoinr --> Simplifies joining a sequence of values. String formatting --> used for desired appearance. String Joiner import java.util.StringJoiner ; public class StringJoinerDemo { public static void main ( String [] args ) { StringJoiner sj = new StringJoiner ( \", \" ); // specify the delimiter sj . add ( \"alpha\" ); sj . add ( \"beta\" ); sj . add ( \"gama\" ); System . out . println ( sj . toString ()); // output --> alpha, beta, gama sj . add ( \"abc\" ). add ( \"def\" ); // chaining methods --> return type of add is StringJoiner System . out . println ( sj . toString ()); // output --> alpha, beta, gama, abc, def StringJoiner sj1 = new StringJoiner ( \"], [\" , \"[\" , \"]\" ); sj1 . add ( \"alpha\" ); sj1 . add ( \"beta\" ); System . out . println ( sj1 . toString ()); // [alpha], [beta] } } Format Specifiers StringJoiner is pretty powerful however we sometimes need more power :) Focus is on how things look Not concerned with how. Use methods like --> String.format or System.out.printf(), Formatter.format Parts of a format specifier --> %[argument index][flags][width][precesion]conversoin Common Format Conversions d - decimal o - octal x = Hex f - float e - Scientific Notation s - String (you can also pass objects not just strings here, if the class has Formattable then it will be used else will go with toString) .... Format Flags - # - Include radix - 0 - 0 pad value - - - left justify - , - Include grouping character - space - Leave space for + or - sign for positive numbers - + - always show signs You can also write formatted text to a stream not just on output screen. There is a class called the Formatter class which provides formatting capabilities. It can help us write formatted content to any class which implements the Appendable interface. The writer stream class implements the Appendable interface. e.g. BufferedWriter writer = new Files . newBufferedWriter ( Paths . get ( \"data.txt\" )); // it will close when formatter is closed. try ( Formatter f = new Formatter ( writer )){ // creating a formatter over the writer stream f . format ( \"Hello %d\" , 10 ); } Regular Expressions Just basics here --> check other places for detials String s1 = \"apple, apple and orange please\" ; String s2 = s1 . replaceAll ( \"ple\\\\b\" , \"ricot\" ); String s1 = \"apple, apple and orange please\" ; String [] parts = s1 . split ( \"//b\" ); for ( String part : parts ){ if ( part . matches ( \"\\\\w+\" )){ System . out . println ( part ); } } Compilation of regular expression can be very processing intensive, so if you are doing this in a loop or over and over again its better to precompile the regular expression and apply. The Pattern class allows us to precompile the regex and then apply. Then the Matcher class can apply the compiled regex to an expression/string. String s1 = \"apple, apple and orange please\" ; Pattern pattern = Pattern . compile ( \"\\\\w+\" ); Matcher matcher = pattern . matcher ( value1 ); while ( matcher . find ()){ System . out . println ( matcher . group ()); } Controlling App Execution Command Line Arguments Arguments are passed as String arrays. Each argument is a seperate element. (seperated by space or if space put in quotes). In intellij you can pass arguemnts when you edit configurations package com.sethirajat.cli ; public class CLIargsDemo { public static void main ( String [] args ) { for ( String arg : args ) { System . out . println ( arg ); } } } Persistable Key Value Pairs Apps often need persistable key value pairs for app config or initial load or other things like state or preferences. (Hashmap will only store when we are running program and its in memory). Use the java.util.Properties class for this. Properties Class --> Inherits from Hash Table. Keys and values are string. properties can be written to and from streams can optionally include comments supports 2 formats --> text and xml key and value are separated by : or , or first white space #=or =! start comment line import java.io.* ; import java.nio.file.Files ; import java.nio.file.Paths ; import java.util.InvalidPropertiesFormatException ; import java.util.Properties ; public class PropertiesDemo { public static void main ( String [] args ) { // properties_reader(); // properties_writer_xml(); properties_reader_xml (); } public static void properties_writer () { Properties props = new Properties (); props . setProperty ( \"key1\" , \"value1\" ); System . out . println ( props . getProperty ( \"key1\" )); try ( Writer writer = Files . newBufferedWriter ( Paths . get ( \"abc.properties\" ))) { props . store ( writer , \"Sample properties\" ); } catch ( IOException e ) { e . printStackTrace (); } } static void properties_reader (){ Properties props = new Properties (); try ( Reader reader = Files . newBufferedReader ( Paths . get ( \"abc.properties\" ))){ props . load ( reader ); System . out . println ( props . getProperty ( \"key1\" )); } catch ( IOException e ) { e . printStackTrace (); } } // xml will work with output streams, so the output stream has to be stored as xml. static void properties_writer_xml (){ Properties props = new Properties (); try ( OutputStream out = Files . newOutputStream ( Paths . get ( \"abc.xml\" ))){ props . setProperty ( \"key1\" , \"value1\" ); props . storeToXML ( out , \"sample properties\" ); } catch ( IOException e ) { e . printStackTrace (); } } static void properties_reader_xml (){ Properties props = new Properties (); try ( InputStream in = Files . newInputStream ( Paths . get ( \"abc.xml\" ))){ props . loadFromXML ( in ); System . out . println ( props . getProperty ( \"key1\" )); } catch ( InvalidPropertiesFormatException e ) { e . printStackTrace (); } catch ( IOException e ) { e . printStackTrace (); } } } Default Properties Properties can also be created with default. static void propertiesWithDefault (){ Properties defaults = new Properties (); defaults . setProperty ( \"os\" , \"Windows\" ); Properties props = new Properties ( defaults ); // created with defaults } Usually a application will store default properties. For that when you are launching an aplication you can include .properties file in that application. Default Class Loading Classes must be in .class files Must be under package directory Specifying class path You can specify the class path. (if you specify path then current directory has to be specified ) if doing via env variables use CLASSPATH classpath is set for all programs and projects not just for current project Class loading with -jar option Class loading is controlled by jar file no other class loading source is used provides tight control over class loading e.g. =java -jar ourapp.jar= Java Log System Logs are used for various uses. errors usage info debug can be of different detail level Log System its centrally managed there is 1 app-wide log manager manages log system config manages objects that do actual logging class LogManager . There will be 1 global instance of it. LogManager.getLogManager Logger class provides methods to do logging use getLogger method to get to the logger from the log manager. each logger instance is named there is also a global logger GLOBAL_LOGGER_NAME Levels Each log entry is associated with a level Each logger has a capture level --> use setLevel method. The logger will ignore any entry below that level Each level has a numeric value 7 basic log levels 2 special levels for Logger you can also define custom log levels (its rare) 1000 - SEVERE 900 - WARNING 800 - INFO 700 - CONFIGURATION 500 - FINE 400 - FINER 300 - FINEST entering - logs at fine level exiting - logs at fine level Components of the log system Logger -> Accepts app calls Handler -> Publishes logging info, a logger can have multiple handlers As logger can have multiple handlers you can set level for each handler. (which should be more restrictive than the logger level) Formatter -> formats log info for publication. each handler has 1 formatter. (check slides as they have a good diagram to explain this) Built in Handlers The built in handlers inherit from the Handler class You can write custom handlers but mostly you will not ConsoleHandler --> writes to System.err StreamHandler --> writes to specified output stream SocketHandler --> writes to network socket FileHandler --> writes to 1 or more files can output to single file can output to rotating set of files specify size in bytes File Handler Substitution patterns values (check demo below) / - Platform specific slash e.g. ./foo.txt \\%T - writes to temp directory \\%h - writes to home directory \\%g - Rotating log generation Built in formatters Inherit from formatter class XMLFormatter Root element log each entry goes under named record SimpleFormatter Formats content as simple text Format is customizable Uses standard formatting notation You can customize this using java.util.logging.SimpleFormatter.format -> pass value with Java -D option when run the program (check slides) Log Configuration File You can create a configuration file for your log system. Using configuration file is much simpler. java.util.logging.ConsoleHandler.level = ALL ... (check slide) then when you launch the program launch with giving file details java - Djava . util . logging . config . file = log . properties com . pluralsight . training . Main // (the last one is is the app name com.plu....) Logger Naming Implies a parent child relationship based on the name we give the loggers. Naming should be hierarchical dot seperates a level generally tied to class\\'s full name e.g. =com.sethirajat.training= com.sethirajat.training.Main com.sethirajat.training.Student . In the above case Main and Student will auto become child loggers of com.sethirajat.training and any log on the child will be logged on the parent also. Making the most of hierarchical system If a logger level is null it inherits parents level So we primarily set level on parents (and is usually restrictive) if we need more detail then we can set more detailed level on the child logger each logger also does not need to have a handler if there is no handler it still passes the info up to its parent which will log it if needed we can add handler to child and start logging at that level as well. (check slide as to how its usually set up) import java.io.IOException ; import java.nio.file.FileSystem ; import java.util.logging.* ; public class LogDemo { static Logger logger = LogManager . getLogManager (). getLogger ( Logger . GLOBAL_LOGGER_NAME ); // static reference to a logger and it can now be used anywhere inside the application public static void main ( String [] args ) { logManagerDemo (); } static void logManagerDemo (){ logger . log ( Level . INFO , \"My first log message\" ); logger . log ( Level . INFO , \"Another message\" ); } // logp allows you to specify the class and method explicityly. log infers it. static void logpDemo (){ logger . logp ( Level . ALL , \"LogDemo\" , \"logpDemo\" , \"Log message\" ); // logp supports parameters } // this method demostrates how you can piece the components of the logger yourself. // There are 3 components 1) Logger 2) Handler 3) Formatter // You can arrange them as shown below (check slides for diagram) static void logComponentsDemo (){ Logger customLogger = Logger . getLogger ( \"com.sethirajat\" ); // if this logger does not exist it will be created Handler h = new ConsoleHandler (); // using a built in handler which outputs to console. Formatter f = new SimpleFormatter (); // using a built in formatter h . setFormatter ( f ); customLogger . addHandler ( h ); customLogger . setLevel ( Level . ALL ); customLogger . log ( Level . INFO , \"We are logging this message\" ); } // this method demonstrates how you can log to a file static void FileHandlerDemo () throws IOException { Logger customLogger = Logger . getLogger ( \"com.sethirajat\" ); FileHandler h = new FileHandler (); // also can do new FileHandler(\"%h/myapp_%g.log\", 1000, 4) --> pattern for // file naming, limit, count (check official documentation by going to class or java docs. h . setFormatter ( new SimpleFormatter ()); customLogger . addHandler ( h ); customLogger . log ( Level . INFO , \"logging\" ); } } Multi-threading and concurrency What is a process instance of a program or application has resources such as memory has at least 1 thread What is a thread It is a sequence of programmed instructions. The thing that executes programs code utilizes process resources. Example problem that would benefit from multithreading An Adder class takes in file with numbers and outputs the total in another file. In a loop we have 6 input files and the work is done sequentially. Since reading from file and writing from file is non cpu task the cpu is idle. so we can do this problem with multithreading approach. Move to multithreading Its a explicit choice. you need to break the problem into parts and hand it off for processing. Java provides high level and low level api\\'s for this. Runtime Info & Reflection Reflection provdies Ability to examine types at runtime Dynamically execute & access members Using reflection can fully examine objects at runtime interfaces implemented members variety of uses determine a types capability tools development type inspector/browser Schema generation construct instances access fields call methods Each type has a Class class instance. It describes the type in detial.","title":"4. The core platform"},{"location":"Java/4.%20The%20core%20platform/#core-platform-and-libraries","text":"","title":"Core platform and libraries"},{"location":"Java/4.%20The%20core%20platform/#streams","text":"Streams in java are ordered input and output data It provides a common input and output model Streams are unidirectional Abstracts away the underline source of destination of the data (abstracts storange or where data is coming from) You can have input stream you can have output stream stream can be either a binary stream or character stream Reading streams Binary (The base class for this is InputStream) Text (The base class for this is Reader) There are various methods which are used when reading stream data read() --> reads 1 byte at a time read(byte[] buff) or read(char[] buff) --> read the data which can fit into buff If you see the below examples it may look like we are creating instances of InputStream, OutputStream, Reader or Writer. These are however abstract classes and cannot be instanciated. There are various subclasses of these which will be inherited based on the type of stream e.g. CharArrayWriter, StringWriter, PipedWriter, InputStreamReader, OutputStreamWriter etc... // Reading one byte(binary) at a time. InputStream input = // create input stream int intVal while (( intVal = input . read ()) >= 0 ){ // end of stream is indicated by .read() method returning -1 byte byteVal = ( byte ) intVal ; // now you can do something with byte value. } // Reading one character at a time. Reader input = // create input stream int intVal while (( intVal = input . read ()) >= 0 ){ // end of stream is indicated by .read() method returning -1 byte byteVal = ( chat ) intVal ; // now you can do something with chat value. } // Reading multiple byte(binary) at a time. InputStream input = // create input stream int length ; byte [] byteBuff = new byte [ 10 ] ; while (( length = input . read ()) >= 0 ){ // end of stream is indicated by .read() method returning -1 for ( int i = 0 ; i < length ; i ++ ){ // now we have array of byte, so loop on array to get single byte and do something with it. byte byteVal = byteBuff [ i ] ; // do something with byteval. } } Writing values OutputSteam output = // create stream byte value = 100 ; output . write ( value ); byte [] byteBuff = { 0 , 63 , 127 }; output . write ( byteBuff ); Writer output = //create stream char a = 'A' ; // can write single char output . write ( a ); char [] b = { 'a' , 'b' }; // can write array of char output . write ( b ); String c = \"HEllo\" ; // can write a string output . write ( c ); Usually when you are dealing with streams you will be using try and catch or try with resources and then closing down the stream as shown below. try { reader = // open reader; } catch ( IOException e ){ } finally (){ if ( reader != null ){ try { reader . close (); } catch (){} } } Use try with resources The above close gets complicated (its still simplified on top) so use try with resources. Try with resources handles the following exceptions Automate cleans ups of 1 or more resources Handle try body Also handles close method calls and exceptions try ( reader = //open reader ){} catch {} DEMO Check stream --> slides and demos. The interface AutoCloseable has a method close. The try and catch block will auto call this method close() even if its not called explicitly so the resources are freed up. Also when you use try with resources you may get only 1 exception however there may be other exceptions which are not printed out. So in that case use method getSuppressed and you can loop on this and print out all the suppressed exceptions. Chaining Streams You can chain streams together. Java provdes FilterReader, FilterWriter, FilterInputSteam and FilterOutputStream abstract classes which provide easier customization for chaining streams. Interacting with Files java.io classes are depricated use java.nio.file for file handling. Use Paths class to locate something in the file system or directory. Use Files static method to interacting with files. Create, copy delete etc... Open file streams like newBufferedReader, newBufferedWriter, newInputStream, newOutputStream Read/write file contents using readAllLines, write (If you see in the pluralSight course this below example was not accepted by Java, so modified it a little and it works) import java.io.BufferedReader ; import java.io.IOException ; import java.nio.file.Files ; import java.nio.file.Paths ; public class ReadFiles { public static void main ( String [] args ) throws IOException { try ( BufferedReader br = Files . newBufferedReader ( Paths . get ( \"file.txt\" )); ){ String data ; while (( data = br . readLine ()) != null ){ System . out . println ( data ); } } } } Working with File Systems When we work with file systems we usually work with computers default file system. java also supports specialized file systems like Zip file system. So we use the File System Type which represents the file system. Class --> FileSystem Check slide and programs for additional references. (Code demos are avaiable there)","title":"Streams"},{"location":"Java/4.%20The%20core%20platform/#strings","text":"Basic concatenation and Stringbuilder are not powerful enough to create complicated strings. so we will be looking at more powerful ways. StringJoinr --> Simplifies joining a sequence of values. String formatting --> used for desired appearance. String Joiner import java.util.StringJoiner ; public class StringJoinerDemo { public static void main ( String [] args ) { StringJoiner sj = new StringJoiner ( \", \" ); // specify the delimiter sj . add ( \"alpha\" ); sj . add ( \"beta\" ); sj . add ( \"gama\" ); System . out . println ( sj . toString ()); // output --> alpha, beta, gama sj . add ( \"abc\" ). add ( \"def\" ); // chaining methods --> return type of add is StringJoiner System . out . println ( sj . toString ()); // output --> alpha, beta, gama, abc, def StringJoiner sj1 = new StringJoiner ( \"], [\" , \"[\" , \"]\" ); sj1 . add ( \"alpha\" ); sj1 . add ( \"beta\" ); System . out . println ( sj1 . toString ()); // [alpha], [beta] } } Format Specifiers StringJoiner is pretty powerful however we sometimes need more power :) Focus is on how things look Not concerned with how. Use methods like --> String.format or System.out.printf(), Formatter.format Parts of a format specifier --> %[argument index][flags][width][precesion]conversoin Common Format Conversions d - decimal o - octal x = Hex f - float e - Scientific Notation s - String (you can also pass objects not just strings here, if the class has Formattable then it will be used else will go with toString) .... Format Flags - # - Include radix - 0 - 0 pad value - - - left justify - , - Include grouping character - space - Leave space for + or - sign for positive numbers - + - always show signs You can also write formatted text to a stream not just on output screen. There is a class called the Formatter class which provides formatting capabilities. It can help us write formatted content to any class which implements the Appendable interface. The writer stream class implements the Appendable interface. e.g. BufferedWriter writer = new Files . newBufferedWriter ( Paths . get ( \"data.txt\" )); // it will close when formatter is closed. try ( Formatter f = new Formatter ( writer )){ // creating a formatter over the writer stream f . format ( \"Hello %d\" , 10 ); } Regular Expressions Just basics here --> check other places for detials String s1 = \"apple, apple and orange please\" ; String s2 = s1 . replaceAll ( \"ple\\\\b\" , \"ricot\" ); String s1 = \"apple, apple and orange please\" ; String [] parts = s1 . split ( \"//b\" ); for ( String part : parts ){ if ( part . matches ( \"\\\\w+\" )){ System . out . println ( part ); } } Compilation of regular expression can be very processing intensive, so if you are doing this in a loop or over and over again its better to precompile the regular expression and apply. The Pattern class allows us to precompile the regex and then apply. Then the Matcher class can apply the compiled regex to an expression/string. String s1 = \"apple, apple and orange please\" ; Pattern pattern = Pattern . compile ( \"\\\\w+\" ); Matcher matcher = pattern . matcher ( value1 ); while ( matcher . find ()){ System . out . println ( matcher . group ()); }","title":"Strings"},{"location":"Java/4.%20The%20core%20platform/#controlling-app-execution","text":"Command Line Arguments Arguments are passed as String arrays. Each argument is a seperate element. (seperated by space or if space put in quotes). In intellij you can pass arguemnts when you edit configurations package com.sethirajat.cli ; public class CLIargsDemo { public static void main ( String [] args ) { for ( String arg : args ) { System . out . println ( arg ); } } } Persistable Key Value Pairs Apps often need persistable key value pairs for app config or initial load or other things like state or preferences. (Hashmap will only store when we are running program and its in memory). Use the java.util.Properties class for this. Properties Class --> Inherits from Hash Table. Keys and values are string. properties can be written to and from streams can optionally include comments supports 2 formats --> text and xml key and value are separated by : or , or first white space #=or =! start comment line import java.io.* ; import java.nio.file.Files ; import java.nio.file.Paths ; import java.util.InvalidPropertiesFormatException ; import java.util.Properties ; public class PropertiesDemo { public static void main ( String [] args ) { // properties_reader(); // properties_writer_xml(); properties_reader_xml (); } public static void properties_writer () { Properties props = new Properties (); props . setProperty ( \"key1\" , \"value1\" ); System . out . println ( props . getProperty ( \"key1\" )); try ( Writer writer = Files . newBufferedWriter ( Paths . get ( \"abc.properties\" ))) { props . store ( writer , \"Sample properties\" ); } catch ( IOException e ) { e . printStackTrace (); } } static void properties_reader (){ Properties props = new Properties (); try ( Reader reader = Files . newBufferedReader ( Paths . get ( \"abc.properties\" ))){ props . load ( reader ); System . out . println ( props . getProperty ( \"key1\" )); } catch ( IOException e ) { e . printStackTrace (); } } // xml will work with output streams, so the output stream has to be stored as xml. static void properties_writer_xml (){ Properties props = new Properties (); try ( OutputStream out = Files . newOutputStream ( Paths . get ( \"abc.xml\" ))){ props . setProperty ( \"key1\" , \"value1\" ); props . storeToXML ( out , \"sample properties\" ); } catch ( IOException e ) { e . printStackTrace (); } } static void properties_reader_xml (){ Properties props = new Properties (); try ( InputStream in = Files . newInputStream ( Paths . get ( \"abc.xml\" ))){ props . loadFromXML ( in ); System . out . println ( props . getProperty ( \"key1\" )); } catch ( InvalidPropertiesFormatException e ) { e . printStackTrace (); } catch ( IOException e ) { e . printStackTrace (); } } } Default Properties Properties can also be created with default. static void propertiesWithDefault (){ Properties defaults = new Properties (); defaults . setProperty ( \"os\" , \"Windows\" ); Properties props = new Properties ( defaults ); // created with defaults } Usually a application will store default properties. For that when you are launching an aplication you can include .properties file in that application. Default Class Loading Classes must be in .class files Must be under package directory Specifying class path You can specify the class path. (if you specify path then current directory has to be specified ) if doing via env variables use CLASSPATH classpath is set for all programs and projects not just for current project Class loading with -jar option Class loading is controlled by jar file no other class loading source is used provides tight control over class loading e.g. =java -jar ourapp.jar=","title":"Controlling App Execution"},{"location":"Java/4.%20The%20core%20platform/#java-log-system","text":"Logs are used for various uses. errors usage info debug can be of different detail level Log System its centrally managed there is 1 app-wide log manager manages log system config manages objects that do actual logging class LogManager . There will be 1 global instance of it. LogManager.getLogManager Logger class provides methods to do logging use getLogger method to get to the logger from the log manager. each logger instance is named there is also a global logger GLOBAL_LOGGER_NAME Levels Each log entry is associated with a level Each logger has a capture level --> use setLevel method. The logger will ignore any entry below that level Each level has a numeric value 7 basic log levels 2 special levels for Logger you can also define custom log levels (its rare) 1000 - SEVERE 900 - WARNING 800 - INFO 700 - CONFIGURATION 500 - FINE 400 - FINER 300 - FINEST entering - logs at fine level exiting - logs at fine level Components of the log system Logger -> Accepts app calls Handler -> Publishes logging info, a logger can have multiple handlers As logger can have multiple handlers you can set level for each handler. (which should be more restrictive than the logger level) Formatter -> formats log info for publication. each handler has 1 formatter. (check slides as they have a good diagram to explain this) Built in Handlers The built in handlers inherit from the Handler class You can write custom handlers but mostly you will not ConsoleHandler --> writes to System.err StreamHandler --> writes to specified output stream SocketHandler --> writes to network socket FileHandler --> writes to 1 or more files can output to single file can output to rotating set of files specify size in bytes File Handler Substitution patterns values (check demo below) / - Platform specific slash e.g. ./foo.txt \\%T - writes to temp directory \\%h - writes to home directory \\%g - Rotating log generation Built in formatters Inherit from formatter class XMLFormatter Root element log each entry goes under named record SimpleFormatter Formats content as simple text Format is customizable Uses standard formatting notation You can customize this using java.util.logging.SimpleFormatter.format -> pass value with Java -D option when run the program (check slides) Log Configuration File You can create a configuration file for your log system. Using configuration file is much simpler. java.util.logging.ConsoleHandler.level = ALL ... (check slide) then when you launch the program launch with giving file details java - Djava . util . logging . config . file = log . properties com . pluralsight . training . Main // (the last one is is the app name com.plu....) Logger Naming Implies a parent child relationship based on the name we give the loggers. Naming should be hierarchical dot seperates a level generally tied to class\\'s full name e.g. =com.sethirajat.training= com.sethirajat.training.Main com.sethirajat.training.Student . In the above case Main and Student will auto become child loggers of com.sethirajat.training and any log on the child will be logged on the parent also. Making the most of hierarchical system If a logger level is null it inherits parents level So we primarily set level on parents (and is usually restrictive) if we need more detail then we can set more detailed level on the child logger each logger also does not need to have a handler if there is no handler it still passes the info up to its parent which will log it if needed we can add handler to child and start logging at that level as well. (check slide as to how its usually set up) import java.io.IOException ; import java.nio.file.FileSystem ; import java.util.logging.* ; public class LogDemo { static Logger logger = LogManager . getLogManager (). getLogger ( Logger . GLOBAL_LOGGER_NAME ); // static reference to a logger and it can now be used anywhere inside the application public static void main ( String [] args ) { logManagerDemo (); } static void logManagerDemo (){ logger . log ( Level . INFO , \"My first log message\" ); logger . log ( Level . INFO , \"Another message\" ); } // logp allows you to specify the class and method explicityly. log infers it. static void logpDemo (){ logger . logp ( Level . ALL , \"LogDemo\" , \"logpDemo\" , \"Log message\" ); // logp supports parameters } // this method demostrates how you can piece the components of the logger yourself. // There are 3 components 1) Logger 2) Handler 3) Formatter // You can arrange them as shown below (check slides for diagram) static void logComponentsDemo (){ Logger customLogger = Logger . getLogger ( \"com.sethirajat\" ); // if this logger does not exist it will be created Handler h = new ConsoleHandler (); // using a built in handler which outputs to console. Formatter f = new SimpleFormatter (); // using a built in formatter h . setFormatter ( f ); customLogger . addHandler ( h ); customLogger . setLevel ( Level . ALL ); customLogger . log ( Level . INFO , \"We are logging this message\" ); } // this method demonstrates how you can log to a file static void FileHandlerDemo () throws IOException { Logger customLogger = Logger . getLogger ( \"com.sethirajat\" ); FileHandler h = new FileHandler (); // also can do new FileHandler(\"%h/myapp_%g.log\", 1000, 4) --> pattern for // file naming, limit, count (check official documentation by going to class or java docs. h . setFormatter ( new SimpleFormatter ()); customLogger . addHandler ( h ); customLogger . log ( Level . INFO , \"logging\" ); } }","title":"Java Log System"},{"location":"Java/4.%20The%20core%20platform/#multi-threading-and-concurrency","text":"What is a process instance of a program or application has resources such as memory has at least 1 thread What is a thread It is a sequence of programmed instructions. The thing that executes programs code utilizes process resources. Example problem that would benefit from multithreading An Adder class takes in file with numbers and outputs the total in another file. In a loop we have 6 input files and the work is done sequentially. Since reading from file and writing from file is non cpu task the cpu is idle. so we can do this problem with multithreading approach. Move to multithreading Its a explicit choice. you need to break the problem into parts and hand it off for processing. Java provides high level and low level api\\'s for this.","title":"Multi-threading and concurrency"},{"location":"Java/4.%20The%20core%20platform/#runtime-info-reflection","text":"Reflection provdies Ability to examine types at runtime Dynamically execute & access members Using reflection can fully examine objects at runtime interfaces implemented members variety of uses determine a types capability tools development type inspector/browser Schema generation construct instances access fields call methods Each type has a Class class instance. It describes the type in detial.","title":"Runtime Info &amp; Reflection"},{"location":"Java/6.%20Database%20with%20JDBC/","text":"JDBC Basics JDBC : Java Database Connectivity (This section is from youtube) Steps to connectivity - Import Package (java.sql) - Load & Register the driver (e.g. com.mysql.jdbc.driver from mysql.connector) - Establish connection - Create the statement - Execute the query - Process Result - Close the conection Basic fetch value operation from database import java.sql.* ; public class TestConnection { public static void main ( String [] args ) throws Exception { Class . forName ( \"com.mysql.cj.jdbc.Driver\" ); Connection con = DriverManager . getConnection ( \"jdbc:mysql://localhost:3306/world\" , \"admin\" , \"admin\" ); // String query = \"select * from country where Code = \\\"IND\\\"\"; String query = \"select * from country\" ; Statement st = con . createStatement (); ResultSet rs = st . executeQuery ( query ); // rs.next(); while ( rs . next ()) { String name = rs . getString ( \"Name\" ); System . out . println ( name ); } st . close (); con . close (); } } Inserting Values into the database DDL --> Data Definition Language (Create table, change database etc...) Use --> st.executeUpdate(query) DML --> Data Manipulation Language (INSERT, UPDATE and DELETE statements) Use --> st.executeUpdate(query) DQL --> Data Query Language (SELECT, SHOW and HELP statements) Use --> st.executeQuery(query) /* DDL --> Data Definition Language (Create table, change database etc...) DML --> Data Manipulation Language (INSERT, UPDATE and DELETE statements) Use --> st.executeUpdate(query) DQL --> Data Query Language (SELECT, SHOW and HELP statements) Use --> st.executeQuery(query) */ import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.Statement ; public class InsertingValuesStringQuery { public static void main ( String [] args ) throws Exception { Class . forName ( \"com.mysql.cj.jdbc.Driver\" ); Connection con = DriverManager . getConnection ( \"jdbc:mysql://localhost:3306/test\" , \"admin\" , \"admin\" ); Statement st = con . createStatement (); String query = \"insert into customer (firstName, lastName, email, dob) values ('Sheldon', 'Cooper', 'Sheldon.Cooper@gmail.com', '1973-01-01')\" ; int rowsAffected = st . executeUpdate ( query ); System . out . println ( \"Number of row(s) affected : \" + rowsAffected ); st . close (); con . close (); } } Use preparedStatement to build query instead of passing the whole string. In this case you can use some variables as well to prepare statement. The below example asks data from the user and inserts into the database. import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.PreparedStatement ; import java.util.Scanner ; public class InsertingValuesPreparedStatement { public static void main ( String [] args ) throws Exception { Class . forName ( \"com.mysql.cj.jdbc.Driver\" ); Connection con = DriverManager . getConnection ( \"jdbc:mysql://localhost:3306/test\" , \"admin\" , \"admin\" ); String query = \"insert into customer (firstName, lastName, email, dob) values (?, ?, ?, ?)\" ; PreparedStatement st = con . prepareStatement ( query ); String firstName = getUserInput ( \"Enter your first name : \" ); String lastName = getUserInput ( \"Enter your last name : \" ); String email = getUserInput ( \"Enter your email : \" ); String dob = getUserInput ( \"Enter your dob in yyyy-mm-dd format : \" ); st . setString ( 1 , firstName ); st . setString ( 2 , lastName ); st . setString ( 3 , email ); st . setString ( 4 , dob ); int rowsAffected = st . executeUpdate (); System . out . println ( \"Number of row(s) affected : \" + rowsAffected ); st . close (); con . close (); } private static String getUserInput ( String message ) { Scanner scan = new Scanner ( System . in ); System . out . print ( message ); return scan . nextLine (). strip (); } } Using SQLite import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.PreparedStatement ; import java.util.Scanner ; public class TestConnectionSQLite { public static void main ( String [] args ) throws Exception { // Class.forName(\"com.mysql.cj.jdbc.Driver\"); Connection con = DriverManager . getConnection ( \"jdbc:sqlite:C:/Users/182362434/dbA.db\" ); String query = \"insert into customer (firstName, lastName, email, dob) values (?, ?, ?, ?)\" ; PreparedStatement st = con . prepareStatement ( query ); String firstName = getUserInput ( \"Enter your first name : \" ); String lastName = getUserInput ( \"Enter your last name : \" ); String email = getUserInput ( \"Enter your email : \" ); String dob = getUserInput ( \"Enter your dob in yyyy-mm-dd format : \" ); st . setString ( 1 , firstName ); st . setString ( 2 , lastName ); st . setString ( 3 , email ); st . setString ( 4 , dob ); int rowsAffected = st . executeUpdate (); System . out . println ( \"Number of row(s) affected : \" + rowsAffected ); st . close (); con . close (); } private static String getUserInput ( String message ) { Scanner scan = new Scanner ( System . in ); System . out . print ( message ); return scan . nextLine (). strip (); } } Creating a DAO using SQLite. /* A data access object (DAO) is an object that provides an abstract interface to some type of database or other persistence mechanism. The DAO should do the CRUD operations. */ import java.sql.* ; class Person { int id ; String firstName ; String lastName ; String email ; String dob ; Person (){} public Person ( String firstName , String lastName , String email , String dob ) { this . firstName = firstName ; this . lastName = lastName ; this . email = email ; this . dob = dob ; } @Override public String toString () { return \"Person{id = \" + id + \", firstName = \" + firstName + \", lastName = \" + lastName + \", email = \" + email + \", dob = \" + dob + \"}\" ; } } class PersonDAO { public Person getPerson ( int id ) throws Exception { try { Person p = new Person (); p . id = id ; String query = \"Select * from Customer where id = \" + id ; Connection con = DriverManager . getConnection ( \"jdbc:sqlite:C:/Users/182362434/dbA.db\" ); Statement st = con . createStatement (); ResultSet rs = st . executeQuery ( query ); rs . next (); p . firstName = rs . getString ( \"firstName\" ); p . lastName = rs . getString ( \"lastName\" ); p . dob = rs . getString ( \"dob\" ); st . close (); con . close (); return p ; } catch ( Exception e ) { System . out . println ( e . getMessage ()); } return null ; } public int addPerson ( Person p ) throws Exception { Connection con = DriverManager . getConnection ( \"jdbc:sqlite:C:/Users/182362434/dbA.db\" ); PreparedStatement st = con . prepareStatement ( \"insert into customer (firstName, lastName, email, dob) values (?, ?, ?, ?)\" ); st . setString ( 1 , p . firstName ); st . setString ( 2 , p . lastName ); st . setString ( 3 , p . email ); st . setString ( 4 , p . dob ); int result = st . executeUpdate (); st . close (); con . close (); return result ; } } // Main class from here onwards. If you see below the main class is executing simple looking statements. // Its very abstract here. public class JdbcDaoDemo { public static void main ( String [] args ) throws Exception { PersonDAO dao = new PersonDAO (); // Getting someone with id = 1 Person p = dao . getPerson ( 1 ); System . out . println ( p ); // Getting someone with id = 2 p = dao . getPerson ( 2 ); System . out . println ( p ); // Inserting someone into the database and then fetching him and displaying the value. Person raj = new Person ( \"Raj\" , \"Kuth\" , \"rk@gmail.com\" , \"2010-01-01\" ); dao . addPerson ( raj ); p = dao . getPerson ( 3 ); System . out . println ( p ); } } JDBC continued (This section is from pluralsight) Basic CRUD operations Some of the databases support scorllable (forward and backward) datasets i.e. you can go forward and back and to first and last rows e.g. MYSql, however some of the databases like oracle or SQLite do not support this. They are forwrad only databases. Lets refactor the code to access the database into something which is more re-useable. // DBType.Java public enum DBType { SQLITEDB , MYSQLDB } We also have a utility class for connections to the db object. // DBUtil.Java import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.SQLException ; public class DBUtil { private static final String SQLiteURL = \"jdbc:sqlite:C:/Users/182362434/dbA.db\" ; private static final String MYSQLURL = \"jdbc:mysql://localhost:3306/world\" ; private static final String MYSQLuser = \"admin\" ; private static final String MYSQLpassword = \"admin\" ; public static Connection getConnection ( DBType dbType ) throws SQLException { switch ( dbType ){ case SQLITEDB : return DriverManager . getConnection ( SQLiteURL ); case MYSQLDB : return DriverManager . getConnection ( MYSQLURL , MYSQLuser , MYSQLpassword ); default : return null ; } } public static void showErrorMessage ( SQLException e ){ System . err . println ( \"Error : \" + e . getMessage ()); System . err . println ( \"Error code : \" + e . getErrorCode ()); } } Sample connection shown below. // TestManageDBResources.Java import java.sql.* ; public class TestManageDBResources { public static void main ( String [] args ) throws SQLException { Connection con = null ; try { con = DBUtil . getConnection ( DBType . MYSQLDB ); System . out . println ( \"Connection Successful\" ); } catch ( SQLException e ){ DBUtil . showErrorMessage ( e ); } finally { if ( con != null ) { con . close (); } } } } How to fetch data. // TestStaticSQLStatement.Java import java.sql.Connection ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class TestStaticSQLStatement { public static void main ( String [] args ) throws SQLException { Connection conn = null ; Statement st = null ; ResultSet rs = null ; try { conn = DBUtil . getConnection ( DBType . MYSQLDB ); st = conn . createStatement (); rs = st . executeQuery ( \"Select * from city\" ); rs . last (); System . out . println ( \"Number of rows : \" + rs . getRow ()); } catch ( SQLException e ) { DBUtil . showErrorMessage ( e ); } finally { if ( rs != null ) rs . close (); if ( st != null ) st . close (); if ( conn != null ) conn . close (); } } } We will fetch the data again from the data base using try with resouces block. In this case you do not need the finally block because after try java auto closes the objects. import java.sql.Connection ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class IteratingResultsetTryWithResources { public static void main ( String [] args ) throws SQLException { try ( Connection conn = DBUtil . getConnection ( DBType . MYSQLDB ); Statement st = conn . createStatement (); ResultSet rs = st . executeQuery ( \"Select * from city\" ) ) { while ( rs . next ()){ System . out . println ( rs . getString ( 1 ) + rs . getString ( 2 )); } } catch ( SQLException e ) { DBUtil . showErrorMessage ( e ); } } } Types of resultsets There are different types of result sets. (Check slide or documentation of resultsets). Some of them will be updated when database is updating (support concurrency). Some of them are scorallable some are not. One useful link is this As discussed some database do not give scrollable resultset by default, when creating a resultset you can specify the type of resultset which you want to get. Eample provided below. // ResultsetScrollableDemo.Java import java.sql.* ; public class ResultsetScrollableDemo { public static void main ( String [] args ) { try ( Connection conn = DBUtil . getConnection ( DBType . ORACLE ); // lets say we had oracle databasesa // creating a scrollable and read only resultset Statement st = conn . createStatement ( ResultSet . TYPE_SCROLL_INSENSITIVE , ResultSet . CONCUR_READ_ONLY ) ; ResultSet rs = (( Statement ) st ). executeQuery ( \"Select * from Customer\" ); ){ rs . beforeFirst (); // move the cursor to before first record; rs . absolute ( 1 ); // goto first row System . out . println ( rs . getString ( 1 ) + \" : \" + rs . getString ( 2 )); } catch ( SQLException e ){ DBUtil . showErrorMessage ( e ); } } } Updatable ResultSet You can also create an updatable resultset. The updates in the result set will make changes to the database. Most common methods are as follows. updateRow() --> update to db deleteRow() --> delete from db refreshRow() --> refresh in rs from db cancelRowUpdates() --> cancel changes in rs insertRow() --> insert in db Some databases will support updateable resultsets. Using updatable resultsets is a good approach for small datasets but for very large datasets it will create performance issues. import java.sql.Connection ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class UpdatableResultSetDemo { public static void main ( String [] args ) throws SQLException { try ( Connection conn = DBUtil . getConnection ( DBType . ORADB ); Statement stmt = conn . createStatement ( ResultSet . TYPE_SCROLL_INSENSITIVE , ResultSet . CONCUR_UPDATABLE ); // creating updatable resultset ResultSet rs = stmt . executeQuery ( \"Select Department_Id, Department_Name, Manager_Id, Location_Id from Departments\" ); ) { // updating a row rs . absolute ( 6 ); rs . updateString ( \"Department_Name\" , \"Information Technology\" ); rs . updateRow (); System . out . println ( \"Record Updated Successfully\" ); // inserting a row rs . moveToInsertRow (); rs . updateInt ( \"Department_Id\" , 999 ); rs . updateString ( \"Department_Name\" , \"Training\" ); rs . updateInt ( \"Manager_Id\" , 200 ); rs . updateInt ( \"Location_Id\" , 1800 ); rs . insertRow (); System . out . println ( \"Record Inserted Successfully\" ); } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } } } Prepared statements Improves performance of the app. Easy to set SQL parameter value Prevent SQL dependency Injection Attacks Although you can use prepared statement for objects with no parameter but most often you will use them for objects with parameters. // Inserting with prepared statement import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.Date ; import java.sql.SQLException ; import java.text.ParseException ; import java.text.SimpleDateFormat ; import java.util.Scanner ; public class TestPreparedInsert { public static void main ( String [] args ) throws SQLException { // TODO Auto-generated method stub Connection conn = DBUtil . getConnection ( DBType . ORADB ); int empno ; String ename , email ; java . sql . Date hiredate ; double salary ; Scanner scanner = new Scanner ( System . in ); System . out . print ( \"Enter Employee ID :\" ); empno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter Employee Name :\" ); ename = scanner . nextLine (); System . out . print ( \"Ente Email : \" ); email = scanner . nextLine (); System . out . print ( \"Enter Date of Joining : \" ); hiredate = java . sql . Date . valueOf ( scanner . nextLine ()); System . out . print ( \"Enter Salary : \" ); salary = scanner . nextDouble (); String sql = \"insert into NewEmployees values ( ?,?,?,?,? )\" ; PreparedStatement pstmt = conn . prepareStatement ( sql ); pstmt . setInt ( 1 , empno ); pstmt . setString ( 2 , ename ); pstmt . setString ( 3 , email ); pstmt . setDate ( 4 , hiredate ); pstmt . setDouble ( 5 , salary ); int result = pstmt . executeUpdate (); if ( result == 1 ) { System . out . println ( \"Record Inserted Successfully.\" ); } else { System . err . println ( \"Error while adding the record.\" ); } scanner . close (); pstmt . close (); conn . close (); } } Updating the record via prepared statement import java.sql.Connection ; import java.sql.SQLException ; import java.util.Scanner ; import java.sql.PreparedStatement ; public class TestPreparedUpdate { public static void main ( String [] args ) throws SQLException { Connection conn = DBUtil . getConnection ( DBType . ORADB ); String sql = \"Update NewEmployees set Salary = ? where Employee_Id = ?\" ; Scanner scanner = new Scanner ( System . in ); System . out . print ( \"Enter Employee ID :\" ); int empno = scanner . nextInt (); System . out . print ( \"Enter New Salary : \" ); double salary = scanner . nextDouble (); PreparedStatement pstmt = conn . prepareStatement ( sql ); pstmt . setDouble ( 1 , salary ); pstmt . setInt ( 2 , empno ); int result = pstmt . executeUpdate (); if ( result == 1 ) { System . out . println ( \"Employee Salary Updated Successfully.\" ); } else { System . err . println ( \"Error while updating the Salary.\" ); } scanner . close (); pstmt . close (); conn . close (); } } Deleting the record via prepared statement. import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.SQLException ; import java.util.Scanner ; public class TestPreparedStatementDelete { public static void main ( String [] args ) throws SQLException { Connection conn = DBUtil . getConnection ( DBType . ORADB ); String sql = \"Delete from NewEmployees where Employee_Id = ?\" ; Scanner scanner = new Scanner ( System . in ); System . out . print ( \"Enter Employee ID :\" ); int empno = scanner . nextInt (); PreparedStatement pstmt = conn . prepareStatement ( sql ); pstmt . setInt ( 1 , empno ); int result = pstmt . executeUpdate (); if ( result == 1 ){ System . out . println ( \"Employee Record Removed Successfully.\" ); } else { System . err . println ( \"Error While Removing Employee Record.\" ); } scanner . close (); pstmt . close (); conn . close (); } } Working with stored procedures Stored Procedures - Stored procedures are a set of SQL statements that perform a particular task. They are useful when you are dealing with complex scenario which may require multiple statements. So instead of executing multiple updates form java to JDBC we create the stored procedure in DB and send the data to that stored procedure in DB. The stored procedure in DB then makes the changes. Its benefitial in case of roll backs. deals better with partial updates (think of multiple updates from db if 1 failes then we have to roll back all of them etc...) performance is also better Each database has its own language for creating stored procedure. The DBA usually creates the stored procedures. - Oracle --> PLSQL - MYSQL --> Stored procedure language - SQLServer --> Transact SQL In JDBC we use callable statements to make a call to Stored Procedures. Demo create a new stored procedure AddNewEmployee in Oracle DB. from jdbc call the store procedure. import java.sql.Connection ; import java.sql.Date ; import java.sql.SQLException ; import java.util.Scanner ; import java.sql.CallableStatement ; public class TestCallableIn { public static void main ( String [] args ) { try ( Connection conn = DBUtil . getConnection ( DBType . ORADB ); // statement type is prepareCall and in \"{}\" // for values we just put ? CallableStatement callableStatement = conn . prepareCall ( \"{call AddNewEmployee(?,?,?,?,?)}\" ); // callable statement ) { Scanner scanner = new Scanner ( System . in ); System . out . print ( \"Enter Employee # : \" ); int empno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter Employee Name : \" ); String ename = scanner . nextLine (); System . out . print ( \"Enter Email ID :\" ); String email = scanner . nextLine (); System . out . print ( \"Enter Hiredate : \" ); Date doj = java . sql . Date . valueOf ( scanner . nextLine ()); System . out . print ( \"Enter Salary :\" ); double salary = scanner . nextDouble (); // setting the values as we did for other statements callableStatement . setInt ( 1 , empno ); callableStatement . setString ( 2 , ename ); callableStatement . setString ( 3 , email ); callableStatement . setDate ( 4 , doj ); callableStatement . setDouble ( 5 , salary ); // call execute method callableStatement . execute (); System . out . println ( \"Employee Record Added Successfully.\" ); } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } } } Batch processing in JDBC Each time a request is made to server we usually establish a connection. Each time database updates java application. lets say we have to insert 100 records then its 100 times to and fro communication between java and sql which leads to network and performance issues. Thats why its benefitial to do batch processing. We do that by creating a batch in jdbc and then submit to database. The batch processing is supported by statement, preparedStatement and CallableStatement Methods to keep in mind void addBatch() int[] executeBatch() import java.sql.Connection ; import java.sql.Date ; import java.sql.SQLException ; import java.util.Scanner ; import java.sql.CallableStatement ; public class TestCallableBatchProcessing { public static void main ( String [] args ) { try ( Connection conn = DBUtil . getConnection ( DBType . ORADB ); CallableStatement callableStatement = conn . prepareCall ( \"call AddNewEmployee(?,?,?,?,?)\" ); ){ String option ; do { Scanner scanner = new Scanner ( System . in ); System . out . print ( \"Enter Employee # : \" ); int empno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter Employee Name : \" ); String ename = scanner . nextLine (); System . out . print ( \"Enter Email ID :\" ); String email = scanner . nextLine (); System . out . print ( \"Enter Hiredate : \" ); Date dob = java . sql . Date . valueOf ( scanner . nextLine ()); System . out . print ( \"Enter Salary :\" ); double salary = Double . parseDouble ( scanner . nextLine ()); callableStatement . setInt ( 1 , empno ); callableStatement . setString ( 2 , ename ); callableStatement . setString ( 3 , email ); callableStatement . setDate ( 4 , dob ); callableStatement . setDouble ( 5 , salary ); callableStatement . addBatch (); // adding to batch instead of executing it. System . out . print ( \"Do You Want To Add Another Record (yes /no): \" ); option = scanner . nextLine (); } while ( option . equals ( \"yes\" )); int [] updateCounts = callableStatement . executeBatch (); // executing the batch, which returns array of ints. System . out . println ( \"Total Records Inserted are : \" + updateCounts . length ); } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } } } Out parameters Stored procedures can not only take IN parameters but can also return OUT parameters. e.g. in case you wish that the database does some sort of count and returns back the value. The below example shows that the stored procedure takes in a value of department ID and returns how many people are in that department ID. import java.sql.CallableStatement ; import java.sql.Connection ; import java.sql.SQLException ; import java.sql.Types ; import java.util.Scanner ; public class TestCallableOut { public static void main ( String [] args ) { // TODO Auto-generated method stub try ( Connection conn = DBUtil . getConnection ( DBType . ORADB ); // In case of IN or OUT parameter the callable Statement definition remains the same. i.e. we just put ? for both types of params. // In this case `GetTotalEmployeesByDepartment` is the stored procedure name. CallableStatement callableStatement = conn . prepareCall ( \"{ call GetTotalEmployeesByDepartment(?,?) }\" ); Scanner scanner = new Scanner ( System . in ); ) { System . out . print ( \"Enter Department ID : \" ); int deptno = Integer . parseInt ( scanner . nextLine ()); // providing value for IN param. callableStatement . setInt ( 1 , deptno ); // registring the out param. callableStatement . registerOutParameter ( 2 , Types . INTEGER ); callableStatement . execute (); // once the statement is executed we get the value of the out param, in this case it was the 2nd param int totalEmployees = callableStatement . getInt ( 2 ); System . out . println ( \"Total Employees Working : \" + totalEmployees ); } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } } } IN OUT parameter The same parameter can be used as both IN and OUT parameters. lets say we send some value and db\\'s stored procedure modify\\'s the value and then stores in teh database. Then the stored procedure returns the updated value back to JAVA. In the example below for a course, the db takes in orignal fee and retuns the final fee as OUT parameter after calculating the discount. (For the details of the stored procedure refer to the slides.) import java.sql.CallableStatement ; import java.sql.Connection ; import java.sql.SQLException ; import java.sql.Types ; import java.util.Scanner ; public class TestCallableInOut { public static void main ( String [] args ) throws SQLException { Connection conn = null ; CallableStatement callableStatement = null ; Scanner scanner = null ; try { conn = DBUtil . getConnection ( DBType . ORADB ); callableStatement = conn . prepareCall ( \"{call GetCourseFeesById(?,?)}\" ); scanner = new Scanner ( System . in ); // Calling Stored procedure 1 // Get the course details with orginal fees. System . out . print ( \"Enter Course ID :\" ); int cid = Integer . parseInt ( scanner . nextLine ()); callableStatement . setInt ( 1 , cid ); callableStatement . registerOutParameter ( 2 , Types . DOUBLE ); callableStatement . execute (); double fees = callableStatement . getDouble ( 2 ); System . out . println ( \"Course Fees : \" + fees ); // Calling Stored procedure 2 // Get the student details System . out . print ( \"Enter Roll Number : \" ); int rno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter Student Name :\" ); String sname = scanner . nextLine (); System . out . print ( \"Enter your Percentage : \" ); double percentage = Double . parseDouble ( scanner . nextLine ()); callableStatement = conn . prepareCall ( \"{ call EnrollStudent(?,?,?,?,?) }\" ); callableStatement . setInt ( \"rno\" , rno ); callableStatement . setString ( \"sname\" , sname ); callableStatement . setInt ( \"cid\" , cid ); callableStatement . setDouble ( \"cfees\" , fees ); callableStatement . setDouble ( \"spercent\" , percentage ); // Register the same variale as OUT parameter which was used as IN parameter also. callableStatement . registerOutParameter ( \"cfees\" , Types . DOUBLE ); callableStatement . execute (); // get the value sent by the database fees = callableStatement . getDouble ( \"cfees\" ); // display it. System . out . println ( sname + \" enrolled for the Course with the ID \" + cid + \" and Final Fees is \" + fees ); } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } finally { scanner . close (); callableStatement . close (); conn . close (); } } } Stored procedure returning ResultSets A stored procedure can also return a resultset. In the example below a stored procedure is returning a oracle cursor. We then need to type cast that to resultset and then loop on it. (Personal opinion --> looks like a lot of boilerplate code, ORM might be better or some DBUtils packages) import java.sql.CallableStatement ; import java.sql.Connection ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.util.Scanner ; import oracle.jdbc.internal.OracleCallableStatement ; import oracle.jdbc.OracleTypes ; public class TestCallableResultSet { public static void main ( String [] args ) throws SQLException { // TODO Auto-generated method stub try ( Connection conn = DBUtil . getConnection ( DBType . ORADB ); // same as always teh 2nd param will be the resultset CallableStatement callableStatement = conn . prepareCall ( \"{call GetEmployeesByRefCursor(?,?)}\" ); Scanner scanner = new Scanner ( System . in ); ){ System . out . print ( \"Enter Department ID : \" ); int deptno = Integer . parseInt ( scanner . nextLine ()); callableStatement . setInt ( 1 , deptno ); callableStatement . registerOutParameter ( 2 , OracleTypes . CURSOR ); // notice the type callableStatement . execute (); // typecast what is returned to resultset ResultSet rs = (( oracle . jdbc . internal . OracleCallableStatement ) callableStatement ). getCursor ( 2 ); String format = \"%-4s%-50s%-25s%-10f\\n\" ; // loop and display values. while ( rs . next ()){ System . out . format ( format , rs . getString ( \"Employee_ID\" ), rs . getString ( \"Employee_Name\" ), rs . getString ( \"Email\" ), rs . getFloat ( \"Salary\" )); } } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } } } Transaction Management using JDBC Lets say we have 2 people. We are deducting \\$100 from personA and depositing to personB. The first operation of deductoin passes but the next one of deposit to personB fails. In this case transaction management can be used to specify commit all or rollback all. import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.util.Scanner ; public class TestTransactionManagement { public static void main ( String [] args ) throws SQLException { try { Connection conn = DBUtil . getConnection ( DBType . ORADB ); // set the auto commit functionality to false. conn . setAutoCommit ( false ); PreparedStatement pstmt = null ; Scanner scanner = new Scanner ( System . in ); System . out . println ( \"PSBank Transactions\" ); System . out . println ( \"----------------------\" ); System . out . print ( \"Enter From Account # :\" ); int fromAcno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter To Account # : \" ); int toAcno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter Amount To Transfer : \" ); double amount = Double . parseDouble ( scanner . nextLine ()); String withdrawSQL = \"Update PSBank set Amount = Amount - ? where Acno = ?\" ; pstmt = conn . prepareStatement ( withdrawSQL ); pstmt . setDouble ( 1 , amount ); pstmt . setInt ( 2 , fromAcno ); pstmt . executeUpdate (); String depositSQL = \"Update PSBank set Amount = Amount + ? where Acno = ?\" ; pstmt = conn . prepareStatement ( depositSQL ); pstmt . setDouble ( 1 , amount ); pstmt . setInt ( 2 , toAcno ); pstmt . executeUpdate (); String sql = \"Select Amount From PSBank where Acno = ?\" ; pstmt = conn . prepareStatement ( sql ); pstmt . setInt ( 1 , fromAcno ); ResultSet rs = pstmt . executeQuery (); double balanceAmount = 0 ; if ( rs . next ()){ balanceAmount = rs . getDouble ( \"Amount\" ); } if ( balanceAmount >= 5000 ){ // if everything is ok we then commit. conn . commit (); System . out . println ( \"Amount Transferred Successfully...\" ); } else { // else we rollback. conn . rollback (); System . out . println ( \"Insufficient Funds : \" + balanceAmount + \" Transactions Rollbacked..\" ); } scanner . close (); pstmt . close (); conn . close (); } catch ( Exception ex ){ System . err . println ( ex . getMessage ()); } } } Working with CLOB and BLOB CLOB CLOB -> (Character Large Object) is a collection of character data stored in database as single entity. Used to store large text documents e.g. plain text or xml. Also not all database support CLOB. MYSQL you need to use LongText which can store upto 4GB data. In ORACLE we use CLOB. In order to store the file we need to get the file. Read the contents of the file with help of any input stream reader. Convert contents to character with help of any ASCI string Then pass to database using prepared statement or callable statement In order to read the CLOB data from database use getClob() Inserting CLOB data import java.io.File ; import java.io.FileNotFoundException ; import java.io.FileReader ; import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.SQLException ; public class InsertCLOBDataToEmpResume { public static void main ( String [] args ) throws SQLException , FileNotFoundException { Connection conn = DBUtil . getConnection ( DBType . ORADB ); PreparedStatement pstmt = null ; // creating the prepared statement the same way. String sql = \"Update NewEmployees set Resume = ? where Employee_ID = 500\" ; pstmt = conn . prepareStatement ( sql ); // read the contents of the file String resumeFile = \"d:/PluralSight Demos/SekharResume.txt\" ; File file = new File ( resumeFile ); FileReader reader = new FileReader ( file ); // Set the file reader into the prepared statement pstmt . setCharacterStream ( 1 , reader , ( int ) file . length ()); // execute the prepared statement to store the file in database pstmt . executeUpdate (); System . out . println ( \"Resume Updated Successfully...\" ); pstmt . close (); conn . close (); } } Fetch CLOB data from DB import java.io.FileWriter ; import java.io.IOException ; import java.io.Reader ; import java.sql.Clob ; import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.ResultSet ; import java.sql.SQLException ; public class RetrieveCLOBDataFromDB { public static void main ( String [] args ) throws SQLException , IOException { Connection conn = DBUtil . getConnection ( DBType . ORADB ); String sql = \"select Resume from NewEmployees where Employee_ID = 500\" ; PreparedStatement pstmt = conn . prepareStatement ( sql ); ResultSet rs = pstmt . executeQuery (); if ( rs . next ()){ // fetch the first row in RS Clob resume = rs . getClob ( \"Resume\" ); Reader data = resume . getCharacterStream (); int i ; String resumeDetails = \"\" ; // conctenate the characters to create a file while ( ( i = data . read ()) != - 1 ){ resumeDetails += (( char ) i ); } System . out . println ( \"Resume Details for Employee 500\" ); System . out . println ( resumeDetails ); } else { System . err . println ( \"No Record Found For Employee With The ID 500.\" ); } rs . close (); pstmt . close (); conn . close (); } } BLOB Data Blob data stands for Binary Large Object. E.g. Pictures, documents etc... Sending data to DB import java.io.File ; import java.io.FileInputStream ; import java.io.IOException ; import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.SQLException ; public class InsertImageWithinDB { public static void main ( String [] args ) throws SQLException , IOException { // TODO Auto-generated method stub Connection conn = DBUtil . getConnection ( DBType . ORADB ); String sql = \"Update NewEmployees Set Photo = ? where Employee_ID = 500\" ; PreparedStatement pstmt = conn . prepareStatement ( sql ); File file = new File ( \"D:/PluralSight Demos/Sekhar.jpg\" ); FileInputStream fis = new FileInputStream ( file ); pstmt . setBinaryStream ( 1 , fis , fis . available ()); int count = pstmt . executeUpdate (); System . out . println ( \"Total Records Updated : \" + count ); pstmt . close (); conn . close (); } } Retrive BLOB data from DB import java.io.FileOutputStream ; import java.io.IOException ; import java.sql.Blob ; import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.ResultSet ; import java.sql.SQLException ; public class RetrieveImageFromDB { public static void main ( String [] args ) throws SQLException , IOException { Connection conn = DBUtil . getConnection ( DBType . ORADB ); String sql = \"Select Photo From NewEmployees Where Employee_Id = 500\" ; PreparedStatement pstmt = conn . prepareStatement ( sql ); ResultSet rs = pstmt . executeQuery (); if ( rs . next ()){ Blob imgBlob = rs . getBlob ( \"Photo\" ); FileOutputStream fos = new FileOutputStream ( \"D:/PluralSight Demos/Downloads/img500.jpg\" ); fos . write ( imgBlob . getBytes ( 1 , ( int ) imgBlob . length ())); fos . flush (); fos . close (); System . out . println ( \"Photo of Employee 500 has been Downloaded successfully\" ); } else { System . out . println ( \"Employee Record Not Found.\" ); } rs . close (); pstmt . close (); conn . close (); } } Working with metadata Check slides and programs Connection pooling Usually each request to DB will not create its own connection to DB (as establishing a connection to DB is very time consuming.) We use the connection pooling to request already established connections to the database and get the logical link to it (not the physical connection itself). Once the transaction to DB is complete we send back the connection to the connection pool. import java.sql.Connection ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; import javax.sql.PooledConnection ; import oracle.jdbc.pool.OracleConnectionPoolDataSource ; public class ConnectionPoolingDemo { public static void main ( String [] args ) throws SQLException { OracleConnectionPoolDataSource ds = new OracleConnectionPoolDataSource (); ds . setDriverType ( \"thin\" ); ds . setServerName ( \"localhost\" ); ds . setPortNumber ( 1521 ); ds . setServiceName ( \"xe\" ); ds . setUser ( \"hr\" ); ds . setPassword ( \"hr\" ); PooledConnection pconn = ds . getPooledConnection (); Connection conn = pconn . getConnection (); Statement stmt = conn . createStatement (); ResultSet rs = stmt . executeQuery ( \"Select * From Departments\" ); String format = \"%-30s%-50s%-25s\\n\" ; System . out . format ( format , \"Department #\" , \"Department Name\" , \"Location\" ); System . out . format ( format , \"-------------\" , \"-----------------\" , \"-------------\" ); while ( rs . next ()){ System . out . format ( format , rs . getString ( \"Department_ID\" ), rs . getString ( \"Department_Name\" ), rs . getString ( \"Location_Id\" )); } rs . close (); stmt . close (); conn . close (); pconn . close (); } }","title":"6. Database with JDBC"},{"location":"Java/6.%20Database%20with%20JDBC/#jdbc-basics","text":"JDBC : Java Database Connectivity (This section is from youtube) Steps to connectivity - Import Package (java.sql) - Load & Register the driver (e.g. com.mysql.jdbc.driver from mysql.connector) - Establish connection - Create the statement - Execute the query - Process Result - Close the conection Basic fetch value operation from database import java.sql.* ; public class TestConnection { public static void main ( String [] args ) throws Exception { Class . forName ( \"com.mysql.cj.jdbc.Driver\" ); Connection con = DriverManager . getConnection ( \"jdbc:mysql://localhost:3306/world\" , \"admin\" , \"admin\" ); // String query = \"select * from country where Code = \\\"IND\\\"\"; String query = \"select * from country\" ; Statement st = con . createStatement (); ResultSet rs = st . executeQuery ( query ); // rs.next(); while ( rs . next ()) { String name = rs . getString ( \"Name\" ); System . out . println ( name ); } st . close (); con . close (); } } Inserting Values into the database DDL --> Data Definition Language (Create table, change database etc...) Use --> st.executeUpdate(query) DML --> Data Manipulation Language (INSERT, UPDATE and DELETE statements) Use --> st.executeUpdate(query) DQL --> Data Query Language (SELECT, SHOW and HELP statements) Use --> st.executeQuery(query) /* DDL --> Data Definition Language (Create table, change database etc...) DML --> Data Manipulation Language (INSERT, UPDATE and DELETE statements) Use --> st.executeUpdate(query) DQL --> Data Query Language (SELECT, SHOW and HELP statements) Use --> st.executeQuery(query) */ import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.Statement ; public class InsertingValuesStringQuery { public static void main ( String [] args ) throws Exception { Class . forName ( \"com.mysql.cj.jdbc.Driver\" ); Connection con = DriverManager . getConnection ( \"jdbc:mysql://localhost:3306/test\" , \"admin\" , \"admin\" ); Statement st = con . createStatement (); String query = \"insert into customer (firstName, lastName, email, dob) values ('Sheldon', 'Cooper', 'Sheldon.Cooper@gmail.com', '1973-01-01')\" ; int rowsAffected = st . executeUpdate ( query ); System . out . println ( \"Number of row(s) affected : \" + rowsAffected ); st . close (); con . close (); } } Use preparedStatement to build query instead of passing the whole string. In this case you can use some variables as well to prepare statement. The below example asks data from the user and inserts into the database. import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.PreparedStatement ; import java.util.Scanner ; public class InsertingValuesPreparedStatement { public static void main ( String [] args ) throws Exception { Class . forName ( \"com.mysql.cj.jdbc.Driver\" ); Connection con = DriverManager . getConnection ( \"jdbc:mysql://localhost:3306/test\" , \"admin\" , \"admin\" ); String query = \"insert into customer (firstName, lastName, email, dob) values (?, ?, ?, ?)\" ; PreparedStatement st = con . prepareStatement ( query ); String firstName = getUserInput ( \"Enter your first name : \" ); String lastName = getUserInput ( \"Enter your last name : \" ); String email = getUserInput ( \"Enter your email : \" ); String dob = getUserInput ( \"Enter your dob in yyyy-mm-dd format : \" ); st . setString ( 1 , firstName ); st . setString ( 2 , lastName ); st . setString ( 3 , email ); st . setString ( 4 , dob ); int rowsAffected = st . executeUpdate (); System . out . println ( \"Number of row(s) affected : \" + rowsAffected ); st . close (); con . close (); } private static String getUserInput ( String message ) { Scanner scan = new Scanner ( System . in ); System . out . print ( message ); return scan . nextLine (). strip (); } } Using SQLite import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.PreparedStatement ; import java.util.Scanner ; public class TestConnectionSQLite { public static void main ( String [] args ) throws Exception { // Class.forName(\"com.mysql.cj.jdbc.Driver\"); Connection con = DriverManager . getConnection ( \"jdbc:sqlite:C:/Users/182362434/dbA.db\" ); String query = \"insert into customer (firstName, lastName, email, dob) values (?, ?, ?, ?)\" ; PreparedStatement st = con . prepareStatement ( query ); String firstName = getUserInput ( \"Enter your first name : \" ); String lastName = getUserInput ( \"Enter your last name : \" ); String email = getUserInput ( \"Enter your email : \" ); String dob = getUserInput ( \"Enter your dob in yyyy-mm-dd format : \" ); st . setString ( 1 , firstName ); st . setString ( 2 , lastName ); st . setString ( 3 , email ); st . setString ( 4 , dob ); int rowsAffected = st . executeUpdate (); System . out . println ( \"Number of row(s) affected : \" + rowsAffected ); st . close (); con . close (); } private static String getUserInput ( String message ) { Scanner scan = new Scanner ( System . in ); System . out . print ( message ); return scan . nextLine (). strip (); } } Creating a DAO using SQLite. /* A data access object (DAO) is an object that provides an abstract interface to some type of database or other persistence mechanism. The DAO should do the CRUD operations. */ import java.sql.* ; class Person { int id ; String firstName ; String lastName ; String email ; String dob ; Person (){} public Person ( String firstName , String lastName , String email , String dob ) { this . firstName = firstName ; this . lastName = lastName ; this . email = email ; this . dob = dob ; } @Override public String toString () { return \"Person{id = \" + id + \", firstName = \" + firstName + \", lastName = \" + lastName + \", email = \" + email + \", dob = \" + dob + \"}\" ; } } class PersonDAO { public Person getPerson ( int id ) throws Exception { try { Person p = new Person (); p . id = id ; String query = \"Select * from Customer where id = \" + id ; Connection con = DriverManager . getConnection ( \"jdbc:sqlite:C:/Users/182362434/dbA.db\" ); Statement st = con . createStatement (); ResultSet rs = st . executeQuery ( query ); rs . next (); p . firstName = rs . getString ( \"firstName\" ); p . lastName = rs . getString ( \"lastName\" ); p . dob = rs . getString ( \"dob\" ); st . close (); con . close (); return p ; } catch ( Exception e ) { System . out . println ( e . getMessage ()); } return null ; } public int addPerson ( Person p ) throws Exception { Connection con = DriverManager . getConnection ( \"jdbc:sqlite:C:/Users/182362434/dbA.db\" ); PreparedStatement st = con . prepareStatement ( \"insert into customer (firstName, lastName, email, dob) values (?, ?, ?, ?)\" ); st . setString ( 1 , p . firstName ); st . setString ( 2 , p . lastName ); st . setString ( 3 , p . email ); st . setString ( 4 , p . dob ); int result = st . executeUpdate (); st . close (); con . close (); return result ; } } // Main class from here onwards. If you see below the main class is executing simple looking statements. // Its very abstract here. public class JdbcDaoDemo { public static void main ( String [] args ) throws Exception { PersonDAO dao = new PersonDAO (); // Getting someone with id = 1 Person p = dao . getPerson ( 1 ); System . out . println ( p ); // Getting someone with id = 2 p = dao . getPerson ( 2 ); System . out . println ( p ); // Inserting someone into the database and then fetching him and displaying the value. Person raj = new Person ( \"Raj\" , \"Kuth\" , \"rk@gmail.com\" , \"2010-01-01\" ); dao . addPerson ( raj ); p = dao . getPerson ( 3 ); System . out . println ( p ); } }","title":"JDBC Basics"},{"location":"Java/6.%20Database%20with%20JDBC/#jdbc-continued","text":"(This section is from pluralsight)","title":"JDBC continued"},{"location":"Java/6.%20Database%20with%20JDBC/#basic-crud-operations","text":"Some of the databases support scorllable (forward and backward) datasets i.e. you can go forward and back and to first and last rows e.g. MYSql, however some of the databases like oracle or SQLite do not support this. They are forwrad only databases. Lets refactor the code to access the database into something which is more re-useable. // DBType.Java public enum DBType { SQLITEDB , MYSQLDB } We also have a utility class for connections to the db object. // DBUtil.Java import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.SQLException ; public class DBUtil { private static final String SQLiteURL = \"jdbc:sqlite:C:/Users/182362434/dbA.db\" ; private static final String MYSQLURL = \"jdbc:mysql://localhost:3306/world\" ; private static final String MYSQLuser = \"admin\" ; private static final String MYSQLpassword = \"admin\" ; public static Connection getConnection ( DBType dbType ) throws SQLException { switch ( dbType ){ case SQLITEDB : return DriverManager . getConnection ( SQLiteURL ); case MYSQLDB : return DriverManager . getConnection ( MYSQLURL , MYSQLuser , MYSQLpassword ); default : return null ; } } public static void showErrorMessage ( SQLException e ){ System . err . println ( \"Error : \" + e . getMessage ()); System . err . println ( \"Error code : \" + e . getErrorCode ()); } } Sample connection shown below. // TestManageDBResources.Java import java.sql.* ; public class TestManageDBResources { public static void main ( String [] args ) throws SQLException { Connection con = null ; try { con = DBUtil . getConnection ( DBType . MYSQLDB ); System . out . println ( \"Connection Successful\" ); } catch ( SQLException e ){ DBUtil . showErrorMessage ( e ); } finally { if ( con != null ) { con . close (); } } } } How to fetch data. // TestStaticSQLStatement.Java import java.sql.Connection ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class TestStaticSQLStatement { public static void main ( String [] args ) throws SQLException { Connection conn = null ; Statement st = null ; ResultSet rs = null ; try { conn = DBUtil . getConnection ( DBType . MYSQLDB ); st = conn . createStatement (); rs = st . executeQuery ( \"Select * from city\" ); rs . last (); System . out . println ( \"Number of rows : \" + rs . getRow ()); } catch ( SQLException e ) { DBUtil . showErrorMessage ( e ); } finally { if ( rs != null ) rs . close (); if ( st != null ) st . close (); if ( conn != null ) conn . close (); } } } We will fetch the data again from the data base using try with resouces block. In this case you do not need the finally block because after try java auto closes the objects. import java.sql.Connection ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class IteratingResultsetTryWithResources { public static void main ( String [] args ) throws SQLException { try ( Connection conn = DBUtil . getConnection ( DBType . MYSQLDB ); Statement st = conn . createStatement (); ResultSet rs = st . executeQuery ( \"Select * from city\" ) ) { while ( rs . next ()){ System . out . println ( rs . getString ( 1 ) + rs . getString ( 2 )); } } catch ( SQLException e ) { DBUtil . showErrorMessage ( e ); } } } Types of resultsets There are different types of result sets. (Check slide or documentation of resultsets). Some of them will be updated when database is updating (support concurrency). Some of them are scorallable some are not. One useful link is this As discussed some database do not give scrollable resultset by default, when creating a resultset you can specify the type of resultset which you want to get. Eample provided below. // ResultsetScrollableDemo.Java import java.sql.* ; public class ResultsetScrollableDemo { public static void main ( String [] args ) { try ( Connection conn = DBUtil . getConnection ( DBType . ORACLE ); // lets say we had oracle databasesa // creating a scrollable and read only resultset Statement st = conn . createStatement ( ResultSet . TYPE_SCROLL_INSENSITIVE , ResultSet . CONCUR_READ_ONLY ) ; ResultSet rs = (( Statement ) st ). executeQuery ( \"Select * from Customer\" ); ){ rs . beforeFirst (); // move the cursor to before first record; rs . absolute ( 1 ); // goto first row System . out . println ( rs . getString ( 1 ) + \" : \" + rs . getString ( 2 )); } catch ( SQLException e ){ DBUtil . showErrorMessage ( e ); } } } Updatable ResultSet You can also create an updatable resultset. The updates in the result set will make changes to the database. Most common methods are as follows. updateRow() --> update to db deleteRow() --> delete from db refreshRow() --> refresh in rs from db cancelRowUpdates() --> cancel changes in rs insertRow() --> insert in db Some databases will support updateable resultsets. Using updatable resultsets is a good approach for small datasets but for very large datasets it will create performance issues. import java.sql.Connection ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class UpdatableResultSetDemo { public static void main ( String [] args ) throws SQLException { try ( Connection conn = DBUtil . getConnection ( DBType . ORADB ); Statement stmt = conn . createStatement ( ResultSet . TYPE_SCROLL_INSENSITIVE , ResultSet . CONCUR_UPDATABLE ); // creating updatable resultset ResultSet rs = stmt . executeQuery ( \"Select Department_Id, Department_Name, Manager_Id, Location_Id from Departments\" ); ) { // updating a row rs . absolute ( 6 ); rs . updateString ( \"Department_Name\" , \"Information Technology\" ); rs . updateRow (); System . out . println ( \"Record Updated Successfully\" ); // inserting a row rs . moveToInsertRow (); rs . updateInt ( \"Department_Id\" , 999 ); rs . updateString ( \"Department_Name\" , \"Training\" ); rs . updateInt ( \"Manager_Id\" , 200 ); rs . updateInt ( \"Location_Id\" , 1800 ); rs . insertRow (); System . out . println ( \"Record Inserted Successfully\" ); } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } } } Prepared statements Improves performance of the app. Easy to set SQL parameter value Prevent SQL dependency Injection Attacks Although you can use prepared statement for objects with no parameter but most often you will use them for objects with parameters. // Inserting with prepared statement import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.Date ; import java.sql.SQLException ; import java.text.ParseException ; import java.text.SimpleDateFormat ; import java.util.Scanner ; public class TestPreparedInsert { public static void main ( String [] args ) throws SQLException { // TODO Auto-generated method stub Connection conn = DBUtil . getConnection ( DBType . ORADB ); int empno ; String ename , email ; java . sql . Date hiredate ; double salary ; Scanner scanner = new Scanner ( System . in ); System . out . print ( \"Enter Employee ID :\" ); empno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter Employee Name :\" ); ename = scanner . nextLine (); System . out . print ( \"Ente Email : \" ); email = scanner . nextLine (); System . out . print ( \"Enter Date of Joining : \" ); hiredate = java . sql . Date . valueOf ( scanner . nextLine ()); System . out . print ( \"Enter Salary : \" ); salary = scanner . nextDouble (); String sql = \"insert into NewEmployees values ( ?,?,?,?,? )\" ; PreparedStatement pstmt = conn . prepareStatement ( sql ); pstmt . setInt ( 1 , empno ); pstmt . setString ( 2 , ename ); pstmt . setString ( 3 , email ); pstmt . setDate ( 4 , hiredate ); pstmt . setDouble ( 5 , salary ); int result = pstmt . executeUpdate (); if ( result == 1 ) { System . out . println ( \"Record Inserted Successfully.\" ); } else { System . err . println ( \"Error while adding the record.\" ); } scanner . close (); pstmt . close (); conn . close (); } } Updating the record via prepared statement import java.sql.Connection ; import java.sql.SQLException ; import java.util.Scanner ; import java.sql.PreparedStatement ; public class TestPreparedUpdate { public static void main ( String [] args ) throws SQLException { Connection conn = DBUtil . getConnection ( DBType . ORADB ); String sql = \"Update NewEmployees set Salary = ? where Employee_Id = ?\" ; Scanner scanner = new Scanner ( System . in ); System . out . print ( \"Enter Employee ID :\" ); int empno = scanner . nextInt (); System . out . print ( \"Enter New Salary : \" ); double salary = scanner . nextDouble (); PreparedStatement pstmt = conn . prepareStatement ( sql ); pstmt . setDouble ( 1 , salary ); pstmt . setInt ( 2 , empno ); int result = pstmt . executeUpdate (); if ( result == 1 ) { System . out . println ( \"Employee Salary Updated Successfully.\" ); } else { System . err . println ( \"Error while updating the Salary.\" ); } scanner . close (); pstmt . close (); conn . close (); } } Deleting the record via prepared statement. import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.SQLException ; import java.util.Scanner ; public class TestPreparedStatementDelete { public static void main ( String [] args ) throws SQLException { Connection conn = DBUtil . getConnection ( DBType . ORADB ); String sql = \"Delete from NewEmployees where Employee_Id = ?\" ; Scanner scanner = new Scanner ( System . in ); System . out . print ( \"Enter Employee ID :\" ); int empno = scanner . nextInt (); PreparedStatement pstmt = conn . prepareStatement ( sql ); pstmt . setInt ( 1 , empno ); int result = pstmt . executeUpdate (); if ( result == 1 ){ System . out . println ( \"Employee Record Removed Successfully.\" ); } else { System . err . println ( \"Error While Removing Employee Record.\" ); } scanner . close (); pstmt . close (); conn . close (); } }","title":"Basic CRUD operations"},{"location":"Java/6.%20Database%20with%20JDBC/#working-with-stored-procedures","text":"Stored Procedures - Stored procedures are a set of SQL statements that perform a particular task. They are useful when you are dealing with complex scenario which may require multiple statements. So instead of executing multiple updates form java to JDBC we create the stored procedure in DB and send the data to that stored procedure in DB. The stored procedure in DB then makes the changes. Its benefitial in case of roll backs. deals better with partial updates (think of multiple updates from db if 1 failes then we have to roll back all of them etc...) performance is also better Each database has its own language for creating stored procedure. The DBA usually creates the stored procedures. - Oracle --> PLSQL - MYSQL --> Stored procedure language - SQLServer --> Transact SQL In JDBC we use callable statements to make a call to Stored Procedures. Demo create a new stored procedure AddNewEmployee in Oracle DB. from jdbc call the store procedure. import java.sql.Connection ; import java.sql.Date ; import java.sql.SQLException ; import java.util.Scanner ; import java.sql.CallableStatement ; public class TestCallableIn { public static void main ( String [] args ) { try ( Connection conn = DBUtil . getConnection ( DBType . ORADB ); // statement type is prepareCall and in \"{}\" // for values we just put ? CallableStatement callableStatement = conn . prepareCall ( \"{call AddNewEmployee(?,?,?,?,?)}\" ); // callable statement ) { Scanner scanner = new Scanner ( System . in ); System . out . print ( \"Enter Employee # : \" ); int empno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter Employee Name : \" ); String ename = scanner . nextLine (); System . out . print ( \"Enter Email ID :\" ); String email = scanner . nextLine (); System . out . print ( \"Enter Hiredate : \" ); Date doj = java . sql . Date . valueOf ( scanner . nextLine ()); System . out . print ( \"Enter Salary :\" ); double salary = scanner . nextDouble (); // setting the values as we did for other statements callableStatement . setInt ( 1 , empno ); callableStatement . setString ( 2 , ename ); callableStatement . setString ( 3 , email ); callableStatement . setDate ( 4 , doj ); callableStatement . setDouble ( 5 , salary ); // call execute method callableStatement . execute (); System . out . println ( \"Employee Record Added Successfully.\" ); } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } } } Batch processing in JDBC Each time a request is made to server we usually establish a connection. Each time database updates java application. lets say we have to insert 100 records then its 100 times to and fro communication between java and sql which leads to network and performance issues. Thats why its benefitial to do batch processing. We do that by creating a batch in jdbc and then submit to database. The batch processing is supported by statement, preparedStatement and CallableStatement Methods to keep in mind void addBatch() int[] executeBatch() import java.sql.Connection ; import java.sql.Date ; import java.sql.SQLException ; import java.util.Scanner ; import java.sql.CallableStatement ; public class TestCallableBatchProcessing { public static void main ( String [] args ) { try ( Connection conn = DBUtil . getConnection ( DBType . ORADB ); CallableStatement callableStatement = conn . prepareCall ( \"call AddNewEmployee(?,?,?,?,?)\" ); ){ String option ; do { Scanner scanner = new Scanner ( System . in ); System . out . print ( \"Enter Employee # : \" ); int empno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter Employee Name : \" ); String ename = scanner . nextLine (); System . out . print ( \"Enter Email ID :\" ); String email = scanner . nextLine (); System . out . print ( \"Enter Hiredate : \" ); Date dob = java . sql . Date . valueOf ( scanner . nextLine ()); System . out . print ( \"Enter Salary :\" ); double salary = Double . parseDouble ( scanner . nextLine ()); callableStatement . setInt ( 1 , empno ); callableStatement . setString ( 2 , ename ); callableStatement . setString ( 3 , email ); callableStatement . setDate ( 4 , dob ); callableStatement . setDouble ( 5 , salary ); callableStatement . addBatch (); // adding to batch instead of executing it. System . out . print ( \"Do You Want To Add Another Record (yes /no): \" ); option = scanner . nextLine (); } while ( option . equals ( \"yes\" )); int [] updateCounts = callableStatement . executeBatch (); // executing the batch, which returns array of ints. System . out . println ( \"Total Records Inserted are : \" + updateCounts . length ); } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } } } Out parameters Stored procedures can not only take IN parameters but can also return OUT parameters. e.g. in case you wish that the database does some sort of count and returns back the value. The below example shows that the stored procedure takes in a value of department ID and returns how many people are in that department ID. import java.sql.CallableStatement ; import java.sql.Connection ; import java.sql.SQLException ; import java.sql.Types ; import java.util.Scanner ; public class TestCallableOut { public static void main ( String [] args ) { // TODO Auto-generated method stub try ( Connection conn = DBUtil . getConnection ( DBType . ORADB ); // In case of IN or OUT parameter the callable Statement definition remains the same. i.e. we just put ? for both types of params. // In this case `GetTotalEmployeesByDepartment` is the stored procedure name. CallableStatement callableStatement = conn . prepareCall ( \"{ call GetTotalEmployeesByDepartment(?,?) }\" ); Scanner scanner = new Scanner ( System . in ); ) { System . out . print ( \"Enter Department ID : \" ); int deptno = Integer . parseInt ( scanner . nextLine ()); // providing value for IN param. callableStatement . setInt ( 1 , deptno ); // registring the out param. callableStatement . registerOutParameter ( 2 , Types . INTEGER ); callableStatement . execute (); // once the statement is executed we get the value of the out param, in this case it was the 2nd param int totalEmployees = callableStatement . getInt ( 2 ); System . out . println ( \"Total Employees Working : \" + totalEmployees ); } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } } } IN OUT parameter The same parameter can be used as both IN and OUT parameters. lets say we send some value and db\\'s stored procedure modify\\'s the value and then stores in teh database. Then the stored procedure returns the updated value back to JAVA. In the example below for a course, the db takes in orignal fee and retuns the final fee as OUT parameter after calculating the discount. (For the details of the stored procedure refer to the slides.) import java.sql.CallableStatement ; import java.sql.Connection ; import java.sql.SQLException ; import java.sql.Types ; import java.util.Scanner ; public class TestCallableInOut { public static void main ( String [] args ) throws SQLException { Connection conn = null ; CallableStatement callableStatement = null ; Scanner scanner = null ; try { conn = DBUtil . getConnection ( DBType . ORADB ); callableStatement = conn . prepareCall ( \"{call GetCourseFeesById(?,?)}\" ); scanner = new Scanner ( System . in ); // Calling Stored procedure 1 // Get the course details with orginal fees. System . out . print ( \"Enter Course ID :\" ); int cid = Integer . parseInt ( scanner . nextLine ()); callableStatement . setInt ( 1 , cid ); callableStatement . registerOutParameter ( 2 , Types . DOUBLE ); callableStatement . execute (); double fees = callableStatement . getDouble ( 2 ); System . out . println ( \"Course Fees : \" + fees ); // Calling Stored procedure 2 // Get the student details System . out . print ( \"Enter Roll Number : \" ); int rno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter Student Name :\" ); String sname = scanner . nextLine (); System . out . print ( \"Enter your Percentage : \" ); double percentage = Double . parseDouble ( scanner . nextLine ()); callableStatement = conn . prepareCall ( \"{ call EnrollStudent(?,?,?,?,?) }\" ); callableStatement . setInt ( \"rno\" , rno ); callableStatement . setString ( \"sname\" , sname ); callableStatement . setInt ( \"cid\" , cid ); callableStatement . setDouble ( \"cfees\" , fees ); callableStatement . setDouble ( \"spercent\" , percentage ); // Register the same variale as OUT parameter which was used as IN parameter also. callableStatement . registerOutParameter ( \"cfees\" , Types . DOUBLE ); callableStatement . execute (); // get the value sent by the database fees = callableStatement . getDouble ( \"cfees\" ); // display it. System . out . println ( sname + \" enrolled for the Course with the ID \" + cid + \" and Final Fees is \" + fees ); } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } finally { scanner . close (); callableStatement . close (); conn . close (); } } } Stored procedure returning ResultSets A stored procedure can also return a resultset. In the example below a stored procedure is returning a oracle cursor. We then need to type cast that to resultset and then loop on it. (Personal opinion --> looks like a lot of boilerplate code, ORM might be better or some DBUtils packages) import java.sql.CallableStatement ; import java.sql.Connection ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.util.Scanner ; import oracle.jdbc.internal.OracleCallableStatement ; import oracle.jdbc.OracleTypes ; public class TestCallableResultSet { public static void main ( String [] args ) throws SQLException { // TODO Auto-generated method stub try ( Connection conn = DBUtil . getConnection ( DBType . ORADB ); // same as always teh 2nd param will be the resultset CallableStatement callableStatement = conn . prepareCall ( \"{call GetEmployeesByRefCursor(?,?)}\" ); Scanner scanner = new Scanner ( System . in ); ){ System . out . print ( \"Enter Department ID : \" ); int deptno = Integer . parseInt ( scanner . nextLine ()); callableStatement . setInt ( 1 , deptno ); callableStatement . registerOutParameter ( 2 , OracleTypes . CURSOR ); // notice the type callableStatement . execute (); // typecast what is returned to resultset ResultSet rs = (( oracle . jdbc . internal . OracleCallableStatement ) callableStatement ). getCursor ( 2 ); String format = \"%-4s%-50s%-25s%-10f\\n\" ; // loop and display values. while ( rs . next ()){ System . out . format ( format , rs . getString ( \"Employee_ID\" ), rs . getString ( \"Employee_Name\" ), rs . getString ( \"Email\" ), rs . getFloat ( \"Salary\" )); } } catch ( SQLException ex ){ DBUtil . showErrorMessage ( ex ); } } }","title":"Working with stored procedures"},{"location":"Java/6.%20Database%20with%20JDBC/#transaction-management-using-jdbc","text":"Lets say we have 2 people. We are deducting \\$100 from personA and depositing to personB. The first operation of deductoin passes but the next one of deposit to personB fails. In this case transaction management can be used to specify commit all or rollback all. import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.util.Scanner ; public class TestTransactionManagement { public static void main ( String [] args ) throws SQLException { try { Connection conn = DBUtil . getConnection ( DBType . ORADB ); // set the auto commit functionality to false. conn . setAutoCommit ( false ); PreparedStatement pstmt = null ; Scanner scanner = new Scanner ( System . in ); System . out . println ( \"PSBank Transactions\" ); System . out . println ( \"----------------------\" ); System . out . print ( \"Enter From Account # :\" ); int fromAcno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter To Account # : \" ); int toAcno = Integer . parseInt ( scanner . nextLine ()); System . out . print ( \"Enter Amount To Transfer : \" ); double amount = Double . parseDouble ( scanner . nextLine ()); String withdrawSQL = \"Update PSBank set Amount = Amount - ? where Acno = ?\" ; pstmt = conn . prepareStatement ( withdrawSQL ); pstmt . setDouble ( 1 , amount ); pstmt . setInt ( 2 , fromAcno ); pstmt . executeUpdate (); String depositSQL = \"Update PSBank set Amount = Amount + ? where Acno = ?\" ; pstmt = conn . prepareStatement ( depositSQL ); pstmt . setDouble ( 1 , amount ); pstmt . setInt ( 2 , toAcno ); pstmt . executeUpdate (); String sql = \"Select Amount From PSBank where Acno = ?\" ; pstmt = conn . prepareStatement ( sql ); pstmt . setInt ( 1 , fromAcno ); ResultSet rs = pstmt . executeQuery (); double balanceAmount = 0 ; if ( rs . next ()){ balanceAmount = rs . getDouble ( \"Amount\" ); } if ( balanceAmount >= 5000 ){ // if everything is ok we then commit. conn . commit (); System . out . println ( \"Amount Transferred Successfully...\" ); } else { // else we rollback. conn . rollback (); System . out . println ( \"Insufficient Funds : \" + balanceAmount + \" Transactions Rollbacked..\" ); } scanner . close (); pstmt . close (); conn . close (); } catch ( Exception ex ){ System . err . println ( ex . getMessage ()); } } }","title":"Transaction Management using JDBC"},{"location":"Java/6.%20Database%20with%20JDBC/#working-with-clob-and-blob","text":"CLOB CLOB -> (Character Large Object) is a collection of character data stored in database as single entity. Used to store large text documents e.g. plain text or xml. Also not all database support CLOB. MYSQL you need to use LongText which can store upto 4GB data. In ORACLE we use CLOB. In order to store the file we need to get the file. Read the contents of the file with help of any input stream reader. Convert contents to character with help of any ASCI string Then pass to database using prepared statement or callable statement In order to read the CLOB data from database use getClob() Inserting CLOB data import java.io.File ; import java.io.FileNotFoundException ; import java.io.FileReader ; import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.SQLException ; public class InsertCLOBDataToEmpResume { public static void main ( String [] args ) throws SQLException , FileNotFoundException { Connection conn = DBUtil . getConnection ( DBType . ORADB ); PreparedStatement pstmt = null ; // creating the prepared statement the same way. String sql = \"Update NewEmployees set Resume = ? where Employee_ID = 500\" ; pstmt = conn . prepareStatement ( sql ); // read the contents of the file String resumeFile = \"d:/PluralSight Demos/SekharResume.txt\" ; File file = new File ( resumeFile ); FileReader reader = new FileReader ( file ); // Set the file reader into the prepared statement pstmt . setCharacterStream ( 1 , reader , ( int ) file . length ()); // execute the prepared statement to store the file in database pstmt . executeUpdate (); System . out . println ( \"Resume Updated Successfully...\" ); pstmt . close (); conn . close (); } } Fetch CLOB data from DB import java.io.FileWriter ; import java.io.IOException ; import java.io.Reader ; import java.sql.Clob ; import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.ResultSet ; import java.sql.SQLException ; public class RetrieveCLOBDataFromDB { public static void main ( String [] args ) throws SQLException , IOException { Connection conn = DBUtil . getConnection ( DBType . ORADB ); String sql = \"select Resume from NewEmployees where Employee_ID = 500\" ; PreparedStatement pstmt = conn . prepareStatement ( sql ); ResultSet rs = pstmt . executeQuery (); if ( rs . next ()){ // fetch the first row in RS Clob resume = rs . getClob ( \"Resume\" ); Reader data = resume . getCharacterStream (); int i ; String resumeDetails = \"\" ; // conctenate the characters to create a file while ( ( i = data . read ()) != - 1 ){ resumeDetails += (( char ) i ); } System . out . println ( \"Resume Details for Employee 500\" ); System . out . println ( resumeDetails ); } else { System . err . println ( \"No Record Found For Employee With The ID 500.\" ); } rs . close (); pstmt . close (); conn . close (); } } BLOB Data Blob data stands for Binary Large Object. E.g. Pictures, documents etc... Sending data to DB import java.io.File ; import java.io.FileInputStream ; import java.io.IOException ; import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.SQLException ; public class InsertImageWithinDB { public static void main ( String [] args ) throws SQLException , IOException { // TODO Auto-generated method stub Connection conn = DBUtil . getConnection ( DBType . ORADB ); String sql = \"Update NewEmployees Set Photo = ? where Employee_ID = 500\" ; PreparedStatement pstmt = conn . prepareStatement ( sql ); File file = new File ( \"D:/PluralSight Demos/Sekhar.jpg\" ); FileInputStream fis = new FileInputStream ( file ); pstmt . setBinaryStream ( 1 , fis , fis . available ()); int count = pstmt . executeUpdate (); System . out . println ( \"Total Records Updated : \" + count ); pstmt . close (); conn . close (); } } Retrive BLOB data from DB import java.io.FileOutputStream ; import java.io.IOException ; import java.sql.Blob ; import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.ResultSet ; import java.sql.SQLException ; public class RetrieveImageFromDB { public static void main ( String [] args ) throws SQLException , IOException { Connection conn = DBUtil . getConnection ( DBType . ORADB ); String sql = \"Select Photo From NewEmployees Where Employee_Id = 500\" ; PreparedStatement pstmt = conn . prepareStatement ( sql ); ResultSet rs = pstmt . executeQuery (); if ( rs . next ()){ Blob imgBlob = rs . getBlob ( \"Photo\" ); FileOutputStream fos = new FileOutputStream ( \"D:/PluralSight Demos/Downloads/img500.jpg\" ); fos . write ( imgBlob . getBytes ( 1 , ( int ) imgBlob . length ())); fos . flush (); fos . close (); System . out . println ( \"Photo of Employee 500 has been Downloaded successfully\" ); } else { System . out . println ( \"Employee Record Not Found.\" ); } rs . close (); pstmt . close (); conn . close (); } }","title":"Working with CLOB and BLOB"},{"location":"Java/6.%20Database%20with%20JDBC/#working-with-metadata","text":"Check slides and programs","title":"Working with metadata"},{"location":"Java/6.%20Database%20with%20JDBC/#connection-pooling","text":"Usually each request to DB will not create its own connection to DB (as establishing a connection to DB is very time consuming.) We use the connection pooling to request already established connections to the database and get the logical link to it (not the physical connection itself). Once the transaction to DB is complete we send back the connection to the connection pool. import java.sql.Connection ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; import javax.sql.PooledConnection ; import oracle.jdbc.pool.OracleConnectionPoolDataSource ; public class ConnectionPoolingDemo { public static void main ( String [] args ) throws SQLException { OracleConnectionPoolDataSource ds = new OracleConnectionPoolDataSource (); ds . setDriverType ( \"thin\" ); ds . setServerName ( \"localhost\" ); ds . setPortNumber ( 1521 ); ds . setServiceName ( \"xe\" ); ds . setUser ( \"hr\" ); ds . setPassword ( \"hr\" ); PooledConnection pconn = ds . getPooledConnection (); Connection conn = pconn . getConnection (); Statement stmt = conn . createStatement (); ResultSet rs = stmt . executeQuery ( \"Select * From Departments\" ); String format = \"%-30s%-50s%-25s\\n\" ; System . out . format ( format , \"Department #\" , \"Department Name\" , \"Location\" ); System . out . format ( format , \"-------------\" , \"-----------------\" , \"-------------\" ); while ( rs . next ()){ System . out . format ( format , rs . getString ( \"Department_ID\" ), rs . getString ( \"Department_Name\" ), rs . getString ( \"Location_Id\" )); } rs . close (); stmt . close (); conn . close (); pconn . close (); } }","title":"Connection pooling"},{"location":"Java/99.%20IDE/","text":"IDE Intellij IDEA Create JAR File Goto File --> Project Structure --> Artifacts --> Add a new artifact If its a Maven project create a folder in Output Layout called META-INF and add the MANIFEST.MF file from your source dir there.","title":"IDE"},{"location":"Java/99.%20IDE/#ide","text":"","title":"IDE"},{"location":"Java/99.%20IDE/#intellij-idea","text":"Create JAR File Goto File --> Project Structure --> Artifacts --> Add a new artifact If its a Maven project create a folder in Output Layout called META-INF and add the MANIFEST.MF file from your source dir there.","title":"Intellij IDEA"},{"location":"Java/Generics/","text":"Note general concepts of generics from youtube or book which are more basic than this one. Basics The java collections can take any type of value as shown below. This will create a problem because we may not be able to control what type of value will go inside the list and cannot program reliably. package com.company ; import java.util.ArrayList ; import java.util.List ; public class Main { public static void main ( String [] args ) { List list = new ArrayList (); list . add ( 1 ); list . add ( \"HEllo world\" ); for ( Object a : list ) { System . out . println ( a ); } } } Instead we should have a list which only contains Strings thus we can work reliably with it. This is done with the help of generics similar to in C++. List < String > list = new ArrayList <> (); Creating a simple generic class. package com.company ; public class CircularBuffer < T > { private T [] buffer ; private int readCursor ; private int writeCursor ; public CircularBuffer ( int size ){ buffer = ( T [] ) new Object [ size ] ; // creating an object array of size and then cast to type T } public boolean offer ( T value ) { if ( buffer [ writeCursor ] != null ) { return false ; } buffer [ writeCursor ] = value ; writeCursor = next ( writeCursor ); return true ; } public T poll () { final T value = buffer [ readCursor ] ; if ( value != null ) { buffer [ readCursor ] = null ; readCursor = next ( readCursor ); } return value ; } private int next ( int index ) { return ( index + 1 ) % buffer . length ; } public static void main ( String [] args ) { CircularBuffer < String > buffer = new CircularBuffer <> ( 10 ); buffer . offer ( \"a\" ); buffer . offer ( \"bc\" ); buffer . offer ( \"d\" ); StringBuilder result = new StringBuilder (); String value ; while (( value = buffer . poll ()) != null ) { result . append ( value ); } System . out . println ( result . toString ()); } } Constraints Lets say you wish to create a generics class which should only take in types of numbers such as Integer, Short, etc... In that case when creating the extend the Numbers class class MyGenericClass < T extends Numbers > { } You can also do this with interface e.g. if you only wish to store objects which implements comparable specify that class MyGenericList < T extends Comparable > { } You can also specify multiple interfaces class MyGenericList < T extends Comparable & Cloneable > { } You can also have generic methods in non generic classes as shown below and can apply certain constraints on them. public class Utils { public static < T extends Comparable < T >> max ( T first , T last ){ // do your thing } } You can have multiple type parameters for generics as shown below. public static < K , V > void print ( K key , V value ){ } You can do the same with classes public class KeyValuePair < K , V > { public K key ; public V value ; public KeyValuePair ( K key , V value ){ this . key = key ; this . value = value ; } } Generics wildcards TBD","title":"Generics"},{"location":"Java/Generics/#basics","text":"The java collections can take any type of value as shown below. This will create a problem because we may not be able to control what type of value will go inside the list and cannot program reliably. package com.company ; import java.util.ArrayList ; import java.util.List ; public class Main { public static void main ( String [] args ) { List list = new ArrayList (); list . add ( 1 ); list . add ( \"HEllo world\" ); for ( Object a : list ) { System . out . println ( a ); } } } Instead we should have a list which only contains Strings thus we can work reliably with it. This is done with the help of generics similar to in C++. List < String > list = new ArrayList <> (); Creating a simple generic class. package com.company ; public class CircularBuffer < T > { private T [] buffer ; private int readCursor ; private int writeCursor ; public CircularBuffer ( int size ){ buffer = ( T [] ) new Object [ size ] ; // creating an object array of size and then cast to type T } public boolean offer ( T value ) { if ( buffer [ writeCursor ] != null ) { return false ; } buffer [ writeCursor ] = value ; writeCursor = next ( writeCursor ); return true ; } public T poll () { final T value = buffer [ readCursor ] ; if ( value != null ) { buffer [ readCursor ] = null ; readCursor = next ( readCursor ); } return value ; } private int next ( int index ) { return ( index + 1 ) % buffer . length ; } public static void main ( String [] args ) { CircularBuffer < String > buffer = new CircularBuffer <> ( 10 ); buffer . offer ( \"a\" ); buffer . offer ( \"bc\" ); buffer . offer ( \"d\" ); StringBuilder result = new StringBuilder (); String value ; while (( value = buffer . poll ()) != null ) { result . append ( value ); } System . out . println ( result . toString ()); } } Constraints Lets say you wish to create a generics class which should only take in types of numbers such as Integer, Short, etc... In that case when creating the extend the Numbers class class MyGenericClass < T extends Numbers > { } You can also do this with interface e.g. if you only wish to store objects which implements comparable specify that class MyGenericList < T extends Comparable > { } You can also specify multiple interfaces class MyGenericList < T extends Comparable & Cloneable > { } You can also have generic methods in non generic classes as shown below and can apply certain constraints on them. public class Utils { public static < T extends Comparable < T >> max ( T first , T last ){ // do your thing } } You can have multiple type parameters for generics as shown below. public static < K , V > void print ( K key , V value ){ } You can do the same with classes public class KeyValuePair < K , V > { public K key ; public V value ; public KeyValuePair ( K key , V value ){ this . key = key ; this . value = value ; } } Generics wildcards TBD","title":"Basics"},{"location":"Java/Lambda%20%26%20Streams/","text":"Lambda and Streams Lambda Lets start with a simple example. To print stuff on the console we created a ConsolePrinter class. The contract is simple and implemented by the interface Printer. // A functional interface which has 1 unimplemented method as below. (It can have other default methods which are implemented) interface Printer { void print ( String message ); } class ConsolePrinter implements Printer { @Override public void print ( String message ) { System . out . println ( message ); } } public class Main14 { public static void main ( String [] args ) { show (); } public static void show (){ greet ( new ConsolePrinter ()); } public static void greet ( Printer printer ){ printer . print ( \"Hello World\" ); } } But why create a class (where actually it will be a .java file), instead lets create anonymus class to replace the ConsolePrinter class. // A functional interface which has 1 unimplemented method as below. (It can have other default methods which are implemented) interface Printer { void print ( String message ); } public class Main14 { public static void main ( String [] args ) { show (); } public static void show (){ greet ( new Printer (){ // Anonymous class @Override public void print ( String message ) { System . out . println ( message ); } }); } public static void greet ( Printer printer ){ printer . print ( \"Hello World\" ); } } Java 8 provided a simpler way to achieve the same result, without writing anonymous inner class. Its called a lambda expression. // A functional interface which has 1 unimplemented method as below. (It can have other default methods which are implemented) interface Printer { void print ( String message ); } public class Main14 { public static void main ( String [] args ) { show (); } public static void show (){ greet ( ( message ) -> System . out . println ( message )); // lambda expression } public static void greet ( Printer printer ){ printer . print ( \"Hello World\" ); } } You can also access local parameters in lambda expressions. public static void show (){ String prefix = \"-\" ; greet ( message -> System . out . println ( prefix + message )); // lambda expression } Method References Sometimes all we do in lambda expression is to pass the variable along to another function such as above where we pass it to println. In this case we can use method references as shown below. id show (){ String prefix = \"-\" ; greet ( message -> System . out :: println )); // Class::method <-- method reference, static or instance or even a constructor. } Types of Functional Interfaces in Java Oracle Docs Link java.util.function. has all the functional interfaces in Java. Consumer : Takes single argument and returns no result. Supplier : Takes no input and returns output. Function : Map a value to a different value Predicate: Takes an input and checks whether that input satifies a criteria. Boolean return. Consumer Interface /* Consumer Interface. */ import java.util.ArrayList ; import java.util.List ; public class Main15 { public static void main ( String [] args ) { List < Integer > list = new ArrayList <> (); list . add ( 1 ); list . add ( 2 ); list . add ( 3 ); list . forEach ( System . out :: println ); // forEach looks for consumer interface and we created a lambda implementation for that consumer interface. } } Chaining consumers. import java.util.ArrayList ; import java.util.List ; import java.util.function.Consumer ; public class Main16 { public static void main ( String [] args ) { List < String > list = new ArrayList <> (); list . add ( \"a\" ); list . add ( \"b\" ); list . add ( \"c\" ); // Creating an implementation of Consumer Interface. Consumer < String > print = System . out :: println ; Consumer < String > printUpperCase = ( item ) -> System . out . println ( item . toUpperCase ()); // Chaining consumers. list . forEach ( print . andThen ( printUpperCase ). andThen ( print )); } } The supplier interface. import java.util.function.Supplier ; public class Main17 { public static void main ( String [] args ) { // Check javadoc for Supplier Interface. Supplier < Double > getRandom = Math :: random ; System . out . println ( getRandom . get ()); } } Example of Function interface. Again as usual check out the java docs to see the contract of the function interface. import java.util.function.Function ; public class Main18 { public static void main ( String [] args ) { Function < String , Integer > map = String :: length ; System . out . println ( map . apply ( \"hello world\" )); } } /* Another more complicated example of Function */ import java.util.function.Function ; public class Main19 { public static void main ( String [] args ) { Function < String , String > replace = ( str ) -> str . replace ( \":\" , \"=\" ); Function < String , String > addBraces = ( str ) -> \"{\" + str + \"}\" ; String result = replace . andThen ( addBraces ) . apply ( \"key:value\" ); System . out . println ( result ); } } The predicate interface. import java.util.function.Predicate ; public class Main20 { public static void main ( String [] args ) { Predicate < String > isLongerThan5 = ( str ) -> str . length () > 5 ; Boolean result = isLongerThan5 . test ( \"Hello World\" ); System . out . println ( result ); } } Combining predicate. import java.util.function.Predicate ; public class Main21 { public static void main ( String [] args ) { Predicate < String > hasLeftBrace = ( str ) -> str . startsWith ( \"{\" ); Predicate < String > hasRightBrace = ( str ) -> str . endsWith ( \"}\" ); System . out . println ( hasLeftBrace . and ( hasRightBrace ). test ( \"{hello world}\" )); System . out . println ( hasLeftBrace . and ( hasRightBrace ). test ( \"{hello worldf\" )); } } Streams Streams were added to Java so that we can process a logic in functional way. Every collection in Java returns a stream of data. /* Streams */ import java.util.ArrayList ; import java.util.List ; class Movie { private final String name ; private final int likes ; public Movie ( String name , int likes ) { this . name = name ; this . likes = likes ; } public int getLikes () { return likes ; } public String getName () { return name ; } } public class Main22 { public static void main ( String [] args ) { List < Movie > movies = new ArrayList <> (); movies . add ( new Movie ( \"a\" , 10 )); movies . add ( new Movie ( \"b\" , 15 )); movies . add ( new Movie ( \"c\" , 20 )); long count = movies . stream () . filter (( movie ) -> movie . getLikes () > 10 ) . count (); System . out . println ( count ); } } We can create a stream from - Collections e.g. list.stream() - Array e.g. Arrays.stream({1,2,3}) - Arbitrary # of objects e.g. Stream.of(new A2(), new A2()).forEach(System.out::println); - Infinite/Finite Streams. e.g. Stream.generate(() -> Math.random()) You can also limit it by using .limit(20) to not have infinite stream. Another way --> Stream.iterate(1, n -> n+1).limit(20).forEach(System.out::println); Map method Applys that method to all objects/values in the stream. /* Streams */ import java.util.ArrayList ; import java.util.List ; class Movie1 { private final String name ; private final int likes ; public Movie1 ( String name , int likes ) { this . name = name ; this . likes = likes ; } public int getLikes () { return likes ; } public String getName () { return name ; } } public class Main23 { public static void main ( String [] args ) { List < Movie1 > movies = new ArrayList <> (); movies . add ( new Movie1 ( \"a\" , 10 )); movies . add ( new Movie1 ( \"b\" , 15 )); movies . add ( new Movie1 ( \"c\" , 20 )); movies . stream () . map ( movie -> movie . getName ()) . forEach ( System . out :: println ); } } Flat Map Lets say if we have a steam of list of integers e.g. integers listed under 2nd hierarchy and we want to work with the intergers use flatmap to get list of integers. ```java import java.util.ArrayList; import java.util.List; import java.util.stream.Stream; public class Main24 { public static void main(String[] args) { List < Integer > list1 = new ArrayList <>(); List < Integer > list2 = new ArrayList <>(); list1 . add ( 1 ); list1 . add ( 2 ); list1 . add ( 3 ); list2 . add ( 4 ); list2 . add ( 5 ); list2 . add ( 6 ); Stream < List < Integer >> stream = Stream . of ( list1 , list2 ); // Using flatmap stream . flatMap ( list - > list . stream ()) . forEach ( System . out :: println ); } } Stream methods fall into 2 categories - Intermediate operations -> they return new stream e.g. Filter, Map - Terminal operations -> they return void and consume the stream e.g. forEach. Intermediate - map / flatmap - filter - limit / skip - sorted - distinct - peek If you do not call terminal operator on your stream nothing happens. This is called lazy evaluation. __Slicing__ - limit(n) - skip(n) - takeWhile(Predicate) - dropWhile(Predicate) We should get the first 2 movies from the list. ```java movies.stream() .limit(2) We should skip the first 2 movies from the list. movies . stream () . skip ( 2 ) Takewhile takes a Predicate and returns movies which get true for that predicate. (akeWhile stops the stream on first false from predicate where as filter does not it iterates through entire stream) Takewhile takes a Predicate and returns movies which get true for that predicate. Dropwhile is opposite of dropWhile. Sorting movies . stream () . sorted (( a , b ) -> a . getName (). compareTo ( b . getName ())) movies . stream () . sorted ( Comparator . comparing ( m -> m . getName ())) movies . stream () . sorted ( Comparator . comparing ( Movie :: getName )) movies . stream () . sorted ( Comparator . comparing ( Movie :: getName ). reversed ()) Unique values movies . stream () . map ( Movie :: getLikes ) . distinct () // unique values using distinct() . forEach ( System . out :: println ) Peeking Lets say if you wish to peek into the stream after every operation (for debugging etc...) then you can do that with peek method. movies . stream () . filter ( m -> m . getLikes () > 10 ) . peek ( m -> System . out . println ( \"filtered\" + m . getName ())) . map ( Movie :: getName ) . peek ( title -> System . out . println ( \"map :\" + title )) . forEach ( System . out :: println ) Reducers All these are terminal operations. count() anyMatch(Predicate) --> returns a boolean (If any object satisfies the condition) allMatch(Predicate) --> returns a boolean (If all object satisfy the condition) noneMatch(Predicate) --> returns a boolean (If none object satisfy the condition) findFirst() --> returns the first object. then do .get() to get the object. findAny() --> returns any random object. then do .get() to get the object. max(Comparator) min(Comparator) General purpose reducer ==> .reduce() In the below example we use reduce to count total # of likes for all movies. movies . stream () . map ( m -> m . getLikes ()) . reduce (( a , b ) -> a + b ) // can also do .reduce(Integer::sum) The above code will return you a Optional<Integer> you can call .get (not null pointer safe) or .orElse for null pointer safety. If you do not wish to worry about the Optional then you should supply a starting value e.g. 0 into the reducer so that if there are no movies it will return 0. movies . stream () . map ( m -> m . getLikes ()) . reduce ( 0 , Integer :: sum ) // now it will return int and not Optional<Integer> Collectors Quiet often we wish to get the result of processing (may be filtering movie or something smilar) and collect them into a list, set, map etc... for this we use collector. movies . stream () . filter ( m -> m . getLikes () > 10 ) . collect ( Collectors . toList ()) // returns a list of movies. Similar to above we have toSet, toMap movies . stream () . filter ( m -> m . getLikes () > 10 ) . collect ( Collectors . toMap ( Movie :: getName , Movie : getLikes )) // returns a hashmap of movies movies . stream () . filter ( m -> m . getLikes () > 10 ) . collect ( Collectors . toMap ( Movie :: getName , m -> m )) // storing movie object instead of # of likes for hashmap value. In above code you can also replace m -> m with Function.identity() which gets a value and simply returns it. movies . stream () . filter ( m -> m . getLikes () > 10 ) . collect ( Collectors . summingInt ( Movie :: getLikes )) // summing all integer values, here likes Can also do the above with reduce, the above is alternative way to do it. You also have summingDouble etc... You also have Collectors.summarizingInt and other primitives where it gives you some stats about the stream integer value you passed. We also have a joining collector Collectors.joining() e.g. you can contactenate movies names joined with some delimiter. Grouping movies . stream () . collect ( Collectors . groupingBy ( Movie :: getGenere )) // this takes in a classifier, This will return map Genere -> Movies List, you can also specify to get result in a set. collect ( Collectors . groupingBy ( Movie :: getGenere , Collectors . toSet ())) // genere to Set of movies. collect ( Collectors . groupingBy ( Movie :: getGenere , Collectors . counting ())) // genere to integer # of movies in that genere collect ( Collectors . groupingBy ( Movie :: getGenere , Collectors . mapping ( Movie :: getName , Collectors . joining ()))) // genere to String with movie names Partition data Splitting data. movies . stream () . collect ( Collectors . partitioningBy ( m -> m . getLikes > 20 )) // get a map of boolean -> movies You can also tack on a second collector as we did above. Primitive Streams IntStream LongStream DoubleStream IntStream . of IntStream . range ( 1 , 5 ) IntStream . rangeClosed ( 1 , 5 )","title":"Lambda and Streams"},{"location":"Java/Lambda%20%26%20Streams/#lambda-and-streams","text":"","title":"Lambda and Streams"},{"location":"Java/Lambda%20%26%20Streams/#lambda","text":"Lets start with a simple example. To print stuff on the console we created a ConsolePrinter class. The contract is simple and implemented by the interface Printer. // A functional interface which has 1 unimplemented method as below. (It can have other default methods which are implemented) interface Printer { void print ( String message ); } class ConsolePrinter implements Printer { @Override public void print ( String message ) { System . out . println ( message ); } } public class Main14 { public static void main ( String [] args ) { show (); } public static void show (){ greet ( new ConsolePrinter ()); } public static void greet ( Printer printer ){ printer . print ( \"Hello World\" ); } } But why create a class (where actually it will be a .java file), instead lets create anonymus class to replace the ConsolePrinter class. // A functional interface which has 1 unimplemented method as below. (It can have other default methods which are implemented) interface Printer { void print ( String message ); } public class Main14 { public static void main ( String [] args ) { show (); } public static void show (){ greet ( new Printer (){ // Anonymous class @Override public void print ( String message ) { System . out . println ( message ); } }); } public static void greet ( Printer printer ){ printer . print ( \"Hello World\" ); } } Java 8 provided a simpler way to achieve the same result, without writing anonymous inner class. Its called a lambda expression. // A functional interface which has 1 unimplemented method as below. (It can have other default methods which are implemented) interface Printer { void print ( String message ); } public class Main14 { public static void main ( String [] args ) { show (); } public static void show (){ greet ( ( message ) -> System . out . println ( message )); // lambda expression } public static void greet ( Printer printer ){ printer . print ( \"Hello World\" ); } } You can also access local parameters in lambda expressions. public static void show (){ String prefix = \"-\" ; greet ( message -> System . out . println ( prefix + message )); // lambda expression } Method References Sometimes all we do in lambda expression is to pass the variable along to another function such as above where we pass it to println. In this case we can use method references as shown below. id show (){ String prefix = \"-\" ; greet ( message -> System . out :: println )); // Class::method <-- method reference, static or instance or even a constructor. } Types of Functional Interfaces in Java Oracle Docs Link java.util.function. has all the functional interfaces in Java. Consumer : Takes single argument and returns no result. Supplier : Takes no input and returns output. Function : Map a value to a different value Predicate: Takes an input and checks whether that input satifies a criteria. Boolean return. Consumer Interface /* Consumer Interface. */ import java.util.ArrayList ; import java.util.List ; public class Main15 { public static void main ( String [] args ) { List < Integer > list = new ArrayList <> (); list . add ( 1 ); list . add ( 2 ); list . add ( 3 ); list . forEach ( System . out :: println ); // forEach looks for consumer interface and we created a lambda implementation for that consumer interface. } } Chaining consumers. import java.util.ArrayList ; import java.util.List ; import java.util.function.Consumer ; public class Main16 { public static void main ( String [] args ) { List < String > list = new ArrayList <> (); list . add ( \"a\" ); list . add ( \"b\" ); list . add ( \"c\" ); // Creating an implementation of Consumer Interface. Consumer < String > print = System . out :: println ; Consumer < String > printUpperCase = ( item ) -> System . out . println ( item . toUpperCase ()); // Chaining consumers. list . forEach ( print . andThen ( printUpperCase ). andThen ( print )); } } The supplier interface. import java.util.function.Supplier ; public class Main17 { public static void main ( String [] args ) { // Check javadoc for Supplier Interface. Supplier < Double > getRandom = Math :: random ; System . out . println ( getRandom . get ()); } } Example of Function interface. Again as usual check out the java docs to see the contract of the function interface. import java.util.function.Function ; public class Main18 { public static void main ( String [] args ) { Function < String , Integer > map = String :: length ; System . out . println ( map . apply ( \"hello world\" )); } } /* Another more complicated example of Function */ import java.util.function.Function ; public class Main19 { public static void main ( String [] args ) { Function < String , String > replace = ( str ) -> str . replace ( \":\" , \"=\" ); Function < String , String > addBraces = ( str ) -> \"{\" + str + \"}\" ; String result = replace . andThen ( addBraces ) . apply ( \"key:value\" ); System . out . println ( result ); } } The predicate interface. import java.util.function.Predicate ; public class Main20 { public static void main ( String [] args ) { Predicate < String > isLongerThan5 = ( str ) -> str . length () > 5 ; Boolean result = isLongerThan5 . test ( \"Hello World\" ); System . out . println ( result ); } } Combining predicate. import java.util.function.Predicate ; public class Main21 { public static void main ( String [] args ) { Predicate < String > hasLeftBrace = ( str ) -> str . startsWith ( \"{\" ); Predicate < String > hasRightBrace = ( str ) -> str . endsWith ( \"}\" ); System . out . println ( hasLeftBrace . and ( hasRightBrace ). test ( \"{hello world}\" )); System . out . println ( hasLeftBrace . and ( hasRightBrace ). test ( \"{hello worldf\" )); } }","title":"Lambda"},{"location":"Java/Lambda%20%26%20Streams/#streams","text":"Streams were added to Java so that we can process a logic in functional way. Every collection in Java returns a stream of data. /* Streams */ import java.util.ArrayList ; import java.util.List ; class Movie { private final String name ; private final int likes ; public Movie ( String name , int likes ) { this . name = name ; this . likes = likes ; } public int getLikes () { return likes ; } public String getName () { return name ; } } public class Main22 { public static void main ( String [] args ) { List < Movie > movies = new ArrayList <> (); movies . add ( new Movie ( \"a\" , 10 )); movies . add ( new Movie ( \"b\" , 15 )); movies . add ( new Movie ( \"c\" , 20 )); long count = movies . stream () . filter (( movie ) -> movie . getLikes () > 10 ) . count (); System . out . println ( count ); } } We can create a stream from - Collections e.g. list.stream() - Array e.g. Arrays.stream({1,2,3}) - Arbitrary # of objects e.g. Stream.of(new A2(), new A2()).forEach(System.out::println); - Infinite/Finite Streams. e.g. Stream.generate(() -> Math.random()) You can also limit it by using .limit(20) to not have infinite stream. Another way --> Stream.iterate(1, n -> n+1).limit(20).forEach(System.out::println); Map method Applys that method to all objects/values in the stream. /* Streams */ import java.util.ArrayList ; import java.util.List ; class Movie1 { private final String name ; private final int likes ; public Movie1 ( String name , int likes ) { this . name = name ; this . likes = likes ; } public int getLikes () { return likes ; } public String getName () { return name ; } } public class Main23 { public static void main ( String [] args ) { List < Movie1 > movies = new ArrayList <> (); movies . add ( new Movie1 ( \"a\" , 10 )); movies . add ( new Movie1 ( \"b\" , 15 )); movies . add ( new Movie1 ( \"c\" , 20 )); movies . stream () . map ( movie -> movie . getName ()) . forEach ( System . out :: println ); } } Flat Map Lets say if we have a steam of list of integers e.g. integers listed under 2nd hierarchy and we want to work with the intergers use flatmap to get list of integers. ```java import java.util.ArrayList; import java.util.List; import java.util.stream.Stream; public class Main24 { public static void main(String[] args) { List < Integer > list1 = new ArrayList <>(); List < Integer > list2 = new ArrayList <>(); list1 . add ( 1 ); list1 . add ( 2 ); list1 . add ( 3 ); list2 . add ( 4 ); list2 . add ( 5 ); list2 . add ( 6 ); Stream < List < Integer >> stream = Stream . of ( list1 , list2 ); // Using flatmap stream . flatMap ( list - > list . stream ()) . forEach ( System . out :: println ); } } Stream methods fall into 2 categories - Intermediate operations -> they return new stream e.g. Filter, Map - Terminal operations -> they return void and consume the stream e.g. forEach. Intermediate - map / flatmap - filter - limit / skip - sorted - distinct - peek If you do not call terminal operator on your stream nothing happens. This is called lazy evaluation. __Slicing__ - limit(n) - skip(n) - takeWhile(Predicate) - dropWhile(Predicate) We should get the first 2 movies from the list. ```java movies.stream() .limit(2) We should skip the first 2 movies from the list. movies . stream () . skip ( 2 ) Takewhile takes a Predicate and returns movies which get true for that predicate. (akeWhile stops the stream on first false from predicate where as filter does not it iterates through entire stream) Takewhile takes a Predicate and returns movies which get true for that predicate. Dropwhile is opposite of dropWhile. Sorting movies . stream () . sorted (( a , b ) -> a . getName (). compareTo ( b . getName ())) movies . stream () . sorted ( Comparator . comparing ( m -> m . getName ())) movies . stream () . sorted ( Comparator . comparing ( Movie :: getName )) movies . stream () . sorted ( Comparator . comparing ( Movie :: getName ). reversed ()) Unique values movies . stream () . map ( Movie :: getLikes ) . distinct () // unique values using distinct() . forEach ( System . out :: println ) Peeking Lets say if you wish to peek into the stream after every operation (for debugging etc...) then you can do that with peek method. movies . stream () . filter ( m -> m . getLikes () > 10 ) . peek ( m -> System . out . println ( \"filtered\" + m . getName ())) . map ( Movie :: getName ) . peek ( title -> System . out . println ( \"map :\" + title )) . forEach ( System . out :: println )","title":"Streams"},{"location":"Java/Lambda%20%26%20Streams/#reducers","text":"All these are terminal operations. count() anyMatch(Predicate) --> returns a boolean (If any object satisfies the condition) allMatch(Predicate) --> returns a boolean (If all object satisfy the condition) noneMatch(Predicate) --> returns a boolean (If none object satisfy the condition) findFirst() --> returns the first object. then do .get() to get the object. findAny() --> returns any random object. then do .get() to get the object. max(Comparator) min(Comparator) General purpose reducer ==> .reduce() In the below example we use reduce to count total # of likes for all movies. movies . stream () . map ( m -> m . getLikes ()) . reduce (( a , b ) -> a + b ) // can also do .reduce(Integer::sum) The above code will return you a Optional<Integer> you can call .get (not null pointer safe) or .orElse for null pointer safety. If you do not wish to worry about the Optional then you should supply a starting value e.g. 0 into the reducer so that if there are no movies it will return 0. movies . stream () . map ( m -> m . getLikes ()) . reduce ( 0 , Integer :: sum ) // now it will return int and not Optional<Integer> Collectors Quiet often we wish to get the result of processing (may be filtering movie or something smilar) and collect them into a list, set, map etc... for this we use collector. movies . stream () . filter ( m -> m . getLikes () > 10 ) . collect ( Collectors . toList ()) // returns a list of movies. Similar to above we have toSet, toMap movies . stream () . filter ( m -> m . getLikes () > 10 ) . collect ( Collectors . toMap ( Movie :: getName , Movie : getLikes )) // returns a hashmap of movies movies . stream () . filter ( m -> m . getLikes () > 10 ) . collect ( Collectors . toMap ( Movie :: getName , m -> m )) // storing movie object instead of # of likes for hashmap value. In above code you can also replace m -> m with Function.identity() which gets a value and simply returns it. movies . stream () . filter ( m -> m . getLikes () > 10 ) . collect ( Collectors . summingInt ( Movie :: getLikes )) // summing all integer values, here likes Can also do the above with reduce, the above is alternative way to do it. You also have summingDouble etc... You also have Collectors.summarizingInt and other primitives where it gives you some stats about the stream integer value you passed. We also have a joining collector Collectors.joining() e.g. you can contactenate movies names joined with some delimiter. Grouping movies . stream () . collect ( Collectors . groupingBy ( Movie :: getGenere )) // this takes in a classifier, This will return map Genere -> Movies List, you can also specify to get result in a set. collect ( Collectors . groupingBy ( Movie :: getGenere , Collectors . toSet ())) // genere to Set of movies. collect ( Collectors . groupingBy ( Movie :: getGenere , Collectors . counting ())) // genere to integer # of movies in that genere collect ( Collectors . groupingBy ( Movie :: getGenere , Collectors . mapping ( Movie :: getName , Collectors . joining ()))) // genere to String with movie names Partition data Splitting data. movies . stream () . collect ( Collectors . partitioningBy ( m -> m . getLikes > 20 )) // get a map of boolean -> movies You can also tack on a second collector as we did above. Primitive Streams IntStream LongStream DoubleStream IntStream . of IntStream . range ( 1 , 5 ) IntStream . rangeClosed ( 1 , 5 )","title":"Reducers"},{"location":"Machine%20Learning/Linear%20Regression/","text":"1. Regularization The regularization term should only be added to the model at the time of training. Once the model is trainined we should use the unregularized version to measure performance. If you are using any regularized model its important to scale data e.g. using StandardScaler . It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are useful, you should prefer Lasso or Elastic Net because they tend to reduce the useless features. 1.1. Ridge Regression Add a regularization term $\\alpha\\frac{1}{2}\\sum_{1=1}^n \\theta_i^2$ to the cost function $J(\\theta)$. This forces the weights to be as small as possible. The hyperparameter $\\alpha$ controls how much regularization is added. If its 0 then its just linear regression, if its too large then the weights will be almost equal to 0. Note that bias term $\\theta_0$ is not regularized. $$J(\\theta) = MSE(\\theta) + \\alpha\\frac{1}{2}\\sum_{1=1}^n \\theta_i^2 $$ Ridge regression can be performed using Gradient Descent or using closed form equation as given below. $$\\hat{\\theta} = (X^\\intercal X + \\alpha A)^{-1} X^\\intercal Y$$ Uses L2 norm 1.2. Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (usually simply called Lasso Regression) The lasso regression is almost similar to Ridge regression instead it uses L1 norm. An important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features (i.e., set them to zero). 1.3. Elastic Net Regression Its a middle ground b/w Ridge and Lasso and the term is a combination of both, also you can set the ratio between the two using $r$ (note $r$ vs $1-r$). $$ J(\\theta) = MSE(\\theta) + r \\alpha \\sum_{i=1}^n |\\theta_i| + \\frac{1-r}{2} \\alpha \\sum_{i=1}^n \\theta^2_i$$ elastic_net = ElasticNet ( alpha = 0.1 , l1_ratio = 0.5 ) # l1_ratio is r ratio","title":"1. Regularization"},{"location":"Machine%20Learning/Linear%20Regression/#1-regularization","text":"The regularization term should only be added to the model at the time of training. Once the model is trainined we should use the unregularized version to measure performance. If you are using any regularized model its important to scale data e.g. using StandardScaler . It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are useful, you should prefer Lasso or Elastic Net because they tend to reduce the useless features.","title":"1. Regularization"},{"location":"Machine%20Learning/Linear%20Regression/#11-ridge-regression","text":"Add a regularization term $\\alpha\\frac{1}{2}\\sum_{1=1}^n \\theta_i^2$ to the cost function $J(\\theta)$. This forces the weights to be as small as possible. The hyperparameter $\\alpha$ controls how much regularization is added. If its 0 then its just linear regression, if its too large then the weights will be almost equal to 0. Note that bias term $\\theta_0$ is not regularized. $$J(\\theta) = MSE(\\theta) + \\alpha\\frac{1}{2}\\sum_{1=1}^n \\theta_i^2 $$ Ridge regression can be performed using Gradient Descent or using closed form equation as given below. $$\\hat{\\theta} = (X^\\intercal X + \\alpha A)^{-1} X^\\intercal Y$$ Uses L2 norm","title":"1.1. Ridge Regression"},{"location":"Machine%20Learning/Linear%20Regression/#12-lasso-regression","text":"Least Absolute Shrinkage and Selection Operator Regression (usually simply called Lasso Regression) The lasso regression is almost similar to Ridge regression instead it uses L1 norm. An important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features (i.e., set them to zero).","title":"1.2. Lasso Regression"},{"location":"Machine%20Learning/Linear%20Regression/#13-elastic-net-regression","text":"Its a middle ground b/w Ridge and Lasso and the term is a combination of both, also you can set the ratio between the two using $r$ (note $r$ vs $1-r$). $$ J(\\theta) = MSE(\\theta) + r \\alpha \\sum_{i=1}^n |\\theta_i| + \\frac{1-r}{2} \\alpha \\sum_{i=1}^n \\theta^2_i$$ elastic_net = ElasticNet ( alpha = 0.1 , l1_ratio = 0.5 ) # l1_ratio is r ratio","title":"1.3. Elastic Net Regression"},{"location":"Machine%20Learning/Logistic%20Regression/","text":"Logistic Regression Multiclass Classification Softmax Regression","title":"Logistic Regression"},{"location":"Machine%20Learning/Logistic%20Regression/#logistic-regression","text":"","title":"Logistic Regression"},{"location":"Machine%20Learning/Logistic%20Regression/#multiclass-classification","text":"","title":"Multiclass Classification"},{"location":"Machine%20Learning/Logistic%20Regression/#softmax-regression","text":"","title":"Softmax Regression"},{"location":"Machine%20Learning/MultiLayerPerceptron/","text":"1. Multi Layer Perceptron 1.1. Perceptron It is one of the simplest ANN architectures. 1.1.1. TLU A TLU is a threshold logical unit. Logic as below: - Compute the weighted sum of its inputs i.e. $z = w_1 x_1 + w_2 x_2 + ... + w_n x_n$ or $z = W\\intercal X$ Apply a step function that outputs the result i.e. $h(x) = step(z) $ The most common step function is the heavy side step function. (Basically its a binary classifier and predicts 0 if $h(x) < 0$ or 1 if $h(x) >= 1$. You can also use any other threshold other than 0 if required. 1.1.2. Perceptron A perceptron is combination of multiple TLU but single layer. You can compute the output of a fully connected layer easily. $$ h(X) = \\phi(XW + b) $$ $\\phi$ is the activation function $b$ is the bias $X$ is the input features $W$ is the weights Perceptron will update its weights using the rule shown below $$w_{new} = w_{current} + \\alpha(y_j - \\hat{y}_j)x $$ Because the decision boundary of each output neuron is linear, so perceptrons cannot learn complex patterns. Remember (Note) Remember that a linear model is one which has it weights as linear, even though the input parameter could be polynomial. Example below : - $$y = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1^2$$ The model is considered to be non linear when the weights are non linear as shown below: - $$y = w_0 + w_1 x_1 + w_2 x_2 + w_1 w_2 x_3$$ Code Sample sklearn provides a Perceptron class that implements a single-TLU network. from sklearn.linear_model import Perceptron Scikit-Learn\u2019s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None i.e. no regularization. 1.2. Multilayer Perceptron The MLP is composed of an input layer, one or more layers of TLU (hidden layers), and final output layer of TLU. When we have many many hidden layers it is considered a deep neural network. 1.2.1. BackPropogation Autodiff : A lot of frameworks provide the functionality of autodiff which computes the gradient automatically. Back propogation algorithm summary : the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step). Random Initialization : It is important to initialize all the layer weights randomly. 1.2.2. Step Functions Choice of step functions include the following : - Sigmoid Hyperbolic tangent ReLU 1.2.3. Cost functions Choice of cost functions Mean Squared Error Mean absolute error (if you have lots of outliers) Huber loss (combination of the above 2) 1.2.4. MLP for regression MLP can be used for regression, in this case do not apply the activation function to the output layer. The MLP can provide single value as well as multiple values (e.g. coordinate values for x, y and z). If the MLP has to provide multiple values as outputs it needs multiple output neurons. model = keras . models . Sequential ([ keras . layers . Dense ( 30 , activation = \"relu\" , input_shape = x_train . shape [ 1 :]), keras . layers . Dense ( 1 ) # No activation function on last layer for regression. ]) model . compile ( loss = \"mean_squared_error\" , optimizer = \"sgd\" ) model . fit ( x_train , y_train , epochs = 20 , validation_data = ( x_valid , y_valid )) 1.2.5. MLP for classification MLP can also be used for classification, for binary output you need just a single neuron using logistic activation function. For multilabel classification you would need more than 1 neuron. If the classification has to be 1 of many (i.e. multiclass classification), then you can add softmax function which will ensure all the probability are between 0 and 1 and add upto 1. model = keras . models . Sequential () model . add ( keras . layers . Flatten ( input_shape = [ 28 , 28 ])) model . add ( keras . layers . Dense ( 300 , activation = 'relu' )) model . add ( keras . layers . Dense ( 100 , activation = \"relu\" )) model . add ( keras . layers . Dense ( 10 , activation = \"softmax\" )) model . compile ( loss = \"sparse_categorical_crossentropy\" , optimizer = \"sgd\" , metrics = [ \"accuracy\" ]) model . fit ( x_train , y_train , epochs = 30 , validation_data = ( x_valid , y_valid )) You can check or enquire about the model as below: - General info about the model model . summary () # Will return the layers of the model model . layers # Get the weights for hidden layer 1 hidden1 . get_weights () # Evaluating the model model . evaluate ( x_test , y_test )","title":"1. Multi Layer Perceptron"},{"location":"Machine%20Learning/MultiLayerPerceptron/#1-multi-layer-perceptron","text":"","title":"1. Multi Layer Perceptron"},{"location":"Machine%20Learning/MultiLayerPerceptron/#11-perceptron","text":"It is one of the simplest ANN architectures.","title":"1.1. Perceptron"},{"location":"Machine%20Learning/MultiLayerPerceptron/#111-tlu","text":"A TLU is a threshold logical unit. Logic as below: - Compute the weighted sum of its inputs i.e. $z = w_1 x_1 + w_2 x_2 + ... + w_n x_n$ or $z = W\\intercal X$ Apply a step function that outputs the result i.e. $h(x) = step(z) $ The most common step function is the heavy side step function. (Basically its a binary classifier and predicts 0 if $h(x) < 0$ or 1 if $h(x) >= 1$. You can also use any other threshold other than 0 if required.","title":"1.1.1. TLU"},{"location":"Machine%20Learning/MultiLayerPerceptron/#112-perceptron","text":"A perceptron is combination of multiple TLU but single layer. You can compute the output of a fully connected layer easily. $$ h(X) = \\phi(XW + b) $$ $\\phi$ is the activation function $b$ is the bias $X$ is the input features $W$ is the weights Perceptron will update its weights using the rule shown below $$w_{new} = w_{current} + \\alpha(y_j - \\hat{y}_j)x $$ Because the decision boundary of each output neuron is linear, so perceptrons cannot learn complex patterns. Remember (Note) Remember that a linear model is one which has it weights as linear, even though the input parameter could be polynomial. Example below : - $$y = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1^2$$ The model is considered to be non linear when the weights are non linear as shown below: - $$y = w_0 + w_1 x_1 + w_2 x_2 + w_1 w_2 x_3$$ Code Sample sklearn provides a Perceptron class that implements a single-TLU network. from sklearn.linear_model import Perceptron Scikit-Learn\u2019s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None i.e. no regularization.","title":"1.1.2. Perceptron"},{"location":"Machine%20Learning/MultiLayerPerceptron/#12-multilayer-perceptron","text":"The MLP is composed of an input layer, one or more layers of TLU (hidden layers), and final output layer of TLU. When we have many many hidden layers it is considered a deep neural network.","title":"1.2. Multilayer Perceptron"},{"location":"Machine%20Learning/MultiLayerPerceptron/#121-backpropogation","text":"Autodiff : A lot of frameworks provide the functionality of autodiff which computes the gradient automatically. Back propogation algorithm summary : the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step). Random Initialization : It is important to initialize all the layer weights randomly.","title":"1.2.1. BackPropogation"},{"location":"Machine%20Learning/MultiLayerPerceptron/#122-step-functions","text":"Choice of step functions include the following : - Sigmoid Hyperbolic tangent ReLU","title":"1.2.2. Step Functions"},{"location":"Machine%20Learning/MultiLayerPerceptron/#123-cost-functions","text":"Choice of cost functions Mean Squared Error Mean absolute error (if you have lots of outliers) Huber loss (combination of the above 2)","title":"1.2.3. Cost functions"},{"location":"Machine%20Learning/MultiLayerPerceptron/#124-mlp-for-regression","text":"MLP can be used for regression, in this case do not apply the activation function to the output layer. The MLP can provide single value as well as multiple values (e.g. coordinate values for x, y and z). If the MLP has to provide multiple values as outputs it needs multiple output neurons. model = keras . models . Sequential ([ keras . layers . Dense ( 30 , activation = \"relu\" , input_shape = x_train . shape [ 1 :]), keras . layers . Dense ( 1 ) # No activation function on last layer for regression. ]) model . compile ( loss = \"mean_squared_error\" , optimizer = \"sgd\" ) model . fit ( x_train , y_train , epochs = 20 , validation_data = ( x_valid , y_valid ))","title":"1.2.4. MLP for regression"},{"location":"Machine%20Learning/MultiLayerPerceptron/#125-mlp-for-classification","text":"MLP can also be used for classification, for binary output you need just a single neuron using logistic activation function. For multilabel classification you would need more than 1 neuron. If the classification has to be 1 of many (i.e. multiclass classification), then you can add softmax function which will ensure all the probability are between 0 and 1 and add upto 1. model = keras . models . Sequential () model . add ( keras . layers . Flatten ( input_shape = [ 28 , 28 ])) model . add ( keras . layers . Dense ( 300 , activation = 'relu' )) model . add ( keras . layers . Dense ( 100 , activation = \"relu\" )) model . add ( keras . layers . Dense ( 10 , activation = \"softmax\" )) model . compile ( loss = \"sparse_categorical_crossentropy\" , optimizer = \"sgd\" , metrics = [ \"accuracy\" ]) model . fit ( x_train , y_train , epochs = 30 , validation_data = ( x_valid , y_valid )) You can check or enquire about the model as below: - General info about the model model . summary () # Will return the layers of the model model . layers # Get the weights for hidden layer 1 hidden1 . get_weights () # Evaluating the model model . evaluate ( x_test , y_test )","title":"1.2.5. MLP for classification"},{"location":"Machine%20Learning/Overview%20of%20Supervised%20Learning/","text":"Overview of Supervised Learning Output Types Quantitative Qualitative e.g. ${0,1,2}$ Prediction Task Regression - we predict quantitative Classification - we predict qualitative Input Types Quantitative inputs Qualitative inputs - Typically represented by codes. e.g. ${0, 1}$. Most common encoding is dummy variables, Here a K-level qualitative variable is represented by a vector of K binary variables or bits, only one of which is \u201con\u201d at a time. Ordered categorical - where there is an ordering between the values, but no metric notion - e.g. Small, Medium, Large Two simple approaches to prediction Linear models and Least Squares Nearest Neighbours","title":"Overview of Supervised Learning"},{"location":"Machine%20Learning/Overview%20of%20Supervised%20Learning/#overview-of-supervised-learning","text":"Output Types Quantitative Qualitative e.g. ${0,1,2}$ Prediction Task Regression - we predict quantitative Classification - we predict qualitative Input Types Quantitative inputs Qualitative inputs - Typically represented by codes. e.g. ${0, 1}$. Most common encoding is dummy variables, Here a K-level qualitative variable is represented by a vector of K binary variables or bits, only one of which is \u201con\u201d at a time. Ordered categorical - where there is an ordering between the values, but no metric notion - e.g. Small, Medium, Large","title":"Overview of Supervised Learning"},{"location":"Machine%20Learning/Overview%20of%20Supervised%20Learning/#two-simple-approaches-to-prediction","text":"","title":"Two simple approaches to prediction"},{"location":"Machine%20Learning/Overview%20of%20Supervised%20Learning/#linear-models-and-least-squares","text":"","title":"Linear models and Least Squares"},{"location":"Machine%20Learning/Overview%20of%20Supervised%20Learning/#nearest-neighbours","text":"","title":"Nearest Neighbours"},{"location":"Machine%20Learning/Pandas/","text":"Basics Extracting a Series vs DataFrame from a Dataframe df = pd . read_csv ( 'housing.csv' ) ocean_prox = df [ 'ocean_proximity' ] # Gives you a series ocean_prox = df [[ 'ocean_proximity' ]] # Gives you a dataframe Correlation df . corr () Data into Bins pd . cut ( df [ \"median_income\" ], 4 ) # make 4 bins from median_income. You can also explicitly give the bin start and end point.","title":"Basics"},{"location":"Machine%20Learning/Pandas/#basics","text":"Extracting a Series vs DataFrame from a Dataframe df = pd . read_csv ( 'housing.csv' ) ocean_prox = df [ 'ocean_proximity' ] # Gives you a series ocean_prox = df [[ 'ocean_proximity' ]] # Gives you a dataframe Correlation df . corr ()","title":"Basics"},{"location":"Machine%20Learning/Pandas/#data-into-bins","text":"pd . cut ( df [ \"median_income\" ], 4 ) # make 4 bins from median_income. You can also explicitly give the bin start and end point.","title":"Data into Bins"},{"location":"Machine%20Learning/Sklearn/","text":"This file will contains most used features of SciKitLearn. I have tried to organize this file in the structure of Sklearn documentation i.e. Module wise. To check the official documentation check this link . Links User Guide API Preprocessing Imputers Simple Imputer Transformers for missing value imputation, there are various strategies like, mean, median, constant value etc... . The SimpleImputer works on numerical values, so if our DataFrame has non numerical values like Strings, etc... we need to remove them before we feed the data into SimpleImputer . Example shown below... import pandas as pd from sklearn.impute import SimpleImputer df = pd . read_csv ( 'housing.csv' ) df_copy = df # Keep a copy of DataFrame # Lets drop the categorical values if any. df . drop ([ \"ocean_proximity\" ], axis = 1 , inplace = True ) si = SimpleImputer ( strategy = \"mean\" ) si . fit ( df ) transformed = si . transform ( df ) # Now we have an nd-array of inputed values # Create a DataFrame if required. Then you can add the categorical values back if required. df = pd . DataFrame ( transformed , columns = df . columns ) Standard Scalar Standardize features by removing the mean and scaling to unit variance. $z = \\frac{(x - u)}{s}$ where u is the mean and s is the standard deviation. Centering and scaling happen independently on each feature. This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data. import pandas as pd from sklearn.preprocessing import StandardScaler df = pd . read_csv ( \"housing.csv\" ) scalar = StandardScaler () scalar . fit ( df ) scalar . transform ( df ) OrdinalEncoder Encode categorical features as an integer array. The below example will encode all ocean_proximity into an array of [1, 3, 0, 1, 2] . Here you see we are converting a dataframe column into numpy arrays to do the encoding. enc = OrdinalEncoder () e = enc . fit_transform ( np . array ( df [ 'ocean_proximity' ]) . reshape ( - 1 , 1 )) You can also encode a column of Dataframe as shown below. ocean_prox = df [[ 'ocean_proximity' ]] # This gives you a df. ocean_prox is a 1 col df oe = OrdinalEncoder () ocean_prox_encoded = oe . fit_transform ( ocean_prox ) The OrdinalEncoder preserves the categories oe.categories_ . If you wish to merge the encoded value back to the Dataframe, you can include them as a new column as shown below. import pandas as pd from sklearn.preprocessing import OrdinalEncoder df = pd . read_csv ( 'housing.csv' ) oe = OrdinalEncoder () ocean_prox = df [[ 'ocean_proximity' ]] # Creates a df df [ 'ocean_prox_encoded' ] = oe . fit_transform ( ocean_prox ) # Adding additional column 1.4. OneHotEncoder There are other encoders avaiable as well such as pd.dummy which are easier to work with, however there are some considerable differences. Remember that these sklearn encoder(s) are classes and we create instance of these classes. Once they encode the values, they remeber which \"String\" maps to which interger value/matrix etc... thus they will perform the same transformation on training set, cross validation set, test set and in production. Thus it is important that we use these or override these when encoding values. Thus we can handle any unknown data in production i.e. handle_unknown = 'error' or handle_unknown = 'ignore' import pandas as pd from sklearn.preprocessing import OneHotEncoder df = pd . read_csv ( 'housing.csv' ) ocean_prox = df [[ 'ocean_proximity' ]] oe = OneHotEncoder ( sparse = False ) # This returns an array df2 = pd . DataFrame ( oe . fit_transform ( ocean_prox ), columns = oe . get_feature_names ()) # Creating a new df based on feature names of ocean_prox col df = pd . concat ([ df , df2 ], axis = 1 ) 1.5. LabelBinarizer Check this Link to clarify b/w OrdinalEncoder, OneHotEncoder, LabelBinarizer 1.6. Custom Transformer Sklearn provides many transformers but there could be possibilities to create custom transformation of data based on your scenario. The custom transformer should work with Sklearn pipelines thus implenting a function will not be as helpful. You just need to implement the following methods. fit() return self transform() fit_transform() - get this free by simply adding TransformerMixin as a base class A very simple example shown below ... \"\"\" Custom Transformer for a Housing dataset. The below is a very simple example, however we can do a lot with this. The transform function can return a df, np.array (combined or not combined) with existing. \"\"\" import pandas as pd from sklearn.base import BaseEstimator , TransformerMixin df = pd . read_csv ( 'housing.csv' ) class MyTransformer ( BaseEstimator , TransformerMixin ): def __init__ ( self ): pass def fit ( self ): return self def transform ( self , df ): return df [[ 'population' ]] ** 2 my_t = MyTransformer () my_t . transform ( df ) # Returns a new df with polynomial 1.7. Pipeline The sklearn.pipeline module implements utilities to build a composite estimator, as a chain of transforms and estimators. Intermediate steps of the pipeline must be \u2018transforms\u2019, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. fit() vs fit_transform() for the pipeline When you call the pipeline\u2019s fit() method, it calls fit_transform() sequentially on all transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it calls the fit() method. fit_transform() transforms based on all of them. import pandas as pd from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer df = pd . read_csv ( \"housing.csv\" ) df_copy = df . drop ( 'ocean_proximity' , axis = 1 ) # Dropping categorical because of SimpleImputer & StandardScaler # The pipeline num_pipeline = Pipeline ([ ( 'imputer' , SimpleImputer ( strategy = \"median\" )), ( 'std_scaler' , StandardScaler ()), ]) # Using the pipeline on df_copy num_pipeline_tr = num_pipeline . fit_transform ( df_copy ) 1.8. Polynomial Features Generating polynomial features is easy as shown below. import numpy as np from sklearn.preprocessing import PolynomialFeatures X = np . random . random ( 4 ) . reshape ( - 1 , 1 ) 2. Models 2.1. Linear Models 2.1.1. Ridge Rigression Add a regularization parameter. import pandas as pd from sklearn.linear_model import Ridge df = pd . read_csv ( 'housing.csv' ) df . dropna ( inplace = True ) X = df . drop ([ 'median_house_value' , 'ocean_proximity' ], axis = 1 ) # drop y and categorical. (Encode in actual) y = df [ 'median_house_value' ] # \u2018cholesky\u2019 uses the standard scipy.linalg.solve function to obtain a closed-form solution. model = Ridge ( alpha = 1 , solver = \"cholesky\" ) model . fit ( X , y ) 2.1.2. Lasso Rigression from sklearn.linear_model import Lasso 2.2. Ensemble Methods 2.2.1. Random Forest from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( random_state = 42 ) result = cross_val_predict ( clf , X_train , y_train_5 , cv = 3 , method = \"predict_proba\" ) 3. Model Selection Sklearn link 3.1. Splitters 3.1.1. Train Test Split Split arrays or matrices into random train and test subsets. from sklearn.model_selection import train_test_split X , y = train_test_split ( df , test_size = . 2 ) # Passing a df There is a paramter stratify which by default is None You can also set the random_state for repeatability. 3.1.2. StratifiedShuffleSplit StratifiedShuffleSplit splits the data based on the percentage of category values. You can state the number of splits which you want e.g. n_splits = 1 Also random_state for repeatability Example below: - import pandas as pd from sklearn.model_selection import StratifiedShuffleSplit df = pd . read_csv ( 'housing.csv' ) split = StratifiedShuffleSplit ( n_splits = 1 , test_size = 0.2 , random_state = 42 ) for train_index , test_index in split . split ( df , df [ \"ocean_proximity\" ]): strat_train_set = df . loc [ train_index ] strat_test_set = df . loc [ test_index ] StratifiedShuffleSplit returns the index of rows which you it has split, example below. >>> ss = StratifiedShuffleSplit ( n_splits = 2 ) >>> split = ss . split ( df , df [ 'ocean_proximity' ]) >>> list ( split ) [( array ([ 2766 , 13808 , 2658 , ... , 12854 , 11468 , 8321 ], dtype = int64 ), array ([ 12116 , 11088 , 1840 , ... , 18747 , 16266 , 14731 ], dtype = int64 )), ( array ([ 15490 , 5435 , 5497 , ... , 8741 , 1524 , 11954 ], dtype = int64 ), array ([ 16105 , 282 , 6746 , ... , 10109 , 17403 , 18546 ], dtype = int64 ))] 3.2. Model Prediction 3.2.1. Cross Val Score Evaluate a score by cross-validation, you can sore based on various strategies . from sklearn.model_selection import cross_val_score cross_val_score ( sgd_clf , X_train , y_train_5 , cv = 3 , scoring = \"accuracy\" ) 3.2.2. Custom Cross Validation You can run a loop while doing StratifiedKFold and fit a model on it and then get the score. 3.3. Hyper-parameter optimizers 3.3.1. Grid Search Grid search runs all combinations of hyperparamters. The hyperparameters are different based on each model. from sklearn.model_selection import GridSearchCV param_grid = [ { 'n_estimators' : [ 3 , 10 , 30 ], 'max_features' : [ 2 , 4 , 6 , 8 ]}, { 'bootstrap' : [ False ], 'n_estimators' : [ 3 , 10 ], 'max_features' : [ 2 , 3 , 4 ]}, ] forest_reg = RandomForestRegressor () grid_search = GridSearchCV ( forest_reg , param_grid , cv = 5 , scoring = 'neg_mean_squared_error' , return_train_score = True ) grid_search.cv_results_ returns the results, it can be transformed into a Pandas Dataframe to get a good look at the best hyperparamters for the model. 3.3.2. RandomizedSearchCV Randomized search on hyper parameters. 4. Metrics 4.1. Precision Recall from sklearn.metrics import precision_score , recall_score precision_score ( y_train_5 , y_train_pred ) recall_score ( y_train_5 , y_train_pred ) 4.2. Confusion Matrix from sklearn.metrics import confusion_matrix confusion_matrix ( y_train_5 , y_train_pred ) 4.3. Area under curve - RoC from sklearn.metrics import roc_auc_score roc_auc_score ( y_train_5 , y_scores )","title":"Sklearn"},{"location":"Machine%20Learning/Sklearn/#preprocessing","text":"","title":"Preprocessing"},{"location":"Machine%20Learning/Sklearn/#imputers","text":"Simple Imputer Transformers for missing value imputation, there are various strategies like, mean, median, constant value etc... . The SimpleImputer works on numerical values, so if our DataFrame has non numerical values like Strings, etc... we need to remove them before we feed the data into SimpleImputer . Example shown below... import pandas as pd from sklearn.impute import SimpleImputer df = pd . read_csv ( 'housing.csv' ) df_copy = df # Keep a copy of DataFrame # Lets drop the categorical values if any. df . drop ([ \"ocean_proximity\" ], axis = 1 , inplace = True ) si = SimpleImputer ( strategy = \"mean\" ) si . fit ( df ) transformed = si . transform ( df ) # Now we have an nd-array of inputed values # Create a DataFrame if required. Then you can add the categorical values back if required. df = pd . DataFrame ( transformed , columns = df . columns )","title":"Imputers"},{"location":"Machine%20Learning/Sklearn/#standard-scalar","text":"Standardize features by removing the mean and scaling to unit variance. $z = \\frac{(x - u)}{s}$ where u is the mean and s is the standard deviation. Centering and scaling happen independently on each feature. This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data. import pandas as pd from sklearn.preprocessing import StandardScaler df = pd . read_csv ( \"housing.csv\" ) scalar = StandardScaler () scalar . fit ( df ) scalar . transform ( df )","title":"Standard Scalar"},{"location":"Machine%20Learning/Sklearn/#ordinalencoder","text":"Encode categorical features as an integer array. The below example will encode all ocean_proximity into an array of [1, 3, 0, 1, 2] . Here you see we are converting a dataframe column into numpy arrays to do the encoding. enc = OrdinalEncoder () e = enc . fit_transform ( np . array ( df [ 'ocean_proximity' ]) . reshape ( - 1 , 1 )) You can also encode a column of Dataframe as shown below. ocean_prox = df [[ 'ocean_proximity' ]] # This gives you a df. ocean_prox is a 1 col df oe = OrdinalEncoder () ocean_prox_encoded = oe . fit_transform ( ocean_prox ) The OrdinalEncoder preserves the categories oe.categories_ . If you wish to merge the encoded value back to the Dataframe, you can include them as a new column as shown below. import pandas as pd from sklearn.preprocessing import OrdinalEncoder df = pd . read_csv ( 'housing.csv' ) oe = OrdinalEncoder () ocean_prox = df [[ 'ocean_proximity' ]] # Creates a df df [ 'ocean_prox_encoded' ] = oe . fit_transform ( ocean_prox ) # Adding additional column","title":"OrdinalEncoder"},{"location":"Machine%20Learning/Sklearn/#14-onehotencoder","text":"There are other encoders avaiable as well such as pd.dummy which are easier to work with, however there are some considerable differences. Remember that these sklearn encoder(s) are classes and we create instance of these classes. Once they encode the values, they remeber which \"String\" maps to which interger value/matrix etc... thus they will perform the same transformation on training set, cross validation set, test set and in production. Thus it is important that we use these or override these when encoding values. Thus we can handle any unknown data in production i.e. handle_unknown = 'error' or handle_unknown = 'ignore' import pandas as pd from sklearn.preprocessing import OneHotEncoder df = pd . read_csv ( 'housing.csv' ) ocean_prox = df [[ 'ocean_proximity' ]] oe = OneHotEncoder ( sparse = False ) # This returns an array df2 = pd . DataFrame ( oe . fit_transform ( ocean_prox ), columns = oe . get_feature_names ()) # Creating a new df based on feature names of ocean_prox col df = pd . concat ([ df , df2 ], axis = 1 )","title":"1.4. OneHotEncoder"},{"location":"Machine%20Learning/Sklearn/#15-labelbinarizer","text":"Check this Link to clarify b/w OrdinalEncoder, OneHotEncoder, LabelBinarizer","title":"1.5. LabelBinarizer"},{"location":"Machine%20Learning/Sklearn/#16-custom-transformer","text":"Sklearn provides many transformers but there could be possibilities to create custom transformation of data based on your scenario. The custom transformer should work with Sklearn pipelines thus implenting a function will not be as helpful. You just need to implement the following methods. fit() return self transform() fit_transform() - get this free by simply adding TransformerMixin as a base class A very simple example shown below ... \"\"\" Custom Transformer for a Housing dataset. The below is a very simple example, however we can do a lot with this. The transform function can return a df, np.array (combined or not combined) with existing. \"\"\" import pandas as pd from sklearn.base import BaseEstimator , TransformerMixin df = pd . read_csv ( 'housing.csv' ) class MyTransformer ( BaseEstimator , TransformerMixin ): def __init__ ( self ): pass def fit ( self ): return self def transform ( self , df ): return df [[ 'population' ]] ** 2 my_t = MyTransformer () my_t . transform ( df ) # Returns a new df with polynomial","title":"1.6. Custom Transformer"},{"location":"Machine%20Learning/Sklearn/#17-pipeline","text":"The sklearn.pipeline module implements utilities to build a composite estimator, as a chain of transforms and estimators. Intermediate steps of the pipeline must be \u2018transforms\u2019, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. fit() vs fit_transform() for the pipeline When you call the pipeline\u2019s fit() method, it calls fit_transform() sequentially on all transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it calls the fit() method. fit_transform() transforms based on all of them. import pandas as pd from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer df = pd . read_csv ( \"housing.csv\" ) df_copy = df . drop ( 'ocean_proximity' , axis = 1 ) # Dropping categorical because of SimpleImputer & StandardScaler # The pipeline num_pipeline = Pipeline ([ ( 'imputer' , SimpleImputer ( strategy = \"median\" )), ( 'std_scaler' , StandardScaler ()), ]) # Using the pipeline on df_copy num_pipeline_tr = num_pipeline . fit_transform ( df_copy )","title":"1.7. Pipeline"},{"location":"Machine%20Learning/Sklearn/#18-polynomial-features","text":"Generating polynomial features is easy as shown below. import numpy as np from sklearn.preprocessing import PolynomialFeatures X = np . random . random ( 4 ) . reshape ( - 1 , 1 )","title":"1.8. Polynomial Features"},{"location":"Machine%20Learning/Sklearn/#2-models","text":"","title":"2. Models"},{"location":"Machine%20Learning/Sklearn/#21-linear-models","text":"","title":"2.1. Linear  Models"},{"location":"Machine%20Learning/Sklearn/#211-ridge-rigression","text":"Add a regularization parameter. import pandas as pd from sklearn.linear_model import Ridge df = pd . read_csv ( 'housing.csv' ) df . dropna ( inplace = True ) X = df . drop ([ 'median_house_value' , 'ocean_proximity' ], axis = 1 ) # drop y and categorical. (Encode in actual) y = df [ 'median_house_value' ] # \u2018cholesky\u2019 uses the standard scipy.linalg.solve function to obtain a closed-form solution. model = Ridge ( alpha = 1 , solver = \"cholesky\" ) model . fit ( X , y )","title":"2.1.1. Ridge Rigression"},{"location":"Machine%20Learning/Sklearn/#212-lasso-rigression","text":"from sklearn.linear_model import Lasso","title":"2.1.2. Lasso Rigression"},{"location":"Machine%20Learning/Sklearn/#22-ensemble-methods","text":"","title":"2.2. Ensemble Methods"},{"location":"Machine%20Learning/Sklearn/#221-random-forest","text":"from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier ( random_state = 42 ) result = cross_val_predict ( clf , X_train , y_train_5 , cv = 3 , method = \"predict_proba\" )","title":"2.2.1. Random Forest"},{"location":"Machine%20Learning/Sklearn/#3-model-selection","text":"Sklearn link","title":"3. Model Selection"},{"location":"Machine%20Learning/Sklearn/#31-splitters","text":"","title":"3.1. Splitters"},{"location":"Machine%20Learning/Sklearn/#311-train-test-split","text":"Split arrays or matrices into random train and test subsets. from sklearn.model_selection import train_test_split X , y = train_test_split ( df , test_size = . 2 ) # Passing a df There is a paramter stratify which by default is None You can also set the random_state for repeatability.","title":"3.1.1. Train Test Split"},{"location":"Machine%20Learning/Sklearn/#312-stratifiedshufflesplit","text":"StratifiedShuffleSplit splits the data based on the percentage of category values. You can state the number of splits which you want e.g. n_splits = 1 Also random_state for repeatability Example below: - import pandas as pd from sklearn.model_selection import StratifiedShuffleSplit df = pd . read_csv ( 'housing.csv' ) split = StratifiedShuffleSplit ( n_splits = 1 , test_size = 0.2 , random_state = 42 ) for train_index , test_index in split . split ( df , df [ \"ocean_proximity\" ]): strat_train_set = df . loc [ train_index ] strat_test_set = df . loc [ test_index ] StratifiedShuffleSplit returns the index of rows which you it has split, example below. >>> ss = StratifiedShuffleSplit ( n_splits = 2 ) >>> split = ss . split ( df , df [ 'ocean_proximity' ]) >>> list ( split ) [( array ([ 2766 , 13808 , 2658 , ... , 12854 , 11468 , 8321 ], dtype = int64 ), array ([ 12116 , 11088 , 1840 , ... , 18747 , 16266 , 14731 ], dtype = int64 )), ( array ([ 15490 , 5435 , 5497 , ... , 8741 , 1524 , 11954 ], dtype = int64 ), array ([ 16105 , 282 , 6746 , ... , 10109 , 17403 , 18546 ], dtype = int64 ))]","title":"3.1.2. StratifiedShuffleSplit"},{"location":"Machine%20Learning/Sklearn/#32-model-prediction","text":"","title":"3.2. Model Prediction"},{"location":"Machine%20Learning/Sklearn/#321-cross-val-score","text":"Evaluate a score by cross-validation, you can sore based on various strategies . from sklearn.model_selection import cross_val_score cross_val_score ( sgd_clf , X_train , y_train_5 , cv = 3 , scoring = \"accuracy\" )","title":"3.2.1. Cross Val Score"},{"location":"Machine%20Learning/Sklearn/#322-custom-cross-validation","text":"You can run a loop while doing StratifiedKFold and fit a model on it and then get the score.","title":"3.2.2. Custom Cross Validation"},{"location":"Machine%20Learning/Sklearn/#33-hyper-parameter-optimizers","text":"","title":"3.3. Hyper-parameter optimizers"},{"location":"Machine%20Learning/Sklearn/#331-grid-search","text":"Grid search runs all combinations of hyperparamters. The hyperparameters are different based on each model. from sklearn.model_selection import GridSearchCV param_grid = [ { 'n_estimators' : [ 3 , 10 , 30 ], 'max_features' : [ 2 , 4 , 6 , 8 ]}, { 'bootstrap' : [ False ], 'n_estimators' : [ 3 , 10 ], 'max_features' : [ 2 , 3 , 4 ]}, ] forest_reg = RandomForestRegressor () grid_search = GridSearchCV ( forest_reg , param_grid , cv = 5 , scoring = 'neg_mean_squared_error' , return_train_score = True ) grid_search.cv_results_ returns the results, it can be transformed into a Pandas Dataframe to get a good look at the best hyperparamters for the model.","title":"3.3.1. Grid Search"},{"location":"Machine%20Learning/Sklearn/#332-randomizedsearchcv","text":"Randomized search on hyper parameters.","title":"3.3.2. RandomizedSearchCV"},{"location":"Machine%20Learning/Sklearn/#4-metrics","text":"","title":"4. Metrics"},{"location":"Machine%20Learning/Sklearn/#41-precision-recall","text":"from sklearn.metrics import precision_score , recall_score precision_score ( y_train_5 , y_train_pred ) recall_score ( y_train_5 , y_train_pred )","title":"4.1. Precision Recall"},{"location":"Machine%20Learning/Sklearn/#42-confusion-matrix","text":"from sklearn.metrics import confusion_matrix confusion_matrix ( y_train_5 , y_train_pred )","title":"4.2. Confusion Matrix"},{"location":"Machine%20Learning/Sklearn/#43-area-under-curve-roc","text":"from sklearn.metrics import roc_auc_score roc_auc_score ( y_train_5 , y_scores )","title":"4.3. Area under curve - RoC"},{"location":"Machine%20Learning/Terms/","text":"Model Parameters They are learned or estimated from the data. e.g. - Weights in neural network - The coefficients in a linear regression or logistic regression. Model Hyperparamters These are external to the model whose value cannot be estimated from the data. Define higher level concepts about the model such as complexity, or capacity to learn. e.g. - Learning rate. - Number of hidden layers in deep neural network. Cross Entropy Variance Bias Standardization Makes the mean as 0 and standard deviation as 1. $z = \\frac{x - \\mu}{\\sigma}$ This does not change the distribution of the data. Min Max - Normalization Scales the values to $[0,1]$ --> $\\frac{x-x_{min}}{max_x - min_x}$","title":"Model Parameters"},{"location":"Machine%20Learning/Terms/#model-parameters","text":"They are learned or estimated from the data. e.g. - Weights in neural network - The coefficients in a linear regression or logistic regression.","title":"Model Parameters"},{"location":"Machine%20Learning/Terms/#model-hyperparamters","text":"These are external to the model whose value cannot be estimated from the data. Define higher level concepts about the model such as complexity, or capacity to learn. e.g. - Learning rate. - Number of hidden layers in deep neural network.","title":"Model Hyperparamters"},{"location":"Machine%20Learning/Terms/#cross-entropy","text":"","title":"Cross Entropy"},{"location":"Machine%20Learning/Terms/#variance","text":"","title":"Variance"},{"location":"Machine%20Learning/Terms/#bias","text":"","title":"Bias"},{"location":"Machine%20Learning/Terms/#standardization","text":"Makes the mean as 0 and standard deviation as 1. $z = \\frac{x - \\mu}{\\sigma}$ This does not change the distribution of the data.","title":"Standardization"},{"location":"Machine%20Learning/Terms/#min-max-normalization","text":"Scales the values to $[0,1]$ --> $\\frac{x-x_{min}}{max_x - min_x}$","title":"Min Max - Normalization"},{"location":"Machine%20Learning/nlp/NPL/","text":"Parts of speech Use this link to get a crash course on grammar. Verb Denotes action e.g. runs, bites. A little more complex example ==> \"Karl Creelman bicycled around the world in 1899, but his diaries and his bicycle were destroyed .\" where were destroyed denotes the action so that is the verb. Noun Word used to name a person, animal, place, thing, and abstract idea. e.g. - Late last year our neighbours bought a goat . - Portia White was an opera singer . - The bus inspector looked at all the passengers' passes. Many common nouns, like \"engineer\" or \"teacher,\" can refer to men or women. Nouns will often add -s or -es for plurels. When Matthew was small he rarely told the truth if he thought he was going to be punished. Many people do not believe that truths are self-evident. Possessive Nouns The red suitcase is Cassandra's . The only luggage that was lost was the prime minister's. The exhausted recruits were woken before dawn by the - drill sergeant's screams. The miner's face was covered in coal dust. There are various types of nouns Proper noun Common noun Concrete noun Abstract noun Countable Non countable Collective Pronoun A pronoun can replace a noun or another pronoun e.g. he, which, none, you. Adjective An adjective modifies a noun or a pronoun by describing, identifying, or quantifying words. An adjective usually precedes the noun or the pronoun which it modifies. The truck-shaped balloon floated over the treetops. Mrs. Morrison papered her kitchen walls with hideous - wall paper. The small boat foundered on the wine dark sea. Adverb An adverb can modify a verb, an adjective, another adverb, a phrase, or a clause. An adverb indicates manner, time, place, cause, or degree and answers questions such as \"how,\" \"when,\" \"where,\" \"how much\". The seamstress quickly made the mourning clothes. The midwives waited patiently through a long labour. Preposition A preposition links nouns, pronouns and phrases to other words in a sentence. The word or phrase that the preposition introduces is called the object of the preposition. The book is on the table. She read the book during class. The children climbed the mountain without fear. (In this sentence, the preposition \"without\" introduces the noun \"fear.\") Conjunction You can use a conjunction to link words, phrases, and clauses, as in the following example: I ate the pizza and the pasta. Call the movers when you are ready. After she had learned to drive, Alice felt more independent. Interjection An interjection is a word added to a sentence to convey emotion. It is not grammatically related to any other part of the sentence. Hey! Put that down! I heard one guy say to another guy, \"He has a new car, - eh ?\" I don't know about you but, good lord , I think taxes are - too high! Regular Expression The most basic tool we have to process text is regular expression . They are heavily used even in ML classifiers etc... as features. Disjunction - Letters inside [] - e.g. [wW] either w or W or ranges e.g. [A-Z] or [a-z] or [0-9] or [A-Za-z] Negation - ^ - [^A-z] - not a capital letter, only if occurs right after a [ . OR - | ? - previous character is optional e.g. colou?r will match with our without the u * - 0 or more of previous character e.g. oo*h --> oh, ooh, oooh, ooooh, ... + - ` 1 or more of previous character . - means any character e.g. beg.n --> begin, began begun beg4n etc... ^ - Begining of line e.g. ^[A-Z] Capital letter at begining of line $ - End of line e.g. [A-Z]$ Capital letter at end of line You can use regular expression to match lots of text but still you will deal with precesion vs recall. Increase accuracy or precesion (minimizing false positive) Increase coverage or recall (minimize false negetive) Word Tokenization Breaking down a sentence into word tokens. Some sentences will have fragments, filled pauses like uh or aah --> should we consider these as tokens ? Lemma : same stem, same word sense etc... e.g. cat, cats = same lemma Wordforms : cat and cats = different wordforms How many words ? N = # of tokens --> an instance of vocabulary type in that text. e.g. total # of words in a sentence V = vocabulary = set of types. $|V|$ is the size of vocabulary. --> how many unique words there are ? Types --> could depend on if you count 2 different words with same lemma as 1 or multiple. There are various issues in Chinese / German / Japanese where you have no concept of word or no spaces between them, the things get a little tough there. Normalize or Stemming Normalization Implicitly define equivalence class of terms ==> U.S.A = USA e.g. remove all . Asymmetric expansion. (Powerful but less efficient) We generally use the first one which is simple but want to pay attention to case folding e.g. US vs us. We also want to do Lemmatization (reduce to base form) e.g. am, are, is --> be | car, cars, car's, cars' --> car Morphemes : Smallest unit that makes up a word e.g. Stem (the core meaning bearing units), affixes (bits and pieces that adhere to stems, they often have gramatical functions) Stemming Reduce terms to their stems. Its crude chopping of affixes Language dependent Its a simplification of Lemmatization","title":"NPL"},{"location":"Machine%20Learning/nlp/NPL/#parts-of-speech","text":"Use this link to get a crash course on grammar. Verb Denotes action e.g. runs, bites. A little more complex example ==> \"Karl Creelman bicycled around the world in 1899, but his diaries and his bicycle were destroyed .\" where were destroyed denotes the action so that is the verb. Noun Word used to name a person, animal, place, thing, and abstract idea. e.g. - Late last year our neighbours bought a goat . - Portia White was an opera singer . - The bus inspector looked at all the passengers' passes. Many common nouns, like \"engineer\" or \"teacher,\" can refer to men or women. Nouns will often add -s or -es for plurels. When Matthew was small he rarely told the truth if he thought he was going to be punished. Many people do not believe that truths are self-evident. Possessive Nouns The red suitcase is Cassandra's . The only luggage that was lost was the prime minister's. The exhausted recruits were woken before dawn by the - drill sergeant's screams. The miner's face was covered in coal dust. There are various types of nouns Proper noun Common noun Concrete noun Abstract noun Countable Non countable Collective Pronoun A pronoun can replace a noun or another pronoun e.g. he, which, none, you. Adjective An adjective modifies a noun or a pronoun by describing, identifying, or quantifying words. An adjective usually precedes the noun or the pronoun which it modifies. The truck-shaped balloon floated over the treetops. Mrs. Morrison papered her kitchen walls with hideous - wall paper. The small boat foundered on the wine dark sea. Adverb An adverb can modify a verb, an adjective, another adverb, a phrase, or a clause. An adverb indicates manner, time, place, cause, or degree and answers questions such as \"how,\" \"when,\" \"where,\" \"how much\". The seamstress quickly made the mourning clothes. The midwives waited patiently through a long labour. Preposition A preposition links nouns, pronouns and phrases to other words in a sentence. The word or phrase that the preposition introduces is called the object of the preposition. The book is on the table. She read the book during class. The children climbed the mountain without fear. (In this sentence, the preposition \"without\" introduces the noun \"fear.\") Conjunction You can use a conjunction to link words, phrases, and clauses, as in the following example: I ate the pizza and the pasta. Call the movers when you are ready. After she had learned to drive, Alice felt more independent. Interjection An interjection is a word added to a sentence to convey emotion. It is not grammatically related to any other part of the sentence. Hey! Put that down! I heard one guy say to another guy, \"He has a new car, - eh ?\" I don't know about you but, good lord , I think taxes are - too high!","title":"Parts of speech"},{"location":"Machine%20Learning/nlp/NPL/#regular-expression","text":"The most basic tool we have to process text is regular expression . They are heavily used even in ML classifiers etc... as features. Disjunction - Letters inside [] - e.g. [wW] either w or W or ranges e.g. [A-Z] or [a-z] or [0-9] or [A-Za-z] Negation - ^ - [^A-z] - not a capital letter, only if occurs right after a [ . OR - | ? - previous character is optional e.g. colou?r will match with our without the u * - 0 or more of previous character e.g. oo*h --> oh, ooh, oooh, ooooh, ... + - ` 1 or more of previous character . - means any character e.g. beg.n --> begin, began begun beg4n etc... ^ - Begining of line e.g. ^[A-Z] Capital letter at begining of line $ - End of line e.g. [A-Z]$ Capital letter at end of line You can use regular expression to match lots of text but still you will deal with precesion vs recall. Increase accuracy or precesion (minimizing false positive) Increase coverage or recall (minimize false negetive)","title":"Regular Expression"},{"location":"Machine%20Learning/nlp/NPL/#word-tokenization","text":"Breaking down a sentence into word tokens. Some sentences will have fragments, filled pauses like uh or aah --> should we consider these as tokens ? Lemma : same stem, same word sense etc... e.g. cat, cats = same lemma Wordforms : cat and cats = different wordforms How many words ? N = # of tokens --> an instance of vocabulary type in that text. e.g. total # of words in a sentence V = vocabulary = set of types. $|V|$ is the size of vocabulary. --> how many unique words there are ? Types --> could depend on if you count 2 different words with same lemma as 1 or multiple. There are various issues in Chinese / German / Japanese where you have no concept of word or no spaces between them, the things get a little tough there.","title":"Word Tokenization"},{"location":"Machine%20Learning/nlp/NPL/#normalize-or-stemming","text":"Normalization Implicitly define equivalence class of terms ==> U.S.A = USA e.g. remove all . Asymmetric expansion. (Powerful but less efficient) We generally use the first one which is simple but want to pay attention to case folding e.g. US vs us. We also want to do Lemmatization (reduce to base form) e.g. am, are, is --> be | car, cars, car's, cars' --> car Morphemes : Smallest unit that makes up a word e.g. Stem (the core meaning bearing units), affixes (bits and pieces that adhere to stems, they often have gramatical functions) Stemming Reduce terms to their stems. Its crude chopping of affixes Language dependent Its a simplification of Lemmatization","title":"Normalize or Stemming"},{"location":"Misc/bash/","text":"Bash/Shell Scripting A simple bash script #! /bin/bash echo \"hello bash scripting\" Once you create a bash script you can make it executable by adding executable permission on it. chmod +x <filename> If you wish the output of the script to go to another file you can #! /bin/bash echo \"hello bash scripting\" > file1.txt This will open shell in write mode and you can write anything and output will go to file.ext cat > file.txt To append use >> and not > multiline comments : ' sdfasf sa fas fasf' Heredoc -- This will show up when you run the file. cat << meaningfull_variable_name Some output here to display meaningfull_variable_name Conditional statements count = 10 if [ $count -eq 10 ] then echo \"this condition is true\" else echo \"this condition is false\" fi for not equal use -ne , -gt greater than. If you wish use > sign use the below format. if (( $count > 9 )) then xxxx else xxxx fi elif syntax is something like this. if (( $count > 9 )) then xxxx elif (( condition )) then xxxx elif (( condition )) then xxxx else xxxx fi and `&& operator age = 10 if [ \" $age \" -gt 18 ] && [ \" $age \" -lt 40 ] # --> same as above if [ \"$age\" -gt 18 -a \"$age\" -lt 40 ] then echo \"Age is correct\" else echo \"Age is not correct\" fi or operator if [ \" $age \" -gt 18 ] || [ \" $age \" -lt 40 ] # or if [ \"$age\" -gt 18 -o \"$age\" -lt 40 ] Loops. #! /bin/bash number = 1 while [ $number -lt 10 ] do echo \" $number \" number = $(( number + 1 )) done You have until loop as well, which runs untill the condition becomes true. --> i.e. the condition is false. until [ condition ] do xxxx done for loop for i in 1 2 3 4 5 do xxxx done for i in { 0 ..20 } do xxx done Increment of 2 for i in { 0 ..20..2 } do xxx done for i in (( i = 0 ; i< 5 ; i++ )) do xxx done Break and continue statement. break and continue Script input $0 $1 $2 --> are the arguments 0th would be the shell script (file) name which you are running. #! /bin/bash echo $0 $1 $3 # running the script > ./sh5.sh today is monday ./sh5.sh today monday Storing them in array instead of individual variables. args =( \" $@ \" ) echo ${ args [0] } ${ args [1] } ${ args [2] } As you see the script name no longer goes to $0 > ./sh5.sh abc def ghi abc def ghi You can also print all the values and not just individual by printing the whole array itsef. args =( \" $@ \" ) echo $@ > /sh5.sh abc def ghi jkl sdf ers abc def ghi jkl sdf ers To measure the length of array you can use $# Read the file one line at a time. while read line do echo \" $line \" done < \" ${ 1 :- /dev/stdin } \"","title":"Bash/Shell Scripting"},{"location":"Misc/bash/#bashshell-scripting","text":"A simple bash script #! /bin/bash echo \"hello bash scripting\" Once you create a bash script you can make it executable by adding executable permission on it. chmod +x <filename> If you wish the output of the script to go to another file you can #! /bin/bash echo \"hello bash scripting\" > file1.txt This will open shell in write mode and you can write anything and output will go to file.ext cat > file.txt To append use >> and not > multiline comments : ' sdfasf sa fas fasf' Heredoc -- This will show up when you run the file. cat << meaningfull_variable_name Some output here to display meaningfull_variable_name Conditional statements count = 10 if [ $count -eq 10 ] then echo \"this condition is true\" else echo \"this condition is false\" fi for not equal use -ne , -gt greater than. If you wish use > sign use the below format. if (( $count > 9 )) then xxxx else xxxx fi elif syntax is something like this. if (( $count > 9 )) then xxxx elif (( condition )) then xxxx elif (( condition )) then xxxx else xxxx fi and `&& operator age = 10 if [ \" $age \" -gt 18 ] && [ \" $age \" -lt 40 ] # --> same as above if [ \"$age\" -gt 18 -a \"$age\" -lt 40 ] then echo \"Age is correct\" else echo \"Age is not correct\" fi or operator if [ \" $age \" -gt 18 ] || [ \" $age \" -lt 40 ] # or if [ \"$age\" -gt 18 -o \"$age\" -lt 40 ] Loops. #! /bin/bash number = 1 while [ $number -lt 10 ] do echo \" $number \" number = $(( number + 1 )) done You have until loop as well, which runs untill the condition becomes true. --> i.e. the condition is false. until [ condition ] do xxxx done for loop for i in 1 2 3 4 5 do xxxx done for i in { 0 ..20 } do xxx done Increment of 2 for i in { 0 ..20..2 } do xxx done for i in (( i = 0 ; i< 5 ; i++ )) do xxx done Break and continue statement. break and continue Script input $0 $1 $2 --> are the arguments 0th would be the shell script (file) name which you are running. #! /bin/bash echo $0 $1 $3 # running the script > ./sh5.sh today is monday ./sh5.sh today monday Storing them in array instead of individual variables. args =( \" $@ \" ) echo ${ args [0] } ${ args [1] } ${ args [2] } As you see the script name no longer goes to $0 > ./sh5.sh abc def ghi abc def ghi You can also print all the values and not just individual by printing the whole array itsef. args =( \" $@ \" ) echo $@ > /sh5.sh abc def ghi jkl sdf ers abc def ghi jkl sdf ers To measure the length of array you can use $# Read the file one line at a time. while read line do echo \" $line \" done < \" ${ 1 :- /dev/stdin } \"","title":"Bash/Shell Scripting"},{"location":"Misc/linx/","text":"Linux File structure Google : \"Linx Filesystem Hierarchy Standard\" to get the big pdf which explains the file system of a linux system. However its summarized in 3.2 Requirements section as to which root directory is for what purpose. The root system starts at / . Below listed are some of the important dirs and their uses. boot - files which boots your system. (It also has files which the grub bootloader reads) dev - files that represent devices. e.g. HardDrive etc... etc - configuration directory. (System and applications) When you remove a package via apt remove it does not remove the configuration files for that package. You can use --purge to remove the configurations files from /etc/ lib/lib64 - library files e.g. packages or shared libraries mnt/media - you attach storage media to the system. (Could be a hadoop cluster or a flash drive). Where media is more removable but mnt is for network related. proc - process system related files root - root user does not a dir in /home . It stores in /root tmp - where temporary files live var - There are various dirs in it but check out /var/log . The linux system pretty much logs everything and you can find some details here. (Mostly used for trouble shooting) Env variables Check this link for details. .bashrc - This file is user specific file that gets loaded each time user creates a new local session i.e. in simple words, opens a new terminal. All environment variables created in this file would take effect every time a new local session is started. .bash_profile - This file is user specific remote login file. Environment variables listed in this file are invoked every time the user is logged in remotely i.e. using ssh session. If this file is not present, system looks for either .bash_login or .profile files. /etc/envirounment - This file is system wide file for creating, editing or removing any environment variables. Environment variables created in this file are accessible all throughout the system, by each and every user, both locally and remotely. /etc/bash.bashrc - System wide bashrc file. This file is loaded once for every user, each time that user opens a local terminal session. Environment variables created in this file are accessible for all users but only through local terminal session. When any user on that machine is accessed remotely via a remote login session, these variables would not be visible. /etc/profile - System wide profile file. All the variables created in this file are accessible by every user on the system, but only if that user\u2019s session is invoked remotely, i.e. via remote login. Any variable in this file will not be accessible for local login session i.e. when user opens a new terminal on his local system. Useful commands Tar .tar file creates a single file bundling multiple files. It does not do compression like zip etc... Creating a tar file - -c create archive file - -f lets you specify file name - -z if you add z e.g. -cfz it will use gzip to compress the tar file. tar -cf del.tar del Opening up a tar file tar -xvf del.tar Viewing a tar file tar tvf del.tar grep Search for a given string in a file or content in a file. grep \"@\" lori.christmas.txt -r is recursive. grep -r \"error\" /var/log/syslog rm Remove del dir and all its contents rm -rf del/ Get info before deleting a file rm -i filename.txt ssh Login to remote host ssh user@ip -p port# sed Convert windows format to unix format file. When you copy a DOS file to Unix, you could find \\r\\n in the end of each line. This example converts the DOS file format to Unix file format using sed command. sed 's/.$//' filename pwd print working directory pwd service Service command is used to run the system V init scripts. i.e Instead of calling the scripts located in the /etc/init.d/ directory with their full path, you can use the service command. Check status service ssh status Check the status of all the services. service --status-all Restart a service. service ssh restart cp Copy file1 to file2 preserving the mode, ownership and timestamp cp -p file1 file2 mv Move file chmod Change mode (permissions) Total 9 charaters govern the permissions (apart from the 1 on the left which tells us that whether its a direcotry (d) file (-) or a symlink (l)). The permissions are divided into 3 of 3 groups (total 9) with user, group and others in that order. First 3 - (Read, Write, Execute) - for user. Next 3 - (Read, Write, Execute) - for group. Last 3 - (Read, Write, Execute) - for others (eveyone else). Each operation has a number assigned to it. Read = 4 Write = 2 Execute = 1 The below command gives all permissions to everyone. chown 777 abc.txt Note : If you do not have execute on a directory then you cannot cd into it. i.e. you cannot make it your current working directory. chown Change ownership whereis When you want to find out where a specific Unix command exists (for example, where does ls command exists?), you can execute the following command. whereis ls whatis Whatis command displays a single line description about a command. su You can switch user using the su command. e.g. su hadoop or if you wish to login as root (not you executing as root priviledges) sudo su - systemctl Use systemctl command to check system services. Some of the options are listed below. start stop status disable - when the server restarts this service will not auto run enable - when the server restarts this service will auto run history The history command gives you the list of all commands you have executed. You can also grep through it as shoown below history | grep apt The history command output -- each command is associated with a number. You can reexeucte that command using the !number as shown below !162 head tail The cat command shows you the whole file, but head or tail and show you (by default 10) first or last lines. You can customize it with -n 100 to see 100 (first or last) lines. You can also use -f to follow a file. (This will not return you the prompt back but will update your terminal window with new lines added to this file so its like a live debugger) For example lets follow the auth log file tail -f /var/log/auth.log journalctl This is (a upcoming) command where you can check out the logs of a particular service. journalctl -u apache2 To follow logs journalctl -fu apache2 df Disk usage command. Tack on -h to get human readable output (also you can do that with ls e.g. ls -lh ) df -h Gives details of that dir and sub dir. du -hsc /home/* You can also use package ncdu to get more detail information about the file system disk usage. find File any file which is in that dir and has name ending in .log Use (-iname) for case insensitive. find /var/log -name \"*.log\" ! acts as a negation. Find which does not end in .log find /var/log ! -name \"*.log\" The find command by default does not distinguish b/w files and directory. You can use the -type d for dirs and -type f for files. find /etc -type d -name \"x*\" Find files which were modified in certain duration (days) sudo find /home/rs -mtime 1 Find all markdown files which were modified within last 3 mins find /home/rs -cmin -3 -name \"*.md\" Find command gets advanced and here is a good example below. Find all dirs in the current dir, where permission is not 755 and execute chmod on those dirs (and not files) to make them 755 find . -type d ! -perm 755 -exec chmod 755 {} \\; Misc topics Checking the logs Check out the /var/logs/auth* file to check out. This file logs like package installs, login failures, sudo access and doing stuff etc... and if you see that someone did something you can go to their history file and check out what command they ran. Check out the syslog file in the same place. It also logs all types of things the system does. (e.g. start service, schedule tasks, configuration failures for applications) Check out hte apt dir in /var/logs to check out all apt related logs. (only for deb based system which use apt, other distros may have something different e.g. yum or arch equivalent) Alias You can create your own shorthand for long commands. alias cb = \"vim ~/.bashrc\" When you create these aliases, they are not stored on the disk and are lost when you close the shell. If you wish to store them you need to store them in your .bashrc file. Working with streams There are total 3 types of streams Standard input - Assigned 0 Standard output - Assigned 1 Standard error - Assigned 2 Any output which is displayed on the terminal is part of the standard output stdout . E.g. take the output produced by the command and save in hello.txt. (As you see 1 is for standard output) echo \"Hello World\" 1 > hello.txt Another example - When you run a find command some of the dirs you will not have access to. So lets say you only wish to see the dirs which do not result in error, then you can send the error entries to /dev/null which is like a black hole in linux. Anything which is sent there (output or file) is effectively deleted or lost. find / -name \"log\" 2 > /dev/null /run/log /usr/src/linux-headers-5.4.0-33-generic/include/config/dm/log /usr/src/linux-headers-5.4.0-33-generic/include/config/nf/log /usr/src/linux-headers-5.4.0-33-generic/include/config/log /usr/src/linux-headers-5.4.0-33-generic/include/config/printk/safe/log /sys/module/vboxguest/parameters/log Find all log files and redirect standard output > (by default its assumed to be 1 if not specified) and also send standard error (2) to standard output(1). So results.txt will contain dirs which you have access to and dirs which you do not have access to. find / -name \"log\" > results.txt 2 > & 1 Another example : push standard output first (files in all dirs which you have access to) and then append standard error (all dirs which you do not have access to) in results.txt find / -name \"log\" > results.txt 2 >>results.txt A quick example of standard input command. cat < results.txt Background and foreground Exit long sunning process - Ctrl + z Get back to long running progress - fg To check applications in backgound run job command References Linx man pages 50 most used commands","title":"Linux"},{"location":"Misc/linx/#linux","text":"","title":"Linux"},{"location":"Misc/linx/#file-structure","text":"Google : \"Linx Filesystem Hierarchy Standard\" to get the big pdf which explains the file system of a linux system. However its summarized in 3.2 Requirements section as to which root directory is for what purpose. The root system starts at / . Below listed are some of the important dirs and their uses. boot - files which boots your system. (It also has files which the grub bootloader reads) dev - files that represent devices. e.g. HardDrive etc... etc - configuration directory. (System and applications) When you remove a package via apt remove it does not remove the configuration files for that package. You can use --purge to remove the configurations files from /etc/ lib/lib64 - library files e.g. packages or shared libraries mnt/media - you attach storage media to the system. (Could be a hadoop cluster or a flash drive). Where media is more removable but mnt is for network related. proc - process system related files root - root user does not a dir in /home . It stores in /root tmp - where temporary files live var - There are various dirs in it but check out /var/log . The linux system pretty much logs everything and you can find some details here. (Mostly used for trouble shooting)","title":"File structure"},{"location":"Misc/linx/#env-variables","text":"Check this link for details. .bashrc - This file is user specific file that gets loaded each time user creates a new local session i.e. in simple words, opens a new terminal. All environment variables created in this file would take effect every time a new local session is started. .bash_profile - This file is user specific remote login file. Environment variables listed in this file are invoked every time the user is logged in remotely i.e. using ssh session. If this file is not present, system looks for either .bash_login or .profile files. /etc/envirounment - This file is system wide file for creating, editing or removing any environment variables. Environment variables created in this file are accessible all throughout the system, by each and every user, both locally and remotely. /etc/bash.bashrc - System wide bashrc file. This file is loaded once for every user, each time that user opens a local terminal session. Environment variables created in this file are accessible for all users but only through local terminal session. When any user on that machine is accessed remotely via a remote login session, these variables would not be visible. /etc/profile - System wide profile file. All the variables created in this file are accessible by every user on the system, but only if that user\u2019s session is invoked remotely, i.e. via remote login. Any variable in this file will not be accessible for local login session i.e. when user opens a new terminal on his local system.","title":"Env variables"},{"location":"Misc/linx/#useful-commands","text":"Tar .tar file creates a single file bundling multiple files. It does not do compression like zip etc... Creating a tar file - -c create archive file - -f lets you specify file name - -z if you add z e.g. -cfz it will use gzip to compress the tar file. tar -cf del.tar del Opening up a tar file tar -xvf del.tar Viewing a tar file tar tvf del.tar grep Search for a given string in a file or content in a file. grep \"@\" lori.christmas.txt -r is recursive. grep -r \"error\" /var/log/syslog rm Remove del dir and all its contents rm -rf del/ Get info before deleting a file rm -i filename.txt ssh Login to remote host ssh user@ip -p port# sed Convert windows format to unix format file. When you copy a DOS file to Unix, you could find \\r\\n in the end of each line. This example converts the DOS file format to Unix file format using sed command. sed 's/.$//' filename pwd print working directory pwd service Service command is used to run the system V init scripts. i.e Instead of calling the scripts located in the /etc/init.d/ directory with their full path, you can use the service command. Check status service ssh status Check the status of all the services. service --status-all Restart a service. service ssh restart cp Copy file1 to file2 preserving the mode, ownership and timestamp cp -p file1 file2 mv Move file chmod Change mode (permissions) Total 9 charaters govern the permissions (apart from the 1 on the left which tells us that whether its a direcotry (d) file (-) or a symlink (l)). The permissions are divided into 3 of 3 groups (total 9) with user, group and others in that order. First 3 - (Read, Write, Execute) - for user. Next 3 - (Read, Write, Execute) - for group. Last 3 - (Read, Write, Execute) - for others (eveyone else). Each operation has a number assigned to it. Read = 4 Write = 2 Execute = 1 The below command gives all permissions to everyone. chown 777 abc.txt Note : If you do not have execute on a directory then you cannot cd into it. i.e. you cannot make it your current working directory. chown Change ownership whereis When you want to find out where a specific Unix command exists (for example, where does ls command exists?), you can execute the following command. whereis ls whatis Whatis command displays a single line description about a command. su You can switch user using the su command. e.g. su hadoop or if you wish to login as root (not you executing as root priviledges) sudo su - systemctl Use systemctl command to check system services. Some of the options are listed below. start stop status disable - when the server restarts this service will not auto run enable - when the server restarts this service will auto run history The history command gives you the list of all commands you have executed. You can also grep through it as shoown below history | grep apt The history command output -- each command is associated with a number. You can reexeucte that command using the !number as shown below !162 head tail The cat command shows you the whole file, but head or tail and show you (by default 10) first or last lines. You can customize it with -n 100 to see 100 (first or last) lines. You can also use -f to follow a file. (This will not return you the prompt back but will update your terminal window with new lines added to this file so its like a live debugger) For example lets follow the auth log file tail -f /var/log/auth.log journalctl This is (a upcoming) command where you can check out the logs of a particular service. journalctl -u apache2 To follow logs journalctl -fu apache2 df Disk usage command. Tack on -h to get human readable output (also you can do that with ls e.g. ls -lh ) df -h Gives details of that dir and sub dir. du -hsc /home/* You can also use package ncdu to get more detail information about the file system disk usage. find File any file which is in that dir and has name ending in .log Use (-iname) for case insensitive. find /var/log -name \"*.log\" ! acts as a negation. Find which does not end in .log find /var/log ! -name \"*.log\" The find command by default does not distinguish b/w files and directory. You can use the -type d for dirs and -type f for files. find /etc -type d -name \"x*\" Find files which were modified in certain duration (days) sudo find /home/rs -mtime 1 Find all markdown files which were modified within last 3 mins find /home/rs -cmin -3 -name \"*.md\" Find command gets advanced and here is a good example below. Find all dirs in the current dir, where permission is not 755 and execute chmod on those dirs (and not files) to make them 755 find . -type d ! -perm 755 -exec chmod 755 {} \\;","title":"Useful commands"},{"location":"Misc/linx/#misc-topics","text":"","title":"Misc topics"},{"location":"Misc/linx/#checking-the-logs","text":"Check out the /var/logs/auth* file to check out. This file logs like package installs, login failures, sudo access and doing stuff etc... and if you see that someone did something you can go to their history file and check out what command they ran. Check out the syslog file in the same place. It also logs all types of things the system does. (e.g. start service, schedule tasks, configuration failures for applications) Check out hte apt dir in /var/logs to check out all apt related logs. (only for deb based system which use apt, other distros may have something different e.g. yum or arch equivalent)","title":"Checking the logs"},{"location":"Misc/linx/#alias","text":"You can create your own shorthand for long commands. alias cb = \"vim ~/.bashrc\" When you create these aliases, they are not stored on the disk and are lost when you close the shell. If you wish to store them you need to store them in your .bashrc file.","title":"Alias"},{"location":"Misc/linx/#working-with-streams","text":"There are total 3 types of streams Standard input - Assigned 0 Standard output - Assigned 1 Standard error - Assigned 2 Any output which is displayed on the terminal is part of the standard output stdout . E.g. take the output produced by the command and save in hello.txt. (As you see 1 is for standard output) echo \"Hello World\" 1 > hello.txt Another example - When you run a find command some of the dirs you will not have access to. So lets say you only wish to see the dirs which do not result in error, then you can send the error entries to /dev/null which is like a black hole in linux. Anything which is sent there (output or file) is effectively deleted or lost. find / -name \"log\" 2 > /dev/null /run/log /usr/src/linux-headers-5.4.0-33-generic/include/config/dm/log /usr/src/linux-headers-5.4.0-33-generic/include/config/nf/log /usr/src/linux-headers-5.4.0-33-generic/include/config/log /usr/src/linux-headers-5.4.0-33-generic/include/config/printk/safe/log /sys/module/vboxguest/parameters/log Find all log files and redirect standard output > (by default its assumed to be 1 if not specified) and also send standard error (2) to standard output(1). So results.txt will contain dirs which you have access to and dirs which you do not have access to. find / -name \"log\" > results.txt 2 > & 1 Another example : push standard output first (files in all dirs which you have access to) and then append standard error (all dirs which you do not have access to) in results.txt find / -name \"log\" > results.txt 2 >>results.txt A quick example of standard input command. cat < results.txt","title":"Working with streams"},{"location":"Misc/linx/#background-and-foreground","text":"Exit long sunning process - Ctrl + z Get back to long running progress - fg To check applications in backgound run job command","title":"Background and foreground"},{"location":"Misc/linx/#references","text":"Linx man pages 50 most used commands","title":"References"},{"location":"Python/Arrays/","text":"Arrays Use arrays (which are as lean as the arrays in language C) if - The list will contain only numbers You need to specify the type when creating the array. Functions to read and write data from and to array are easy and very fast as compared to reading/writing text files. (upto 60 times faster) - Array.tofile - Array.fromfile Creating a simple Array from array import array floats = array ( 'd' , ( 10.1 , 10.2 )) # array('d', [10.1, 10.2]) print ( floats ) Saving data in file from array import array from random import random data = array ( 'd' , ( 10.1 , 10.2 )) # Write to file. fp = open ( 'data.bin' , 'wb' ) data . tofile ( fp ) fp . close () Reading data from file You can use array.fromfile(f, n) where f is file and n is number of items. (Yes you need to know the number of items.) Check other methods here","title":"Arrays"},{"location":"Python/Arrays/#arrays","text":"Use arrays (which are as lean as the arrays in language C) if - The list will contain only numbers You need to specify the type when creating the array. Functions to read and write data from and to array are easy and very fast as compared to reading/writing text files. (upto 60 times faster) - Array.tofile - Array.fromfile Creating a simple Array from array import array floats = array ( 'd' , ( 10.1 , 10.2 )) # array('d', [10.1, 10.2]) print ( floats ) Saving data in file from array import array from random import random data = array ( 'd' , ( 10.1 , 10.2 )) # Write to file. fp = open ( 'data.bin' , 'wb' ) data . tofile ( fp ) fp . close () Reading data from file You can use array.fromfile(f, n) where f is file and n is number of items. (Yes you need to know the number of items.) Check other methods here","title":"Arrays"},{"location":"Python/Bytes/","text":"Bytes Strings are sequences which are immutable and store only unicode texts. Bytes and mutatble alternative bytearray store only bytes as sequence values. (which are 0 \\<= x \\<= 256). ``` {.python results=\"output\" exports=\"both\"} print(list(b'foo bar')) # use b to convert to byte print(tuple(b'foo bar')) ``` {.example} [102, 111, 111, 32, 98, 97, 114] (102, 111, 111, 32, 98, 97, 114) ``` {.python results=\"output\" exports=\"both\"} print(bytes([100,101,102, 103])) ``` {.example} b'defg' The bytes and bytearray allow to work with raw binary data e.g. image files, video, audio and network packets etc... You can create a bytearray using bytearray constructor like this bytearray(b'foo bar') There are various other things you can do with bytes and bytes arrays which you can check out online. {.python results=\"output\" exports=\"both\"}","title":"Bytes"},{"location":"Python/Bytes/#bytes","text":"Strings are sequences which are immutable and store only unicode texts. Bytes and mutatble alternative bytearray store only bytes as sequence values. (which are 0 \\<= x \\<= 256). ``` {.python results=\"output\" exports=\"both\"} print(list(b'foo bar')) # use b to convert to byte print(tuple(b'foo bar')) ``` {.example} [102, 111, 111, 32, 98, 97, 114] (102, 111, 111, 32, 98, 97, 114) ``` {.python results=\"output\" exports=\"both\"} print(bytes([100,101,102, 103])) ``` {.example} b'defg' The bytes and bytearray allow to work with raw binary data e.g. image files, video, audio and network packets etc... You can create a bytearray using bytearray constructor like this bytearray(b'foo bar') There are various other things you can do with bytes and bytes arrays which you can check out online. {.python results=\"output\" exports=\"both\"}","title":"Bytes"},{"location":"Python/Callables/","text":"Callables The __call__ has a syntactic operator () . To check if the object is callable use the callable() buit in function. The python data model lists 7 types of callable types. User Defined Functions like named or lambda functions Built in functions like =len=\\ Buit in methods like dict.get Methods of a class Classes. (When invoked it runs __new__ and then __init__ ) Classes instalce --> If the class has a __call__ method Generator functions --> use yield There are other 2 as well but they deal with async and co-routines check this link for further details. Link You can implement __call__ method and make objects callable e.g. example given below. This is an interesting way to implement function like callables which need an internal state. This can also be done with the help of a closure . from random import randint class RandomNumber : def __init__ ( self ): self . calls = 0 def __call__ ( self ): self . calls += 1 return randint ( 1 , 100 ) Testing >>> r = RandomNumber () >>> r () 62 >>> r () 28 >>> r . calls 2","title":"Callables"},{"location":"Python/Callables/#callables","text":"The __call__ has a syntactic operator () . To check if the object is callable use the callable() buit in function. The python data model lists 7 types of callable types. User Defined Functions like named or lambda functions Built in functions like =len=\\ Buit in methods like dict.get Methods of a class Classes. (When invoked it runs __new__ and then __init__ ) Classes instalce --> If the class has a __call__ method Generator functions --> use yield There are other 2 as well but they deal with async and co-routines check this link for further details. Link You can implement __call__ method and make objects callable e.g. example given below. This is an interesting way to implement function like callables which need an internal state. This can also be done with the help of a closure . from random import randint class RandomNumber : def __init__ ( self ): self . calls = 0 def __call__ ( self ): self . calls += 1 return randint ( 1 , 100 ) Testing >>> r = RandomNumber () >>> r () 62 >>> r () 28 >>> r . calls 2","title":"Callables"},{"location":"Python/Collections/","text":"Collections Queue & Deque Deque The class collections.deque is a thread-safe double-ended queue designed for fast inserting and removing from both ends. >>> from collections import deque >>> dq = deque ( range ( 10 ), maxlen = 10 ) >>> dq deque ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ], maxlen = 10 ) >>> dq . rotate ( 3 ) >>> dq deque ([ 7 , 8 , 9 , 0 , 1 , 2 , 3 , 4 , 5 , 6 ], maxlen = 10 ) >>> dq . rotate ( - 4 ) >>> dq deque ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 0 ], maxlen = 10 ) >>> dq . appendleft ( - 1 ) >>> dq deque ([ - 1 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ], maxlen = 10 ) >>> dq . extend ([ 11 , 22 , 33 ]) >>> dq deque ([ 3 , 4 , 5 , 6 , 7 , 8 , 9 , 11 , 22 , 33 ], maxlen = 10 ) >>> dq . extendleft ([ 10 , 20 , 30 , 40 ]) >>> dq deque ([ 40 , 30 , 20 , 10 , 3 , 4 , 5 , 6 , 7 , 8 ], maxlen = 10 ) Other queues There are multiple other types of queues which are supported by Python queue multiprocessing --> Queue asyncio --> Queue, LifoQueue, PriorityQueue, JoinableQueue Named Tuples Creating named tuples is easy. nt = namedtuple('t', ('a', 'b')) - Where \\'t\\' is the name of named tuple - a and b are parameters namedtuple can be used to build classes of objects that are just bundles of attributes with no custom methods, like a database record. import collections Student = collections . namedtuple ( 'Student' , ( 'name' , 'age' , 'id' )) sam = Student ( \"Sam\" , 10 , 11 ) print ( sam ) print ( sam . name ) print ( sam . age ) You can check the fields of a named tuple using _fields >>> Student . _fields ( 'name' , 'age' , 'id' ) Named tuples can be nested in another named tuples. Other useful methods _asdict() >>> sam = Student ( \"Sam\" , 10 , 11 ) >>> >>> sam . _asdict () OrderedDict ([( 'name' , 'Sam' ), ( 'age' , 10 ), ( 'id' , 11 )]) _make() ~make~() allow you to instantiate a named tuple from an iterable Why can some tuples be hashed and some do not? Tuples are immutable objects (by themselves). However tuples can contain lists which are mutable (and cannot be hashed). Thus you get the scenario below. In [ 7 ]: t1 = ( 1 ) In [ 8 ]: t2 = ([ 1 ]) In [ 9 ]: hash ( t1 ) Out [ 9 ]: 1 In [ 10 ]: hash ( t2 ) --------------------------------------------------------------------------- TypeError Traceback ( most recent call last ) < ipython - input - 10 - c1655d61dc02 > in < module > ----> 1 hash ( t2 ) TypeError : unhashable type : 'list'","title":"Collections"},{"location":"Python/Collections/#collections","text":"","title":"Collections"},{"location":"Python/Collections/#queue-deque","text":"","title":"Queue &amp; Deque"},{"location":"Python/Collections/#deque","text":"The class collections.deque is a thread-safe double-ended queue designed for fast inserting and removing from both ends. >>> from collections import deque >>> dq = deque ( range ( 10 ), maxlen = 10 ) >>> dq deque ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ], maxlen = 10 ) >>> dq . rotate ( 3 ) >>> dq deque ([ 7 , 8 , 9 , 0 , 1 , 2 , 3 , 4 , 5 , 6 ], maxlen = 10 ) >>> dq . rotate ( - 4 ) >>> dq deque ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 0 ], maxlen = 10 ) >>> dq . appendleft ( - 1 ) >>> dq deque ([ - 1 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ], maxlen = 10 ) >>> dq . extend ([ 11 , 22 , 33 ]) >>> dq deque ([ 3 , 4 , 5 , 6 , 7 , 8 , 9 , 11 , 22 , 33 ], maxlen = 10 ) >>> dq . extendleft ([ 10 , 20 , 30 , 40 ]) >>> dq deque ([ 40 , 30 , 20 , 10 , 3 , 4 , 5 , 6 , 7 , 8 ], maxlen = 10 )","title":"Deque"},{"location":"Python/Collections/#other-queues","text":"There are multiple other types of queues which are supported by Python queue multiprocessing --> Queue asyncio --> Queue, LifoQueue, PriorityQueue, JoinableQueue","title":"Other queues"},{"location":"Python/Collections/#named-tuples","text":"Creating named tuples is easy. nt = namedtuple('t', ('a', 'b')) - Where \\'t\\' is the name of named tuple - a and b are parameters namedtuple can be used to build classes of objects that are just bundles of attributes with no custom methods, like a database record. import collections Student = collections . namedtuple ( 'Student' , ( 'name' , 'age' , 'id' )) sam = Student ( \"Sam\" , 10 , 11 ) print ( sam ) print ( sam . name ) print ( sam . age ) You can check the fields of a named tuple using _fields >>> Student . _fields ( 'name' , 'age' , 'id' ) Named tuples can be nested in another named tuples. Other useful methods _asdict() >>> sam = Student ( \"Sam\" , 10 , 11 ) >>> >>> sam . _asdict () OrderedDict ([( 'name' , 'Sam' ), ( 'age' , 10 ), ( 'id' , 11 )]) _make() ~make~() allow you to instantiate a named tuple from an iterable","title":"Named Tuples"},{"location":"Python/Collections/#why-can-some-tuples-be-hashed-and-some-do-not","text":"Tuples are immutable objects (by themselves). However tuples can contain lists which are mutable (and cannot be hashed). Thus you get the scenario below. In [ 7 ]: t1 = ( 1 ) In [ 8 ]: t2 = ([ 1 ]) In [ 9 ]: hash ( t1 ) Out [ 9 ]: 1 In [ 10 ]: hash ( t2 ) --------------------------------------------------------------------------- TypeError Traceback ( most recent call last ) < ipython - input - 10 - c1655d61dc02 > in < module > ----> 1 hash ( t2 ) TypeError : unhashable type : 'list'","title":"Why can some tuples be hashed and some do not?"},{"location":"Python/Context%20Managers/","text":"Context Managers The with statement sets up a temporary context and reliably tears it down. The with statement was designed to simplify the try/finally pattern which gaureentees even if the block is aborted even because of an exception the finally clause will release the critical resource. The context manager works with __enter__ and __exit__ protocol. Usage The context manager should be used for common setup and tear down code, which is not just applicable for files. There can be various uses for it. # Sample context manager for file. class File : def __init__ ( self , file , method ): self . file = open ( file , method ) def __enter__ ( self ): return self . file # returns the object which will sit in \"f\" --> with File(abc.txt, w) as \"f\" def __exit__ ( self , exception_type , exception_value , traceback ): # The exit must take in these 4 params else it raises exception self . file . close () The context manager which you see above provides a pattern of __enter__ and __exit__ . However if you see there is no exception which is handled here. One way is to handle exceptionions manually... # Another example of a context manager. import time class Timer : def __enter__ ( self ): self . start = time . time_ns () return self def __exit__ ( self , exception_type , exception_value , traceback ): self . end = time . time_ns () self . processing_time = self . end - self . start with Timer () as t : time . sleep ( . 1 ) for i in range ( 10 ): print ( i ) t . processing_time 0 1 2 3 4 5 6 7 8 9 100219600 contextmanager Handling all sorts of exceptions can be difficult so python has provided with a decorator that turns a generator function into a context manager. Because think of it __enter__ is starting something and pausing and then __exit__ takes over when we have to tear it down or finish the work which is exactly what a generator does it yeilds and waits. You can use the contextlib library to import the contextmanager decorator to turn your generator into a context manager. Go and check out the code of contextmanager it does handle a lot of exceptions and makes it easier to create a context manager and do away with the __enter__ and __exit__ protocol. from contextlib import contextmanager import time @contextmanager def timer (): start = time . time_ns () yield # same concept it should return something which goes into the with \"as\" variable. end = time . time_ns () print ( f \"Time taken in ns = { end - start } \" ) with timer () as t : time . sleep ( . 1 ) for i in range ( 10 ): print ( i ) 0 1 2 3 4 5 6 7 8 9 Time taken in ns = 100953200 @contextmanager def file_manager ( f ): file = open ( f ) yield file # same concept it should return something which goes into the with \"as\" variable. file . close () print ( \"File closed\" ) with file_manager ( \"django.md\" ) as f : print ( f . readlines ()) [ '# Forms\\n' , '\\n' , '### Formsets\\n' , '\\n' , '\\n' , 'Create a simple model with few fields.\\n' , '\\n' , '```python\\n' , '# models.py\\n' , '\\n' , 'class Employee(models.Model):\\n' , '\\n' , ' name = models.CharField(max_length=40)\\n' , ' is_manager = models.BooleanField(default=False)\\n' , ' email = models.CharField(max_length = 100)\\n' , '\\n' , ' def __str__(self):\\n' , ' return self.name\\n' , '```\\n' , '\\n' , 'After creating a model, lets create a model form for that model. Also create the formset for that model and form as shown below.\\n' , '\\n' , '```python\\n' , '# forms.py\\n' , '\\n' , 'from django import forms\\n' , 'from .models import *\\n' , 'from django.forms import modelformset_factory\\n' , '\\n' , '\\n' , 'class EmployeeForm(forms.ModelForm):\\n' , ' email = forms.EmailField(disabled=True) # disable field\\n' , '\\n' , ' class Meta:\\n' , ' model = Employee\\n' , \" fields = ['email', 'name', 'is_manager']\\n\" , '\\n' , '\\n' , 'EmployeeFormSet = modelformset_factory(Employee, form=EmployeeForm, max_num=0)\\n' , '\\n' , '```\\n' , '\\n' , 'Use the formset created in the view.\\n' , '\\n' , '```python\\n' , '# views.py\\n' , '\\n' , 'from django.shortcuts import render\\n' , 'from .forms import *\\n' , 'from django.views import View\\n' , 'from .models import *\\n' , '\\n' , '\\n' , 'def index(request):\\n' , ' context = {}\\n' , \" if request.method == 'GET':\\n\" , ' \\n' , ' formset = EmployeeFormSet()\\n' , \" return render(request, 'tryformsets/index.html', {'formset' : formset})\\n\" , '\\n' , \" if request.method == 'POST':\\n\" , '\\n' , ' # formset = EmployeeFormSet(request.POST or None, request.FILES or None)\\n' , ' formset = EmployeeFormSet(request.POST)\\n' , ' if formset.is_valid():\\n' , ' formset.save()\\n' , ' formset = EmployeeFormSet()\\n' , \" return render(request, 'tryformsets/index.html', {'formset' : formset})\\n\" , '\\n' , '``` \\n' , '\\n' , 'Then create the template and use the formset. Make sure to include the `{{formset.management_form}}` else it gives error. For more information check the [link](https://docs.djangoproject.com/en/2.2/topics/forms/formsets/#understanding-the-managementform)\\n' , '\\n' , '```html\\n' , '<form method=\"POST\" action=\".\">{% csrf_token %}\\n' , '\\n' , ' {{ formset.management_form }}\\n' , '\\n' , ' <table class=\"table\">\\n' , ' <thead>\\n' , ' <tr>\\n' , ' <th>Name</th>\\n' , ' <th>Item Name</th>\\n' , ' <th>Item Price</th>\\n' , ' </tr>\\n' , ' </thead>\\n' , ' {% for form in formset %}\\n' , '\\n' , ' <tbody>\\n' , ' {{ form.id }}\\n' , ' <tr>\\n' , ' <td>{{ form.name }}</td>\\n' , ' <td>{{ form.email }}</td>\\n' , ' <td>{{ form.is_manager }}</td>\\n' , ' </tr>\\n' , ' {% endfor %}\\n' , ' </tbody>\\n' , '\\n' , ' </table>\\n' , ' <button type=\"submit\">Submit</button>\\n' , '</form>\\n' , '```\\n' , '\\n' , '<hr>\\n' , '\\n' , '## Working with DB\\n' , '\\n' , '### Bulk Update\\n' , '\\n' , '```python\\n' , '# Coverage is a model which we need to update in bulk.\\n' , '# Here we are trying to update the rows with pk = 33, 34 and 35 with different values.\\n' , '>>> objs = []\\n' , '>>> objs.append(Coverage.objects.get(pk = 33)) \\n' , '>>> \\n' , '>>> objs.append(Coverage.objects.get(pk = 34)) \\n' , '>>> objs.append(Coverage.objects.get(pk = 35)) \\n' , '>>> \\n' , '>>> objs[1].coverage_needed = True \\n' , '>>> \\n' , '>>> objs[1].coverage_needed \\n' , 'True \\n' , '>>> \\n' , '>>> objs[2].coverage_needed \\n' , 'False \\n' , '>>> \\n' , '>>> objs[0].unavailable = True \\n' , '>>> \\n' , '>>> objs[2].supply_called = True \\n' , '>>> \\n' , \">>> Coverage.objects.bulk_update(objs, ['coverage_needed', 'supply_called', 'unavailable'])\\n\" , '```\\n' , '\\n' , '## Links\\n' , '\\n' , '### Forms\\n' , '\\n' , '- [DataTable Editable](https://stackoverflow.com/questions/56290703/django-edit-html-table-rows-and-update-database)\\n' , '- [Django Forms - HTML](https://stackoverflow.com/questions/39183155/django-with-html-forms)\\n' , '- [Django Forms - Rendering each form element manually for better styling.](https://simpleisbetterthancomplex.com/article/2017/08/19/how-to-render-django-form-manually.html)\\n' , '\\n' , '\\n' , '\\n' ] File closed There could be certain cases where you need to handle and tackle excpetions manually or have some explicit requirements which are better suited in class then you can go ahead and use the traditional way.","title":"Context Managers"},{"location":"Python/Context%20Managers/#context-managers","text":"The with statement sets up a temporary context and reliably tears it down. The with statement was designed to simplify the try/finally pattern which gaureentees even if the block is aborted even because of an exception the finally clause will release the critical resource. The context manager works with __enter__ and __exit__ protocol. Usage The context manager should be used for common setup and tear down code, which is not just applicable for files. There can be various uses for it. # Sample context manager for file. class File : def __init__ ( self , file , method ): self . file = open ( file , method ) def __enter__ ( self ): return self . file # returns the object which will sit in \"f\" --> with File(abc.txt, w) as \"f\" def __exit__ ( self , exception_type , exception_value , traceback ): # The exit must take in these 4 params else it raises exception self . file . close () The context manager which you see above provides a pattern of __enter__ and __exit__ . However if you see there is no exception which is handled here. One way is to handle exceptionions manually... # Another example of a context manager. import time class Timer : def __enter__ ( self ): self . start = time . time_ns () return self def __exit__ ( self , exception_type , exception_value , traceback ): self . end = time . time_ns () self . processing_time = self . end - self . start with Timer () as t : time . sleep ( . 1 ) for i in range ( 10 ): print ( i ) t . processing_time 0 1 2 3 4 5 6 7 8 9 100219600","title":"Context Managers"},{"location":"Python/Context%20Managers/#contextmanager","text":"Handling all sorts of exceptions can be difficult so python has provided with a decorator that turns a generator function into a context manager. Because think of it __enter__ is starting something and pausing and then __exit__ takes over when we have to tear it down or finish the work which is exactly what a generator does it yeilds and waits. You can use the contextlib library to import the contextmanager decorator to turn your generator into a context manager. Go and check out the code of contextmanager it does handle a lot of exceptions and makes it easier to create a context manager and do away with the __enter__ and __exit__ protocol. from contextlib import contextmanager import time @contextmanager def timer (): start = time . time_ns () yield # same concept it should return something which goes into the with \"as\" variable. end = time . time_ns () print ( f \"Time taken in ns = { end - start } \" ) with timer () as t : time . sleep ( . 1 ) for i in range ( 10 ): print ( i ) 0 1 2 3 4 5 6 7 8 9 Time taken in ns = 100953200 @contextmanager def file_manager ( f ): file = open ( f ) yield file # same concept it should return something which goes into the with \"as\" variable. file . close () print ( \"File closed\" ) with file_manager ( \"django.md\" ) as f : print ( f . readlines ()) [ '# Forms\\n' , '\\n' , '### Formsets\\n' , '\\n' , '\\n' , 'Create a simple model with few fields.\\n' , '\\n' , '```python\\n' , '# models.py\\n' , '\\n' , 'class Employee(models.Model):\\n' , '\\n' , ' name = models.CharField(max_length=40)\\n' , ' is_manager = models.BooleanField(default=False)\\n' , ' email = models.CharField(max_length = 100)\\n' , '\\n' , ' def __str__(self):\\n' , ' return self.name\\n' , '```\\n' , '\\n' , 'After creating a model, lets create a model form for that model. Also create the formset for that model and form as shown below.\\n' , '\\n' , '```python\\n' , '# forms.py\\n' , '\\n' , 'from django import forms\\n' , 'from .models import *\\n' , 'from django.forms import modelformset_factory\\n' , '\\n' , '\\n' , 'class EmployeeForm(forms.ModelForm):\\n' , ' email = forms.EmailField(disabled=True) # disable field\\n' , '\\n' , ' class Meta:\\n' , ' model = Employee\\n' , \" fields = ['email', 'name', 'is_manager']\\n\" , '\\n' , '\\n' , 'EmployeeFormSet = modelformset_factory(Employee, form=EmployeeForm, max_num=0)\\n' , '\\n' , '```\\n' , '\\n' , 'Use the formset created in the view.\\n' , '\\n' , '```python\\n' , '# views.py\\n' , '\\n' , 'from django.shortcuts import render\\n' , 'from .forms import *\\n' , 'from django.views import View\\n' , 'from .models import *\\n' , '\\n' , '\\n' , 'def index(request):\\n' , ' context = {}\\n' , \" if request.method == 'GET':\\n\" , ' \\n' , ' formset = EmployeeFormSet()\\n' , \" return render(request, 'tryformsets/index.html', {'formset' : formset})\\n\" , '\\n' , \" if request.method == 'POST':\\n\" , '\\n' , ' # formset = EmployeeFormSet(request.POST or None, request.FILES or None)\\n' , ' formset = EmployeeFormSet(request.POST)\\n' , ' if formset.is_valid():\\n' , ' formset.save()\\n' , ' formset = EmployeeFormSet()\\n' , \" return render(request, 'tryformsets/index.html', {'formset' : formset})\\n\" , '\\n' , '``` \\n' , '\\n' , 'Then create the template and use the formset. Make sure to include the `{{formset.management_form}}` else it gives error. For more information check the [link](https://docs.djangoproject.com/en/2.2/topics/forms/formsets/#understanding-the-managementform)\\n' , '\\n' , '```html\\n' , '<form method=\"POST\" action=\".\">{% csrf_token %}\\n' , '\\n' , ' {{ formset.management_form }}\\n' , '\\n' , ' <table class=\"table\">\\n' , ' <thead>\\n' , ' <tr>\\n' , ' <th>Name</th>\\n' , ' <th>Item Name</th>\\n' , ' <th>Item Price</th>\\n' , ' </tr>\\n' , ' </thead>\\n' , ' {% for form in formset %}\\n' , '\\n' , ' <tbody>\\n' , ' {{ form.id }}\\n' , ' <tr>\\n' , ' <td>{{ form.name }}</td>\\n' , ' <td>{{ form.email }}</td>\\n' , ' <td>{{ form.is_manager }}</td>\\n' , ' </tr>\\n' , ' {% endfor %}\\n' , ' </tbody>\\n' , '\\n' , ' </table>\\n' , ' <button type=\"submit\">Submit</button>\\n' , '</form>\\n' , '```\\n' , '\\n' , '<hr>\\n' , '\\n' , '## Working with DB\\n' , '\\n' , '### Bulk Update\\n' , '\\n' , '```python\\n' , '# Coverage is a model which we need to update in bulk.\\n' , '# Here we are trying to update the rows with pk = 33, 34 and 35 with different values.\\n' , '>>> objs = []\\n' , '>>> objs.append(Coverage.objects.get(pk = 33)) \\n' , '>>> \\n' , '>>> objs.append(Coverage.objects.get(pk = 34)) \\n' , '>>> objs.append(Coverage.objects.get(pk = 35)) \\n' , '>>> \\n' , '>>> objs[1].coverage_needed = True \\n' , '>>> \\n' , '>>> objs[1].coverage_needed \\n' , 'True \\n' , '>>> \\n' , '>>> objs[2].coverage_needed \\n' , 'False \\n' , '>>> \\n' , '>>> objs[0].unavailable = True \\n' , '>>> \\n' , '>>> objs[2].supply_called = True \\n' , '>>> \\n' , \">>> Coverage.objects.bulk_update(objs, ['coverage_needed', 'supply_called', 'unavailable'])\\n\" , '```\\n' , '\\n' , '## Links\\n' , '\\n' , '### Forms\\n' , '\\n' , '- [DataTable Editable](https://stackoverflow.com/questions/56290703/django-edit-html-table-rows-and-update-database)\\n' , '- [Django Forms - HTML](https://stackoverflow.com/questions/39183155/django-with-html-forms)\\n' , '- [Django Forms - Rendering each form element manually for better styling.](https://simpleisbetterthancomplex.com/article/2017/08/19/how-to-render-django-form-manually.html)\\n' , '\\n' , '\\n' , '\\n' ] File closed There could be certain cases where you need to handle and tackle excpetions manually or have some explicit requirements which are better suited in class then you can go ahead and use the traditional way.","title":"contextmanager"},{"location":"Python/Data%20Model%20Methods/","text":"Data Model Methods There are many data model or special methods in Python, a handfull of them are covered below. For details of other methods please check out this ref link from python docs __call__ Allows the instance to be called with the parentheses syntax: instance() class A : def __init__ ( self ): print ( \"In Init\" ) def __call__ ( self , * args , ** kwargs ): print ( \"In Call\" ) a = A () # In Init a () # In Call --> instance() --> invokes the __call__ function. There can be various uses as shown below. class TriangleArea : def __call__ ( self , a , b , c ): p = ( a + b + c ) / 2 result = ( p * ( p - a ) * ( p - b ) * ( p - c )) ** 0.5 return result area = TriangleArea () print ( area ( 3 , 4 , 5 )) __getitem__ The __getitem__ function if implemented on a class will support the following - object[0] - get by position - you can use the standard random.choice() to get any from the object. - slicing object[x:y] # __getitem__ example import collections from random import choice Card = collections . namedtuple ( 'Card' , ( 'rank' , 'suit' )) class FrenchDeck : ranks = [ str ( n ) for n in range ( 2 , 11 )] + list ( 'JQKA' ) suits = \"spades diamonds clubs hearts\" . split ( \" \" ) def __init__ ( self ): self . _cards = [ Card ( rank , suit ) for suit in self . suits for rank in self . ranks ] def __len__ ( self ): return len ( self . _cards ) def __getitem__ ( self , position ): return self . _cards [ position ] if __name__ == \"__main__\" : deck = FrenchDeck () print ( len ( deck )) print ( deck [ 4 ]) print ( deck [ 4 : 10 ]) print ( choice ( deck )) print ( Card ( 'Q' , 'hearts' ) in deck ) Slicing can return only the array but if you want slicing to return a new object of that class but only sliced entries then you have to implement something like this where components are arrays. def __len__ ( self ): return len ( self . _components ) def __getitem__ ( self , index ): cls = type ( self ) if isinstance ( index , slice ): return cls ( self . _components [ index ]) elif isinstance ( index , numbers . Integral ): return self . _components [ index ] else : msg = ' {cls.__name__} indices must be integers' raise TypeError ( msg . format ( cls = cls )) If there is no __iter__ python falls back on __getitem__ for iteration. If there is no __contains__ python falls back on __getitem__ to check for x in y . __repr__ Provides the string representation of the object. The representation should match the source code needed to recreating this instance. __str__ __str__ is called by the str() and implicitly used by the print statement. This should return a string suitable for end users. The differnce between __str__ and __repr__ is illustrated by this example below. If you only implement one of these special methods, choose __repr__ , because when no custom __str__ is available, Python will call __repr__ as a fallback. class Point : def __init__ ( self , x , y ): self . x = x self . y = y def __repr__ ( self ): return 'Point( {} , {} )' . format ( self . x , self . y ) def __str__ ( self ): return 'Point at {} and {} ' . format ( self . x , self . y ) >>> p1 = Point ( 2 , 10 ) >>> p1 Point ( 2 , 10 ) >>> print ( p1 ) Point at 2 and 10 __abs__ gives absolute value __add__ Addition of 2 objs __mul__ Multiplication of 2 objs __bool__ Python will accept any object in boolean context because it calls __bool__ to determine truthy or falsy __del__ Python will call the __del__ method (if defined) on an object before destroying it. (When the reference count reaches 0). This is for Cpython which uses the reference counting and not all implementation of python e.g. Pypy which uses garbage collection (this del ) may not be called immidiately. __format__ With datetime you can work with printing specific formats like >>> format(now, '%H:%M:%S') and with float values you can use as below. pi = 22 / 7 print ( f ' { pi : .4f } ' ) The __format__ function lets you define your own format for your object. __hash__ Make your objects hashable. Check example from fluent python __getattr__ __getattr__ allows you to specify custom attributes on your object which can get values of your elements in the class. e.g. n-d vector may choose to implement x, y, z e.g. v.x v.y and v.z for first few dimensions of it... __setattr__ along with getattr you may need to implement setattr in certain scenarios. super().__setattr__(name, value) You can also use setattr to restrict setting any other/or any specific attributes on that class as shown below. # cannot set any attribute on the class. class X : def __setattr__ ( self , name , val ): msg = \"Cannot add any other attributes\" raise Exception __iter__ __iter__ is a generator function which, when called, builds a generator object that implements the iterator interface. __hash__ provides the hash of an object.","title":"Data Model Methods"},{"location":"Python/Data%20Model%20Methods/#data-model-methods","text":"There are many data model or special methods in Python, a handfull of them are covered below. For details of other methods please check out this ref link from python docs","title":"Data Model Methods"},{"location":"Python/Data%20Model%20Methods/#__call__","text":"Allows the instance to be called with the parentheses syntax: instance() class A : def __init__ ( self ): print ( \"In Init\" ) def __call__ ( self , * args , ** kwargs ): print ( \"In Call\" ) a = A () # In Init a () # In Call --> instance() --> invokes the __call__ function. There can be various uses as shown below. class TriangleArea : def __call__ ( self , a , b , c ): p = ( a + b + c ) / 2 result = ( p * ( p - a ) * ( p - b ) * ( p - c )) ** 0.5 return result area = TriangleArea () print ( area ( 3 , 4 , 5 ))","title":"__call__"},{"location":"Python/Data%20Model%20Methods/#__getitem__","text":"The __getitem__ function if implemented on a class will support the following - object[0] - get by position - you can use the standard random.choice() to get any from the object. - slicing object[x:y] # __getitem__ example import collections from random import choice Card = collections . namedtuple ( 'Card' , ( 'rank' , 'suit' )) class FrenchDeck : ranks = [ str ( n ) for n in range ( 2 , 11 )] + list ( 'JQKA' ) suits = \"spades diamonds clubs hearts\" . split ( \" \" ) def __init__ ( self ): self . _cards = [ Card ( rank , suit ) for suit in self . suits for rank in self . ranks ] def __len__ ( self ): return len ( self . _cards ) def __getitem__ ( self , position ): return self . _cards [ position ] if __name__ == \"__main__\" : deck = FrenchDeck () print ( len ( deck )) print ( deck [ 4 ]) print ( deck [ 4 : 10 ]) print ( choice ( deck )) print ( Card ( 'Q' , 'hearts' ) in deck ) Slicing can return only the array but if you want slicing to return a new object of that class but only sliced entries then you have to implement something like this where components are arrays. def __len__ ( self ): return len ( self . _components ) def __getitem__ ( self , index ): cls = type ( self ) if isinstance ( index , slice ): return cls ( self . _components [ index ]) elif isinstance ( index , numbers . Integral ): return self . _components [ index ] else : msg = ' {cls.__name__} indices must be integers' raise TypeError ( msg . format ( cls = cls )) If there is no __iter__ python falls back on __getitem__ for iteration. If there is no __contains__ python falls back on __getitem__ to check for x in y .","title":"__getitem__"},{"location":"Python/Data%20Model%20Methods/#__repr__","text":"Provides the string representation of the object. The representation should match the source code needed to recreating this instance.","title":"__repr__"},{"location":"Python/Data%20Model%20Methods/#__str__","text":"__str__ is called by the str() and implicitly used by the print statement. This should return a string suitable for end users. The differnce between __str__ and __repr__ is illustrated by this example below. If you only implement one of these special methods, choose __repr__ , because when no custom __str__ is available, Python will call __repr__ as a fallback. class Point : def __init__ ( self , x , y ): self . x = x self . y = y def __repr__ ( self ): return 'Point( {} , {} )' . format ( self . x , self . y ) def __str__ ( self ): return 'Point at {} and {} ' . format ( self . x , self . y ) >>> p1 = Point ( 2 , 10 ) >>> p1 Point ( 2 , 10 ) >>> print ( p1 ) Point at 2 and 10","title":"__str__"},{"location":"Python/Data%20Model%20Methods/#__abs__","text":"gives absolute value","title":"__abs__"},{"location":"Python/Data%20Model%20Methods/#__add__","text":"Addition of 2 objs","title":"__add__"},{"location":"Python/Data%20Model%20Methods/#__mul__","text":"Multiplication of 2 objs","title":"__mul__"},{"location":"Python/Data%20Model%20Methods/#__bool__","text":"Python will accept any object in boolean context because it calls __bool__ to determine truthy or falsy","title":"__bool__"},{"location":"Python/Data%20Model%20Methods/#__del__","text":"Python will call the __del__ method (if defined) on an object before destroying it. (When the reference count reaches 0). This is for Cpython which uses the reference counting and not all implementation of python e.g. Pypy which uses garbage collection (this del ) may not be called immidiately.","title":"__del__"},{"location":"Python/Data%20Model%20Methods/#__format__","text":"With datetime you can work with printing specific formats like >>> format(now, '%H:%M:%S') and with float values you can use as below. pi = 22 / 7 print ( f ' { pi : .4f } ' ) The __format__ function lets you define your own format for your object.","title":"__format__"},{"location":"Python/Data%20Model%20Methods/#__hash__","text":"Make your objects hashable. Check example from fluent python","title":"__hash__"},{"location":"Python/Data%20Model%20Methods/#__getattr__","text":"__getattr__ allows you to specify custom attributes on your object which can get values of your elements in the class. e.g. n-d vector may choose to implement x, y, z e.g. v.x v.y and v.z for first few dimensions of it...","title":"__getattr__"},{"location":"Python/Data%20Model%20Methods/#__setattr__","text":"along with getattr you may need to implement setattr in certain scenarios. super().__setattr__(name, value) You can also use setattr to restrict setting any other/or any specific attributes on that class as shown below. # cannot set any attribute on the class. class X : def __setattr__ ( self , name , val ): msg = \"Cannot add any other attributes\" raise Exception","title":"__setattr__"},{"location":"Python/Data%20Model%20Methods/#__iter__","text":"__iter__ is a generator function which, when called, builds a generator object that implements the iterator interface.","title":"__iter__"},{"location":"Python/Data%20Model%20Methods/#__hash___1","text":"provides the hash of an object.","title":"__hash__"},{"location":"Python/DataClasses/","text":"DataClasses Data classes are a new addition to Python 3.7 The dataclass decorator auto writes the code for __init__() , __repr__() and __eq__() dunder methods. It also can make the instance frozen by taking in an argument frozen=True in the decorator as an argument, thus the instance can be hashable. from dataclasses import dataclass @dataclass ( frozen = True ) class FrozenVector : x : int y : int","title":"DataClasses"},{"location":"Python/DataClasses/#dataclasses","text":"Data classes are a new addition to Python 3.7 The dataclass decorator auto writes the code for __init__() , __repr__() and __eq__() dunder methods. It also can make the instance frozen by taking in an argument frozen=True in the decorator as an argument, thus the instance can be hashable. from dataclasses import dataclass @dataclass ( frozen = True ) class FrozenVector : x : int y : int","title":"DataClasses"},{"location":"Python/Decorators/","text":"Decorators def upper_case ( function ): def wrapper (): func = function () uppercase = func . upper () return uppercase return wrapper def get_len ( function ): def wrapper (): return len ( function ()) return wrapper @get_len @upper_case def say_hi (): return 'hi hello' print ( say_hi ()) Multiple decorators are applied from bottom to top. from time import time , sleep def timer ( function ): def wrapper ( * a , ** kw ): before = time () answer = function ( * a , ** kw ) sleep ( . 00005 ) after = time () return after - before return wrapper @timer def adder ( x , y ): return x + y # adder = timer(adder) x = adder ( 10 , 20 ) print ( x ) When are decorators executed ? A key feature of decorators is that they run right after the decorated function is defined. That is usually at import time. Decorators in standard library property classmethod staticmethod functools.lru~cache~ functools.singledispatch Stacked decorators Parameterized decorators \"\"\" Simple decorator. \"\"\" import functools def funcA (): print ( \"In func A\" ) def decoratorA ( func ): def wrapper (): print ( \"In wrapper\" ) return func () return wrapper funcA = decoratorA ( funcA ) @decoratorA def funcB (): print ( \"In funcB\" ) # ------------------------------------------------- # Doing something in the wrapper. import time def timerA ( func ): def wrapper (): before = time . time () time . sleep ( . 0001 ) result = func () after = time . time () print ( f 'Time taken is { after - before } ' ) return result return wrapper @timerA def funcC (): print ( \"Hello world\" ) # ------------------------------------------------ # What if the function accepts arguments. def timerB ( func ): @functools . wraps ( func ) def wrapper ( * args , ** kwargs ): before = time . perf_counter_ns () result = func ( * args , ** kwargs ) after = time . perf_counter_ns () print ( f 'Time taken is { after - before } ' ) return result return wrapper @timerB def funcD ( a , b ): return a * b funcD ( 10 , 12312312213321 ) # ------------------------------------------------ # Decorator with arguments and function with arguments def repeat ( num_times = 0 ): def decorator_repeat ( func ): @functools . wraps ( func ) def wrapper ( * args , ** kwargs ): for i in range ( num_times ): func ( * args , ** kwargs ) return wrapper return decorator_repeat @repeat ( num_times = 4 ) def add ( x , y ): print ( x + y ) # ----------------------------------------------- # Using class as decorators. class Decorator : def __init__ ( self , func ): self . func = func def __call__ ( self , * args , ** kwargs ): print ( \"Before Function Call\" ) result = self . func ( * args , ** kwargs ) print ( result ) print ( \"After function call\" ) return result @Decorator def sub ( x , y ): return x - y \"\"\" Where can they be used in? - Timer functions - Logging functions - Plugin system - Authentication - is user logged in? - Singleton \"\"\" \"\"\" You can write class decorators as well i.e. which decorate the class and not functions e.g. check out the dataclass - Decorators can be stacked on top of each other. \"\"\" # In some cases it may be usefull to keep track of the state in a decorator. # In the example below we add num_calls as a variable of a function itself. def count_calls ( func ): @functools . wraps ( func ) def wrapper_count_calls ( * args , ** kwargs ): wrapper_count_calls . num_calls += 1 # same as wrapper_count_calls.variable print ( f \"Call { wrapper_count_calls . num_calls } of { func . __name__ !r} \" ) return func ( * args , ** kwargs ) wrapper_count_calls . num_calls = 0 return wrapper_count_calls @count_calls def x (): pass @count_calls def y (): pass # Will track the state for x and y separately. x () x () y () y () Common usages Argument Checking Caching Proxy Context Provider Debugging","title":"Decorators"},{"location":"Python/Decorators/#decorators","text":"def upper_case ( function ): def wrapper (): func = function () uppercase = func . upper () return uppercase return wrapper def get_len ( function ): def wrapper (): return len ( function ()) return wrapper @get_len @upper_case def say_hi (): return 'hi hello' print ( say_hi ()) Multiple decorators are applied from bottom to top. from time import time , sleep def timer ( function ): def wrapper ( * a , ** kw ): before = time () answer = function ( * a , ** kw ) sleep ( . 00005 ) after = time () return after - before return wrapper @timer def adder ( x , y ): return x + y # adder = timer(adder) x = adder ( 10 , 20 ) print ( x )","title":"Decorators"},{"location":"Python/Decorators/#when-are-decorators-executed","text":"A key feature of decorators is that they run right after the decorated function is defined. That is usually at import time.","title":"When are decorators executed ?"},{"location":"Python/Decorators/#decorators-in-standard-library","text":"property classmethod staticmethod functools.lru~cache~ functools.singledispatch","title":"Decorators in standard library"},{"location":"Python/Decorators/#stacked-decorators","text":"","title":"Stacked decorators"},{"location":"Python/Decorators/#parameterized-decorators","text":"\"\"\" Simple decorator. \"\"\" import functools def funcA (): print ( \"In func A\" ) def decoratorA ( func ): def wrapper (): print ( \"In wrapper\" ) return func () return wrapper funcA = decoratorA ( funcA ) @decoratorA def funcB (): print ( \"In funcB\" ) # ------------------------------------------------- # Doing something in the wrapper. import time def timerA ( func ): def wrapper (): before = time . time () time . sleep ( . 0001 ) result = func () after = time . time () print ( f 'Time taken is { after - before } ' ) return result return wrapper @timerA def funcC (): print ( \"Hello world\" ) # ------------------------------------------------ # What if the function accepts arguments. def timerB ( func ): @functools . wraps ( func ) def wrapper ( * args , ** kwargs ): before = time . perf_counter_ns () result = func ( * args , ** kwargs ) after = time . perf_counter_ns () print ( f 'Time taken is { after - before } ' ) return result return wrapper @timerB def funcD ( a , b ): return a * b funcD ( 10 , 12312312213321 ) # ------------------------------------------------ # Decorator with arguments and function with arguments def repeat ( num_times = 0 ): def decorator_repeat ( func ): @functools . wraps ( func ) def wrapper ( * args , ** kwargs ): for i in range ( num_times ): func ( * args , ** kwargs ) return wrapper return decorator_repeat @repeat ( num_times = 4 ) def add ( x , y ): print ( x + y ) # ----------------------------------------------- # Using class as decorators. class Decorator : def __init__ ( self , func ): self . func = func def __call__ ( self , * args , ** kwargs ): print ( \"Before Function Call\" ) result = self . func ( * args , ** kwargs ) print ( result ) print ( \"After function call\" ) return result @Decorator def sub ( x , y ): return x - y \"\"\" Where can they be used in? - Timer functions - Logging functions - Plugin system - Authentication - is user logged in? - Singleton \"\"\" \"\"\" You can write class decorators as well i.e. which decorate the class and not functions e.g. check out the dataclass - Decorators can be stacked on top of each other. \"\"\" # In some cases it may be usefull to keep track of the state in a decorator. # In the example below we add num_calls as a variable of a function itself. def count_calls ( func ): @functools . wraps ( func ) def wrapper_count_calls ( * args , ** kwargs ): wrapper_count_calls . num_calls += 1 # same as wrapper_count_calls.variable print ( f \"Call { wrapper_count_calls . num_calls } of { func . __name__ !r} \" ) return func ( * args , ** kwargs ) wrapper_count_calls . num_calls = 0 return wrapper_count_calls @count_calls def x (): pass @count_calls def y (): pass # Will track the state for x and y separately. x () x () y () y ()","title":"Parameterized decorators"},{"location":"Python/Decorators/#common-usages","text":"Argument Checking Caching Proxy Context Provider Debugging","title":"Common usages"},{"location":"Python/Dict/","text":"Dict They are highliy optimized. Are based on hash tables - Thus only objects which are hashable can be used as keys. The __hash__ data model method provides the hash value of an object. The __eq__ method is used to compare the keys, if two objects are equal they should have the same hash. Though rare but collisions of hashes are possible. Starting with Python 3.7 the order of insertion of item is preserved in the dict. >>> class Person : ... def __init__ ( self , name ): ... self . name = name ... ... ... >>> sam1 = Person ( \"sam\" ) >>> sam2 = Person ( \"sam\" ) >>> >>> hash ( sam1 ) == hash ( sam2 ) False >>> a = b = 1 >>> hash ( a ) == hash ( b ) True dict comprehension they are similar to list comprehension Variations of dict dict defualtdict ordereddict collections.ChainMap collections.Counter collections.UserDict types.mappingproxy collections.UserDict is designed to be subclassed, an example below. class StrKeyDict ( collections . UserDict ): def __missing__ ( self , key ): if isinstance ( key , str ): raise KeyError ( key ) return self [ str ( key )] def __contains__ ( self , key ): return str ( key ) in self . data def __setitem__ ( self , key , item ): self . data [ str ( key )] = item setdefualt() ``` {.python results=\"output\" exports=\"both\"} ``` {.python results=\"output\" exports=\"both\"} >>> d = {'Name': 'Zara', 'Age': 7} >>> d.setdefault(\"Sex\", None) >>> d {'Name': 'Zara', 'Age': 7, 'Sex': None} >>> d.setdefault(\"Name\", 'Peter') 'Zara' >>> d {'Name': 'Zara', 'Age': 7, 'Sex': None} Defaultdict Defualt dict is very similar to `dict` but you can pass a callable that is used to produce a default value whenever __getitem__ is passed a nonexistant key. >>> s = [( 'yellow' , 1 ), ( 'blue' , 2 ), ( 'yellow' , 3 ), ( 'blue' , 4 ), ( 'red' , 1 )] >>> d = defaultdict ( list ) >>> for k , v in s : ... d [ k ] . append ( v ) ... >>> sorted ( d . items ()) [( 'blue' , [ 2 , 4 ]), ( 'red' , [ 1 ]), ( 'yellow' , [ 1 , 3 ])] Dict\\'s are very fast but they do consume lots of memory becasue they need hash tables to operate which should be sparsely populated. If you are processing large amounts of data you should create them as rows of tuples/named tuples vs creating them as arrays of dicts (JSON) for space considerations. Check out the `_~slots~__` attribute which changes the storage of instance attributes from a dict to a tuple in each instance. The dict implementation is an example of trading space for time: dictionaries have significant memory overhead, but they provide fast access regardless of the size of the dictionary---as long as it fits in memory. Dictionaries are a keystone of Python. Beyond the basic dict, the standard library offers handy, ready-to-use specialized mappings like defaultdict, OrderedDict, ChainMap, and Counter, all defined in the collections module. The same module also provides the easy-to-extend UserDict class. Two powerful methods available in most mappings are setdefault and update. The setdefault method is used to update items holding mutable values, for example, in a dict of list values, to avoid redundant searches for the same key. The update method allows bulk insertion or overwriting of items from any other mapping, from iterables providing (key, value) pairs and from keyword arguments. Mapping constructors also use update internally, allowing instances to be initialized from mappings, iterables, or keyword arguments. A clever hook in the mapping API is the [[missing]{.underline}]{.underline} method, which lets you customize what happens when a key is not found. The collections.abc module provides the Mapping and MutableMapping abstract base classes for reference and type checking. The little-known MappingProxyType from the types module creates immutable mappings. There are also ABCs for Set and Mutable","title":"Dict"},{"location":"Python/Dict/#dict","text":"They are highliy optimized. Are based on hash tables - Thus only objects which are hashable can be used as keys. The __hash__ data model method provides the hash value of an object. The __eq__ method is used to compare the keys, if two objects are equal they should have the same hash. Though rare but collisions of hashes are possible. Starting with Python 3.7 the order of insertion of item is preserved in the dict. >>> class Person : ... def __init__ ( self , name ): ... self . name = name ... ... ... >>> sam1 = Person ( \"sam\" ) >>> sam2 = Person ( \"sam\" ) >>> >>> hash ( sam1 ) == hash ( sam2 ) False >>> a = b = 1 >>> hash ( a ) == hash ( b ) True dict comprehension they are similar to list comprehension Variations of dict dict defualtdict ordereddict collections.ChainMap collections.Counter collections.UserDict types.mappingproxy collections.UserDict is designed to be subclassed, an example below. class StrKeyDict ( collections . UserDict ): def __missing__ ( self , key ): if isinstance ( key , str ): raise KeyError ( key ) return self [ str ( key )] def __contains__ ( self , key ): return str ( key ) in self . data def __setitem__ ( self , key , item ): self . data [ str ( key )] = item setdefualt() ``` {.python results=\"output\" exports=\"both\"} ``` {.python results=\"output\" exports=\"both\"} >>> d = {'Name': 'Zara', 'Age': 7} >>> d.setdefault(\"Sex\", None) >>> d {'Name': 'Zara', 'Age': 7, 'Sex': None} >>> d.setdefault(\"Name\", 'Peter') 'Zara' >>> d {'Name': 'Zara', 'Age': 7, 'Sex': None}","title":"Dict"},{"location":"Python/Dict/#defaultdict","text":"Defualt dict is very similar to `dict` but you can pass a callable that is used to produce a default value whenever __getitem__ is passed a nonexistant key. >>> s = [( 'yellow' , 1 ), ( 'blue' , 2 ), ( 'yellow' , 3 ), ( 'blue' , 4 ), ( 'red' , 1 )] >>> d = defaultdict ( list ) >>> for k , v in s : ... d [ k ] . append ( v ) ... >>> sorted ( d . items ()) [( 'blue' , [ 2 , 4 ]), ( 'red' , [ 1 ]), ( 'yellow' , [ 1 , 3 ])] Dict\\'s are very fast but they do consume lots of memory becasue they need hash tables to operate which should be sparsely populated. If you are processing large amounts of data you should create them as rows of tuples/named tuples vs creating them as arrays of dicts (JSON) for space considerations. Check out the `_~slots~__` attribute which changes the storage of instance attributes from a dict to a tuple in each instance. The dict implementation is an example of trading space for time: dictionaries have significant memory overhead, but they provide fast access regardless of the size of the dictionary---as long as it fits in memory. Dictionaries are a keystone of Python. Beyond the basic dict, the standard library offers handy, ready-to-use specialized mappings like defaultdict, OrderedDict, ChainMap, and Counter, all defined in the collections module. The same module also provides the easy-to-extend UserDict class. Two powerful methods available in most mappings are setdefault and update. The setdefault method is used to update items holding mutable values, for example, in a dict of list values, to avoid redundant searches for the same key. The update method allows bulk insertion or overwriting of items from any other mapping, from iterables providing (key, value) pairs and from keyword arguments. Mapping constructors also use update internally, allowing instances to be initialized from mappings, iterables, or keyword arguments. A clever hook in the mapping API is the [[missing]{.underline}]{.underline} method, which lets you customize what happens when a key is not found. The collections.abc module provides the Mapping and MutableMapping abstract base classes for reference and type checking. The little-known MappingProxyType from the types module creates immutable mappings. There are also ABCs for Set and Mutable","title":"Defaultdict"},{"location":"Python/Disassembler%20for%20Python%20bytecode/","text":"Disassembler for Python bytecode >>> import dis >>> dis . dis ( 's[a] += b' ) 1 0 LOAD_NAME 0 ( s ) 2 LOAD_NAME 1 ( a ) 4 DUP_TOP_TWO 6 BINARY_SUBSCR 8 LOAD_NAME 2 ( b ) 10 INPLACE_ADD 12 ROT_THREE 14 STORE_SUBSCR 16 LOAD_CONST 0 ( None ) 18 RETURN_VALUE","title":"Disassembler for Python bytecode"},{"location":"Python/Disassembler%20for%20Python%20bytecode/#disassembler-for-python-bytecode","text":">>> import dis >>> dis . dis ( 's[a] += b' ) 1 0 LOAD_NAME 0 ( s ) 2 LOAD_NAME 1 ( a ) 4 DUP_TOP_TWO 6 BINARY_SUBSCR 8 LOAD_NAME 2 ( b ) 10 INPLACE_ADD 12 ROT_THREE 14 STORE_SUBSCR 16 LOAD_CONST 0 ( None ) 18 RETURN_VALUE","title":"Disassembler for Python bytecode"},{"location":"Python/Enum/","text":"Enum Enum is supported by many languages such as C, C++, Java etc... Below is a very brief overview, there is a lot more going on with them so check out this link Creating an enum from enum import Enum class Weekday ( Enum ): MONDAY = 0 TUESDAY = 1 >>> print ( Weekday . MONDAY ) Weekday . MONDAY The enums are hashable so they can be used as dictionary keys. >>> print ( Weekday ( 1 )) Weeksay . TUESDAY You can use the decorator @enum.unique to ensure that enum values are unique.","title":"Enum"},{"location":"Python/Enum/#enum","text":"Enum is supported by many languages such as C, C++, Java etc... Below is a very brief overview, there is a lot more going on with them so check out this link Creating an enum from enum import Enum class Weekday ( Enum ): MONDAY = 0 TUESDAY = 1 >>> print ( Weekday . MONDAY ) Weekday . MONDAY The enums are hashable so they can be used as dictionary keys. >>> print ( Weekday ( 1 )) Weeksay . TUESDAY You can use the decorator @enum.unique to ensure that enum values are unique.","title":"Enum"},{"location":"Python/Exceptions/","text":"Exceptions Consider the below example, here if someone does not enter a number then the conversion to int will fail. while True : x = int ( input ( \"Please enter a number : \" )) break We can handle this exception as shown below. In the below example the user will be asked to enter the number again and again as long as he enters it because any non number will be handled by the exception. while True : try : x = int ( input ( \"Please enter a number : \" )) break except Exception : print ( \"Not a valid number, please try agian!\" ) There can be more than 1 except in a try clause, based on the exception, however only 1 is ever executed. Usually in the last except we emit the exception name(s) to act as a catch-all. for cls in [ B , C , D ]: try : raise cls () except ( RuntimeError , TypeError ): print ( \"D\" ) except C : print ( \"My own user defined exception\" ) except : # omit the exception type for catch-all print ( \"General Exception\" ) Try-except also has an optional else clause. It is useful for the code which must be excecuted if an exception is not raised. try : print ( 10 / 10 ) except Exception : print ( \"Exception raised\" ) else : print ( \"Else clause executed\" ) 1.0 Else clause executed The exception may also have an argument associated with it which we can access as below. try : 10 / 0 except Exception as e : print ( \"Exception : \" , e ) Exception : division by zero You can also raise your own exception as shown below. There are different types of excpetions, thus you can raise them as such raise ValueError , raise NameError('HiThere') etc... try : raise Exception ( 'foo' , 'bar' ) except Exception as e : print ( \"Exception\" , e ) Exception ( 'foo' , 'bar' ) User Defined Exceptions Programs may create their own exceptions by subclassing their Exception class directly or indirectly. Given below is an example adapted from standard python tutorial. In the exception module we have a base class Error which is subclassing Exception class. Then we have various granular classes which raise particular type of exceptions and take in various arguments for better handling or logging. class Error ( Exception ): \"\"\"Base class for exceptions in this module.\"\"\" pass class InputError ( Error ): \"\"\"Exception raised for errors in the input. Attributes: expression -- input expression in which the error occurred message -- explanation of the error \"\"\" def __init__ ( self , expression , message ): self . expression = expression self . message = message class TransitionError ( Error ): \"\"\"Raised when an operation attempts a state transition that's not allowed. Attributes: previous -- state at beginning of transition next -- attempted new state message -- explanation of why the specific transition is not allowed \"\"\" def __init__ ( self , previous , next , message ): self . previous = previous self . next = next self . message = message user_input = \"Incorrect Input\" try : if user_input != 0 : raise InputError ( \"Num Input\" , \"Incorrect Input Received\" ) except InputError as err : print ( \"Raised due to exception : \" , err . expression ) print ( \"Message : \" , err . message ) Raised due to exception : Num Input Message : Incorrect Input Received The try clause also has another optional clause which is finally . This is executed under all circumstances. Usually cleanup actions are performed under the finally clause.","title":"Exceptions"},{"location":"Python/Exceptions/#exceptions","text":"Consider the below example, here if someone does not enter a number then the conversion to int will fail. while True : x = int ( input ( \"Please enter a number : \" )) break We can handle this exception as shown below. In the below example the user will be asked to enter the number again and again as long as he enters it because any non number will be handled by the exception. while True : try : x = int ( input ( \"Please enter a number : \" )) break except Exception : print ( \"Not a valid number, please try agian!\" ) There can be more than 1 except in a try clause, based on the exception, however only 1 is ever executed. Usually in the last except we emit the exception name(s) to act as a catch-all. for cls in [ B , C , D ]: try : raise cls () except ( RuntimeError , TypeError ): print ( \"D\" ) except C : print ( \"My own user defined exception\" ) except : # omit the exception type for catch-all print ( \"General Exception\" ) Try-except also has an optional else clause. It is useful for the code which must be excecuted if an exception is not raised. try : print ( 10 / 10 ) except Exception : print ( \"Exception raised\" ) else : print ( \"Else clause executed\" ) 1.0 Else clause executed The exception may also have an argument associated with it which we can access as below. try : 10 / 0 except Exception as e : print ( \"Exception : \" , e ) Exception : division by zero You can also raise your own exception as shown below. There are different types of excpetions, thus you can raise them as such raise ValueError , raise NameError('HiThere') etc... try : raise Exception ( 'foo' , 'bar' ) except Exception as e : print ( \"Exception\" , e ) Exception ( 'foo' , 'bar' )","title":"Exceptions"},{"location":"Python/Exceptions/#user-defined-exceptions","text":"Programs may create their own exceptions by subclassing their Exception class directly or indirectly. Given below is an example adapted from standard python tutorial. In the exception module we have a base class Error which is subclassing Exception class. Then we have various granular classes which raise particular type of exceptions and take in various arguments for better handling or logging. class Error ( Exception ): \"\"\"Base class for exceptions in this module.\"\"\" pass class InputError ( Error ): \"\"\"Exception raised for errors in the input. Attributes: expression -- input expression in which the error occurred message -- explanation of the error \"\"\" def __init__ ( self , expression , message ): self . expression = expression self . message = message class TransitionError ( Error ): \"\"\"Raised when an operation attempts a state transition that's not allowed. Attributes: previous -- state at beginning of transition next -- attempted new state message -- explanation of why the specific transition is not allowed \"\"\" def __init__ ( self , previous , next , message ): self . previous = previous self . next = next self . message = message user_input = \"Incorrect Input\" try : if user_input != 0 : raise InputError ( \"Num Input\" , \"Incorrect Input Received\" ) except InputError as err : print ( \"Raised due to exception : \" , err . expression ) print ( \"Message : \" , err . message ) Raised due to exception : Num Input Message : Incorrect Input Received The try clause also has another optional clause which is finally . This is executed under all circumstances. Usually cleanup actions are performed under the finally clause.","title":"User Defined Exceptions"},{"location":"Python/Functions/","text":"Functions Attributes of a function You can check attributes of a function by using the dir function dir(func) . e.g. the __dict__ attribute is used to store the user attributes assigned to it. def x ( a = 2 ): a = 10 b = 20 >>> x . __dict__ {} Assigning arbitrary attributes to functions as shown below is not a very common practice in general. def x ( a = 2 ): a = 10 b = 20 >>> x . a = 10 >>> x . __dict__ { 'a' : 10 } Only positional arguments Python3 introduces only positional arguments for a function, you need to put a * as shown below. def main ( * , a , b ): # This function will only take 2 keyword arguments return a + b >>> main ( a = 10 , b = 20 ) 30 There can be various variations of it, e.g. =def main(a, *, b)= where a is positional and b is keyword. Note that keyword-only arguments do not need to have a default value: they can be mandatory. Inspect Module You can do a lot with inspect module to check the code of the function etc... e.g. checking the signature using inspect.signature() . Inspect module is used a lot when creating an ORM becuase when converting a dynamic class to SQL you need to look into the class or that instance of the class. Functional programming modules operator module functools.partials Closures A closure is a function with an extended scope that encompasses nonglobal variables referenced in the body of the function but not defined there. It does not matter whether the function is anonymous or not; what matters is that it can access nonglobal variables that are defined outside of its body. # make averager using closures. This can also be done using class and __call__ method. def averager (): series = [] def ave ( num ): series . append ( num ) return sum ( series ) / len ( series ) return ave Here you see that ave has closure over the series variable in the averager . >>> a = averager () >>> >>> a ( 10 ) 10.0 >>> a ( 1 ) 5.5 >>> a ( 123 ) 44.666666666666664 Lets inspect the a for variables and free variables (which are not bound to the local scope) >>> a . __code__ < code object ave at 0x000001D277E3C660 , file \"main.py\" , line 4 > >>> a . __code__ . co_varnames ( 'num' ,) >>> a . __code__ . co_freevars ( 'series' ,) >>> a . __closure__ ( < cell at 0x000001D27829CF48 : list object at 0x000001D2788111C8 > ,) The binding for series is kept in the closure attribute of the returned function. Each item in a.__closure__ corresponds to a name in a.__code__.co_free vars . These items are cells, and they have an attribute called cell~contents~ where the actual value can be found. >>> a . __closure__ ( < cell at 0x000001D27829CF48 : list object at 0x000001D2788111C8 > ,) >>> >>> a . __closure__ [ 0 ] < cell at 0x000001D27829CF48 : list object at 0x000001D2788111C8 > >>> a . __closure__ [ 0 ] . cell_contents [ 10 , 1 , 123 ] To summarize: a closure is a function that retains the bindings of the free variables that exist when the function is defined, so that they can be used later when the function is invoked and the defining scope is no longer available. nonlocal Consider the example below. def outer (): counter = 0 def inner ( num ): counter += num return counter return inner my_counter = outer () my_counter ( 10 ) When you run this python gives an error UnboundLocalError: local variable 'counter' referenced before assignment . The reason is that when you are trying to increment counter + num= you are treating it as a bounded variable and not as a free variable. So to use that (and because counter is not a global variable) you need to use the keyword nonlocal . def outer (): counter = 0 def inner ( num ): nonlocal counter counter += num return counter return inner my_counter = outer () my_counter ( 10 ) >>> my_counter ( 1 ) 11 >>> my_counter ( 12 ) 23","title":"Functions"},{"location":"Python/Functions/#functions","text":"","title":"Functions"},{"location":"Python/Functions/#attributes-of-a-function","text":"You can check attributes of a function by using the dir function dir(func) . e.g. the __dict__ attribute is used to store the user attributes assigned to it. def x ( a = 2 ): a = 10 b = 20 >>> x . __dict__ {} Assigning arbitrary attributes to functions as shown below is not a very common practice in general. def x ( a = 2 ): a = 10 b = 20 >>> x . a = 10 >>> x . __dict__ { 'a' : 10 }","title":"Attributes of a function"},{"location":"Python/Functions/#only-positional-arguments","text":"Python3 introduces only positional arguments for a function, you need to put a * as shown below. def main ( * , a , b ): # This function will only take 2 keyword arguments return a + b >>> main ( a = 10 , b = 20 ) 30 There can be various variations of it, e.g. =def main(a, *, b)= where a is positional and b is keyword. Note that keyword-only arguments do not need to have a default value: they can be mandatory.","title":"Only positional arguments"},{"location":"Python/Functions/#inspect-module","text":"You can do a lot with inspect module to check the code of the function etc... e.g. checking the signature using inspect.signature() . Inspect module is used a lot when creating an ORM becuase when converting a dynamic class to SQL you need to look into the class or that instance of the class.","title":"Inspect Module"},{"location":"Python/Functions/#functional-programming-modules","text":"operator module functools.partials","title":"Functional programming modules"},{"location":"Python/Functions/#closures","text":"A closure is a function with an extended scope that encompasses nonglobal variables referenced in the body of the function but not defined there. It does not matter whether the function is anonymous or not; what matters is that it can access nonglobal variables that are defined outside of its body. # make averager using closures. This can also be done using class and __call__ method. def averager (): series = [] def ave ( num ): series . append ( num ) return sum ( series ) / len ( series ) return ave Here you see that ave has closure over the series variable in the averager . >>> a = averager () >>> >>> a ( 10 ) 10.0 >>> a ( 1 ) 5.5 >>> a ( 123 ) 44.666666666666664 Lets inspect the a for variables and free variables (which are not bound to the local scope) >>> a . __code__ < code object ave at 0x000001D277E3C660 , file \"main.py\" , line 4 > >>> a . __code__ . co_varnames ( 'num' ,) >>> a . __code__ . co_freevars ( 'series' ,) >>> a . __closure__ ( < cell at 0x000001D27829CF48 : list object at 0x000001D2788111C8 > ,) The binding for series is kept in the closure attribute of the returned function. Each item in a.__closure__ corresponds to a name in a.__code__.co_free vars . These items are cells, and they have an attribute called cell~contents~ where the actual value can be found. >>> a . __closure__ ( < cell at 0x000001D27829CF48 : list object at 0x000001D2788111C8 > ,) >>> >>> a . __closure__ [ 0 ] < cell at 0x000001D27829CF48 : list object at 0x000001D2788111C8 > >>> a . __closure__ [ 0 ] . cell_contents [ 10 , 1 , 123 ] To summarize: a closure is a function that retains the bindings of the free variables that exist when the function is defined, so that they can be used later when the function is invoked and the defining scope is no longer available.","title":"Closures"},{"location":"Python/Functions/#nonlocal","text":"Consider the example below. def outer (): counter = 0 def inner ( num ): counter += num return counter return inner my_counter = outer () my_counter ( 10 ) When you run this python gives an error UnboundLocalError: local variable 'counter' referenced before assignment . The reason is that when you are trying to increment counter + num= you are treating it as a bounded variable and not as a free variable. So to use that (and because counter is not a global variable) you need to use the keyword nonlocal . def outer (): counter = 0 def inner ( num ): nonlocal counter counter += num return counter return inner my_counter = outer () my_counter ( 10 ) >>> my_counter ( 1 ) 11 >>> my_counter ( 12 ) 23","title":"nonlocal"},{"location":"Python/Generator/","text":"Generator Background of Iterators How re works import re w = re . compile ( '\\w+' ) w . findall ( \"This is word\" ) # ['This', 'is', 'word'] import re RE_WORD = re . compile ( '\\w+' ) # The below class implements the necessary methods for sequence. class Sentence : def __init__ ( self , text ): self . text = text self . word = RE_WORD . findall ( self . text ) def __getitem__ ( self , index ): return self . word [ index ] def __len__ ( self ): return len ( self . words ) s1 = Sentence ( \"The quick brown fox jumped over the lazy dog\" ) for word in s1 : print ( word ) The quick brown fox jumped over the lazy dog Why sequences are iterable even though we did not implement the __iter__ method ? Whenver we try to iterate over an object by using lets say for loop or any other method the python calls the iter(object) The built in iter funtion then checks the following : - Is __iter__ implement for that object? If no then it falls back on __getitem__ and creates an iterator on top of that starting form index 0 If that fails too raise TypeError Generally a good idea to implement __iter__ too. __getietm__ only works for backward compatability. What is iterable ? Anything which can be iterated on, which can be done in couple of ways Objects implementing __iter__ Objects which are sequences i.e. implementing __getitem__ What is iterator then ? Python obtains iterator from iterables.... iter ( s1 ) # This will give you an iterator. < iterator at 0x629e790 > Iterator Details The iterator should have the following methods implemented. __iter__ - Returns self; this allows iterators to be used where an iterable is expected, for example, in a for loop. __next__ - Returns the next available item and takes no arguments, raising StopIteration when there are no more items. Once all the items are exhausted you need to call iter(object) again. Converting our Sentence class into iterator by implementing the next and iter methods. However this is a bad idea. import re RE_WORD = re . compile ( '\\w+' ) # The below class implements the necessary methods for sequence. class Sentence : def __init__ ( self , text ): self . text = text self . word = RE_WORD . findall ( self . text ) self . index = 0 def __next__ ( self ): try : word = self . word [ self . index ] except : raise StopIteration () self . index += 1 return word def __iter__ ( self ): return self s = Sentence ( \"Hello World today is Monday\" ) iter1 = iter ( s ) iter2 = iter ( s ) print ( next ( iter1 )) print ( next ( iter2 )) # So we have a problem here, we cannot have 2 iterators on the s object individually. Hello World The best way to solve this would be to have 2 classes - our normal Sentence class --> will have the __iter__ method which will return a new instance of SentenceIterator class SentenceIterator(self.words) every time iter is called on the Sentence class thus making sure each iterator has independent execution. - Sentence iterator class --> will have the __iter__ method which will return self and the __next__ method for raising StopException and taking index into account. However this is a much long winded approach. Generators Check out Pep 255 What is a generator ? A python function which has yeild in its body is a generator function. (It will return a generator object). In other words, a generator function is a generator factory. A generator function builds a generator object and wraps the body of the function in it. Convert iter to a generator object. # A simple generator def my_generator (): yield 1 print ( \"printing --> 1\" ) yield 2 print ( \"printing --> 2\" ) yield 3 print ( \"printing --> 3\" ) return None # explicit next g1 = my_generator () print ( f \"Generator object --> { g1 } \" ) print ( next ( g1 )) print ( next ( g1 )) print ( next ( g1 )) Generator object --> < generator object my_generator at 0x00BD2970 > 1 printing --> 1 2 printing --> 2 3 # implicit next g1 = my_generator () print ( f \"Generator object --> { g1 } \" ) for x in g1 : print ( x ) print ( '--' ) Generator object --> < generator object my_generator at 0x00BD2B30 > 1 -- printing --> 1 2 -- printing --> 2 3 -- printing --> 3 Converting the Sentence to a generator function import re RE_WORD = re . compile ( '\\w+' ) class Sentence : def __init__ ( self , text ): self . text = text # you can also replace \"findall\" by \"finditer\" which is the lazy version of it. But then this statement goes in __iter__ self . word = RE_WORD . findall ( self . text ) self . index = 0 def __iter__ ( self ): for word in self . word : yield word s = Sentence ( \"Hello World today is Monday\" ) iter1 = iter ( s ) iter2 = iter ( s ) print ( next ( iter1 )) print ( next ( iter2 )) # We do not have that problem here, we can have 2 iterators on the s object individually. Hello Hello iter1 , iter2 # Check these out they are both independent objects thus they do not share the state. ( < generator object Sentence . __iter__ at 0x00BD2B70 > , < generator object Sentence . __iter__ at 0x00BD2830 > ) Examples Using a generator to capitalize sentence. def capitalize ( values ): print ( values ) for value in values : yield value . upper () print ( \"\" . join ( capitalize ( 'Hello Sir' ))) # HELLO SIR Sending data to generator using yield statement Generator Expression Its a lazy version of list comprehension. () instead of [] Generator expressions are syntactic sugar: they can always be replaced by generator functions, but sometimes are more convenient. On the other hand, generator functions are much more flexible: you can code complex logic with multiple statements, and can even use them as coroutines. import re RE_WORD = re . compile ( '\\w+' ) class Sentence : def __init__ ( self , text ): self . text = text def __iter__ ( self ): # match.group() --> Returns one or more subgroups of the match #If there is a single argument, the result is a single string; if there are multiple arguments, the result is a tuple with one item per argument. return ( word . group () for word in RE_WORD . finditer ( self . text )) # <-- This is a generator expression. s = Sentence ( \"The quick brown fox\" ) for word in s : print ( word ) The quick brown fox iter ( s ) # returns a generator. < generator object Sentence . __iter__ .< locals >.< genexpr > at 0x010C0330 > Built in Generators There are lots of built in generators in Python e.g. the itertools module provides some 19 generators to use e.g. =itertools.count= and itertools.takewhile . Check out the official documentation to explore more generators. # Itertools.count is a generator import itertools itertools . count ( 1 ) # Will run for ever count ( 1 ) # Using itertools.takewhile with itertools.count --> 2 generators. gen = itertools . takewhile ( lambda n : n < 5 , itertools . count ( 1 )) list ( gen ) [ 1 , 2 , 3 , 4 ] Yield From Introduced in Python 3.3 check out PEP380 for more information s = 'ABC' n = ( 1 , 2 , 3 ) def chain ( * iterables ): for i in iterables : yield from i c = chain ( s , n ) c < generator object chain at 0x010C0530 > list ( c ) [ 'A' , 'B' , 'C' , 1 , 2 , 3 ]","title":"Generator"},{"location":"Python/Generator/#generator","text":"","title":"Generator"},{"location":"Python/Generator/#background-of-iterators","text":"How re works import re w = re . compile ( '\\w+' ) w . findall ( \"This is word\" ) # ['This', 'is', 'word'] import re RE_WORD = re . compile ( '\\w+' ) # The below class implements the necessary methods for sequence. class Sentence : def __init__ ( self , text ): self . text = text self . word = RE_WORD . findall ( self . text ) def __getitem__ ( self , index ): return self . word [ index ] def __len__ ( self ): return len ( self . words ) s1 = Sentence ( \"The quick brown fox jumped over the lazy dog\" ) for word in s1 : print ( word ) The quick brown fox jumped over the lazy dog Why sequences are iterable even though we did not implement the __iter__ method ? Whenver we try to iterate over an object by using lets say for loop or any other method the python calls the iter(object) The built in iter funtion then checks the following : - Is __iter__ implement for that object? If no then it falls back on __getitem__ and creates an iterator on top of that starting form index 0 If that fails too raise TypeError Generally a good idea to implement __iter__ too. __getietm__ only works for backward compatability. What is iterable ? Anything which can be iterated on, which can be done in couple of ways Objects implementing __iter__ Objects which are sequences i.e. implementing __getitem__ What is iterator then ? Python obtains iterator from iterables.... iter ( s1 ) # This will give you an iterator. < iterator at 0x629e790 > Iterator Details The iterator should have the following methods implemented. __iter__ - Returns self; this allows iterators to be used where an iterable is expected, for example, in a for loop. __next__ - Returns the next available item and takes no arguments, raising StopIteration when there are no more items. Once all the items are exhausted you need to call iter(object) again. Converting our Sentence class into iterator by implementing the next and iter methods. However this is a bad idea. import re RE_WORD = re . compile ( '\\w+' ) # The below class implements the necessary methods for sequence. class Sentence : def __init__ ( self , text ): self . text = text self . word = RE_WORD . findall ( self . text ) self . index = 0 def __next__ ( self ): try : word = self . word [ self . index ] except : raise StopIteration () self . index += 1 return word def __iter__ ( self ): return self s = Sentence ( \"Hello World today is Monday\" ) iter1 = iter ( s ) iter2 = iter ( s ) print ( next ( iter1 )) print ( next ( iter2 )) # So we have a problem here, we cannot have 2 iterators on the s object individually. Hello World The best way to solve this would be to have 2 classes - our normal Sentence class --> will have the __iter__ method which will return a new instance of SentenceIterator class SentenceIterator(self.words) every time iter is called on the Sentence class thus making sure each iterator has independent execution. - Sentence iterator class --> will have the __iter__ method which will return self and the __next__ method for raising StopException and taking index into account. However this is a much long winded approach.","title":"Background of Iterators"},{"location":"Python/Generator/#generators","text":"Check out Pep 255 What is a generator ? A python function which has yeild in its body is a generator function. (It will return a generator object). In other words, a generator function is a generator factory. A generator function builds a generator object and wraps the body of the function in it. Convert iter to a generator object. # A simple generator def my_generator (): yield 1 print ( \"printing --> 1\" ) yield 2 print ( \"printing --> 2\" ) yield 3 print ( \"printing --> 3\" ) return None # explicit next g1 = my_generator () print ( f \"Generator object --> { g1 } \" ) print ( next ( g1 )) print ( next ( g1 )) print ( next ( g1 )) Generator object --> < generator object my_generator at 0x00BD2970 > 1 printing --> 1 2 printing --> 2 3 # implicit next g1 = my_generator () print ( f \"Generator object --> { g1 } \" ) for x in g1 : print ( x ) print ( '--' ) Generator object --> < generator object my_generator at 0x00BD2B30 > 1 -- printing --> 1 2 -- printing --> 2 3 -- printing --> 3 Converting the Sentence to a generator function import re RE_WORD = re . compile ( '\\w+' ) class Sentence : def __init__ ( self , text ): self . text = text # you can also replace \"findall\" by \"finditer\" which is the lazy version of it. But then this statement goes in __iter__ self . word = RE_WORD . findall ( self . text ) self . index = 0 def __iter__ ( self ): for word in self . word : yield word s = Sentence ( \"Hello World today is Monday\" ) iter1 = iter ( s ) iter2 = iter ( s ) print ( next ( iter1 )) print ( next ( iter2 )) # We do not have that problem here, we can have 2 iterators on the s object individually. Hello Hello iter1 , iter2 # Check these out they are both independent objects thus they do not share the state. ( < generator object Sentence . __iter__ at 0x00BD2B70 > , < generator object Sentence . __iter__ at 0x00BD2830 > )","title":"Generators"},{"location":"Python/Generator/#examples","text":"Using a generator to capitalize sentence. def capitalize ( values ): print ( values ) for value in values : yield value . upper () print ( \"\" . join ( capitalize ( 'Hello Sir' ))) # HELLO SIR","title":"Examples"},{"location":"Python/Generator/#sending-data-to-generator-using-yield-statement","text":"","title":"Sending data to generator using yield statement"},{"location":"Python/Generator/#generator-expression","text":"Its a lazy version of list comprehension. () instead of [] Generator expressions are syntactic sugar: they can always be replaced by generator functions, but sometimes are more convenient. On the other hand, generator functions are much more flexible: you can code complex logic with multiple statements, and can even use them as coroutines. import re RE_WORD = re . compile ( '\\w+' ) class Sentence : def __init__ ( self , text ): self . text = text def __iter__ ( self ): # match.group() --> Returns one or more subgroups of the match #If there is a single argument, the result is a single string; if there are multiple arguments, the result is a tuple with one item per argument. return ( word . group () for word in RE_WORD . finditer ( self . text )) # <-- This is a generator expression. s = Sentence ( \"The quick brown fox\" ) for word in s : print ( word ) The quick brown fox iter ( s ) # returns a generator. < generator object Sentence . __iter__ .< locals >.< genexpr > at 0x010C0330 > Built in Generators There are lots of built in generators in Python e.g. the itertools module provides some 19 generators to use e.g. =itertools.count= and itertools.takewhile . Check out the official documentation to explore more generators. # Itertools.count is a generator import itertools itertools . count ( 1 ) # Will run for ever count ( 1 ) # Using itertools.takewhile with itertools.count --> 2 generators. gen = itertools . takewhile ( lambda n : n < 5 , itertools . count ( 1 )) list ( gen ) [ 1 , 2 , 3 , 4 ]","title":"Generator Expression"},{"location":"Python/Generator/#yield-from","text":"Introduced in Python 3.3 check out PEP380 for more information s = 'ABC' n = ( 1 , 2 , 3 ) def chain ( * iterables ): for i in iterables : yield from i c = chain ( s , n ) c < generator object chain at 0x010C0530 > list ( c ) [ 'A' , 'B' , 'C' , 1 , 2 , 3 ]","title":"Yield From"},{"location":"Python/Isolating%20Envirounments%20in%20Python/","text":"Isolating Envirounments in Python Sometimes pip is not install on linux based system even if python is installed. In these cases you can install pip for python3 by using the following command. sudo apt install python3-pip Getting details of pip. pip3 show pip Installing system-wide packages directly from PyPI is not recommended, and should be avoided. The dependencies should be isolated. There are 2 types of isolations - Application level isolation - Venv - System level isolation - VMWare, virtualbox, docker Using venv Create a python envirounment using venv python3 -m venv ENV This will create a folder name ENV which will store the python envirounment. You can activate it using the following command. source ENV/bin/activate After this your prompt will change (ENV) \u279c ENV which means you have activate it. Once you install packages using pip3 in your local ENV you can see the packages installed using the following command. pip3 freeze To store the list of packages in a requirements.txt file so that the envirounment can be created again on another machine if required. Pipe the output of pip3 freeze into that text file. pip3 freeze > requirements.txt Your text file will look something like below stating each package installed on that env. Remember pip3 freeze does not know which package is being used or not. It just compiles a list of packages installed. click == 7 .1.2 Flask == 1 .1.2 itsdangerous == 1 .1.0 Jinja2 == 2 .11.2 MarkupSafe == 1 .1.1 Werkzeug == 1 .0.1 To create the same envirounment you can use the command. pip3 install -r requirements.txt You can also deactivate using that python implementation by using command pip3 freeze","title":"Isolating Envirounments in Python"},{"location":"Python/Isolating%20Envirounments%20in%20Python/#isolating-envirounments-in-python","text":"Sometimes pip is not install on linux based system even if python is installed. In these cases you can install pip for python3 by using the following command. sudo apt install python3-pip Getting details of pip. pip3 show pip Installing system-wide packages directly from PyPI is not recommended, and should be avoided. The dependencies should be isolated. There are 2 types of isolations - Application level isolation - Venv - System level isolation - VMWare, virtualbox, docker Using venv Create a python envirounment using venv python3 -m venv ENV This will create a folder name ENV which will store the python envirounment. You can activate it using the following command. source ENV/bin/activate After this your prompt will change (ENV) \u279c ENV which means you have activate it. Once you install packages using pip3 in your local ENV you can see the packages installed using the following command. pip3 freeze To store the list of packages in a requirements.txt file so that the envirounment can be created again on another machine if required. Pipe the output of pip3 freeze into that text file. pip3 freeze > requirements.txt Your text file will look something like below stating each package installed on that env. Remember pip3 freeze does not know which package is being used or not. It just compiles a list of packages installed. click == 7 .1.2 Flask == 1 .1.2 itsdangerous == 1 .1.0 Jinja2 == 2 .11.2 MarkupSafe == 1 .1.1 Werkzeug == 1 .0.1 To create the same envirounment you can use the command. pip3 install -r requirements.txt You can also deactivate using that python implementation by using command pip3 freeze","title":"Isolating Envirounments in Python"},{"location":"Python/List/","text":"List Lists in Python are contiguous arrays of references to other objects. The head of the list stores the pointer to this list and its length. When new elements are added to the list there may be a chance that the array of references needs to be reallocated (as the previous allocated memory is exhausted.) Slicing For slicing with [start:stop:increment] under the hood python calls seq.__getitem__(slice(start, stop, step)) By using the above knowledge we can define our own named slices and make the code more elegant. >>> data = \"\"\" 1010 CA Ontario 1020 US New York 1030 IN Delhi \"\"\" >>> ID = slice ( 0 , 4 ) >>> COUNTRY = slice ( 5 , 7 ) >>> >>> for row in data . split ( \" \\n \" ): print ( row [ ID ]) 1010 1020 1030 __getitem__ and __setitem__ The [] operator is handled by getitem and setitem. In other words, to evaluate a[i, j], Python calls a.__getitem__((i, j)) . Copy Copies are shallow by default. Example of shallow copy, even though l1 and l2 the id is different their inner list refers to the same list so appending 10 to l1 appends it to l2 as well. In [ 15 ]: l = [ 1 , [ 1 ]] In [ 16 ]: l1 = [ 1 , [ 1 ]] In [ 17 ]: l2 = list ( l1 ) In [ 18 ]: l1 [ 1 ] . append ( 10 ) In [ 19 ]: l1 Out [ 19 ]: [ 1 , [ 1 , 10 ]] In [ 20 ]: l2 Out [ 20 ]: [ 1 , [ 1 , 10 ]] In [ 21 ]: id ( l1 ) Out [ 21 ]: 2159380834888 In [ 22 ]: id ( l2 ) Out [ 22 ]: 2159379541128 You can use the following for copying - Shallow copy - copy.copy() - Deep copy - copy.deepcopy() In order for a class to define its own copy implementation, it can define special methods __copy__() and __deepcopy__() Tip - Never have mutable types as defualt parameters of your arguments. Starred Expression first , second , * rest = [ 1 , 2 , 3 , 4 , 5 ]","title":"List"},{"location":"Python/List/#list","text":"Lists in Python are contiguous arrays of references to other objects. The head of the list stores the pointer to this list and its length. When new elements are added to the list there may be a chance that the array of references needs to be reallocated (as the previous allocated memory is exhausted.)","title":"List"},{"location":"Python/List/#slicing","text":"For slicing with [start:stop:increment] under the hood python calls seq.__getitem__(slice(start, stop, step)) By using the above knowledge we can define our own named slices and make the code more elegant. >>> data = \"\"\" 1010 CA Ontario 1020 US New York 1030 IN Delhi \"\"\" >>> ID = slice ( 0 , 4 ) >>> COUNTRY = slice ( 5 , 7 ) >>> >>> for row in data . split ( \" \\n \" ): print ( row [ ID ]) 1010 1020 1030 __getitem__ and __setitem__ The [] operator is handled by getitem and setitem. In other words, to evaluate a[i, j], Python calls a.__getitem__((i, j)) .","title":"Slicing"},{"location":"Python/List/#copy","text":"Copies are shallow by default. Example of shallow copy, even though l1 and l2 the id is different their inner list refers to the same list so appending 10 to l1 appends it to l2 as well. In [ 15 ]: l = [ 1 , [ 1 ]] In [ 16 ]: l1 = [ 1 , [ 1 ]] In [ 17 ]: l2 = list ( l1 ) In [ 18 ]: l1 [ 1 ] . append ( 10 ) In [ 19 ]: l1 Out [ 19 ]: [ 1 , [ 1 , 10 ]] In [ 20 ]: l2 Out [ 20 ]: [ 1 , [ 1 , 10 ]] In [ 21 ]: id ( l1 ) Out [ 21 ]: 2159380834888 In [ 22 ]: id ( l2 ) Out [ 22 ]: 2159379541128 You can use the following for copying - Shallow copy - copy.copy() - Deep copy - copy.deepcopy() In order for a class to define its own copy implementation, it can define special methods __copy__() and __deepcopy__() Tip - Never have mutable types as defualt parameters of your arguments.","title":"Copy"},{"location":"Python/List/#starred-expression","text":"first , second , * rest = [ 1 , 2 , 3 , 4 , 5 ]","title":"Starred Expression"},{"location":"Python/MRO/","text":"MRO Class.__mro__ will give you the method resolution order of that class. This is useful in figuring out the multiple inheritance MRO.","title":"MRO"},{"location":"Python/MRO/#mro","text":"Class.__mro__ will give you the method resolution order of that class. This is useful in figuring out the multiple inheritance MRO.","title":"MRO"},{"location":"Python/Sets/","text":"Sets They are implemented very similiarly to dictionaries. The key is the element itself (Behind the scenes they are implemented exactly like dictionaries but with dummy values). Sets allow for very fast addition, deletion and checking for existance. Sets are mutable. Set A set is a collection of unique objects. A set is mutable. Creating a set >>> a = { 'a' , 'b' } >>> type ( a ) < class ' set '> Intersection a & b Union a | b Difference a - b >>> foods = { \"chicken\" , \"bread\" , \"tomatao\" , \"onions\" } >>> veges = { \"tomatao\" , \"onions\" } >>> >>> foods - veges { 'bread' , 'chicken' } >>> >>> foods & veges { 'tomatao' , 'onions' } You cannot create an empty set like this {} , empty set needs to be created with a constructor like s = set() . Sets are also implemented with hash tables in the background, the value is hashed and stored (so we have unique values). Similar to dicts they are fast in membership testing but not so good on memory. Frozen Set A frozenset is immutable and thus hashable. You can create a frozenset as shown below. >>> x = frozenset ({ 1 , 2 , 3 }) >>> type ( x ) < class ' frozenset '> >>> x . __hash__ () - 272375401224217160","title":"Sets"},{"location":"Python/Sets/#sets","text":"They are implemented very similiarly to dictionaries. The key is the element itself (Behind the scenes they are implemented exactly like dictionaries but with dummy values). Sets allow for very fast addition, deletion and checking for existance. Sets are mutable.","title":"Sets"},{"location":"Python/Sets/#set","text":"A set is a collection of unique objects. A set is mutable. Creating a set >>> a = { 'a' , 'b' } >>> type ( a ) < class ' set '> Intersection a & b Union a | b Difference a - b >>> foods = { \"chicken\" , \"bread\" , \"tomatao\" , \"onions\" } >>> veges = { \"tomatao\" , \"onions\" } >>> >>> foods - veges { 'bread' , 'chicken' } >>> >>> foods & veges { 'tomatao' , 'onions' } You cannot create an empty set like this {} , empty set needs to be created with a constructor like s = set() . Sets are also implemented with hash tables in the background, the value is hashed and stored (so we have unique values). Similar to dicts they are fast in membership testing but not so good on memory.","title":"Set"},{"location":"Python/Sets/#frozen-set","text":"A frozenset is immutable and thus hashable. You can create a frozenset as shown below. >>> x = frozenset ({ 1 , 2 , 3 }) >>> type ( x ) < class ' frozenset '> >>> x . __hash__ () - 272375401224217160","title":"Frozen Set"},{"location":"Python/Slots/","text":"Slots __slots__ can save space if you are dealing with millions of instances with few attributes. (They use tuples instead of the standard __dict__ to store the instance attributes.) class Vector2d : __slots__ = ( '__x' , '__y' ) Slots will treat x and y as instance variables. There are some considerations which come with __slots__ like cannot add other instance variables unless you add __dict__ to slots and cannot use weakref unless you add __weakref__ to slots.","title":"Slots"},{"location":"Python/Slots/#slots","text":"__slots__ can save space if you are dealing with millions of instances with few attributes. (They use tuples instead of the standard __dict__ to store the instance attributes.) class Vector2d : __slots__ = ( '__x' , '__y' ) Slots will treat x and y as instance variables. There are some considerations which come with __slots__ like cannot add other instance variables unless you add __dict__ to slots and cannot use weakref unless you add __weakref__ to slots.","title":"Slots"},{"location":"Python/Subclassing%20built-ins/","text":"Subclassing built-ins Its not a good idea to subclass the builtins like `dict` and `list` because they ignore the user defined functions, instead you should subclass the collections module using UserDict, UserList, and UserString, which are designed to be easily extended. The below examples subclasses UserDict and raises error if the dict has any non unique value for any key. from collections import UserDict class DistinctError ( ValueError ): \"\"\"Raised when value is not distinct\"\"\" pass class distinctdict ( UserDict ): def __setitem__ ( self , key , value ): if value in self . values (): raise DistinctError ( \"Duplciate Value\" ) super () . __setitem__ ( key , value ) dd = distinctdict () dd [ 'k1' ] = 10 dd [ 'k2' ] = 20 for k , v in dd . items (): print ( f ' { k } --> { v } ' ) dd [ 'k11' ] = 10 # __main__.DistinctError: Duplicate Value The collections module contains extendend lists and other built in types which may suite your implementation needs without you having to create you own container type by subclassing the built in types.","title":"Subclassing built-ins"},{"location":"Python/Subclassing%20built-ins/#subclassing-built-ins","text":"Its not a good idea to subclass the builtins like `dict` and `list` because they ignore the user defined functions, instead you should subclass the collections module using UserDict, UserList, and UserString, which are designed to be easily extended. The below examples subclasses UserDict and raises error if the dict has any non unique value for any key. from collections import UserDict class DistinctError ( ValueError ): \"\"\"Raised when value is not distinct\"\"\" pass class distinctdict ( UserDict ): def __setitem__ ( self , key , value ): if value in self . values (): raise DistinctError ( \"Duplciate Value\" ) super () . __setitem__ ( key , value ) dd = distinctdict () dd [ 'k1' ] = 10 dd [ 'k2' ] = 20 for k , v in dd . items (): print ( f ' { k } --> { v } ' ) dd [ 'k11' ] = 10 # __main__.DistinctError: Duplicate Value The collections module contains extendend lists and other built in types which may suite your implementation needs without you having to create you own container type by subclassing the built in types.","title":"Subclassing built-ins"},{"location":"Python/Tuples/","text":"Tuples Tuples are immutable and thus hashable. Tuple unpacking >>> student1 = ( \"Sam\" , 14 , 8 ) >>> name , age , grade = student1 # tuple unpacking >>> name 'Sam' >>> age 14 >>> grade 8 Tuple unpacking can also be used to return/accept multiple values from a function. def return2 (): return 'a' , 'b' >>> return2 () ( 'a' , 'b' ) >>> val1 , val2 = return2 () >>> val1 'a' >>> val2 'b' Use * for extra values. The * does not need to be the last parameter. ``` {.python} a, b, *rest = range(1, 5) a, b, rest (1, 2, [3, 4])","title":"Tuples"},{"location":"Python/Tuples/#tuples","text":"Tuples are immutable and thus hashable.","title":"Tuples"},{"location":"Python/Tuples/#tuple-unpacking","text":">>> student1 = ( \"Sam\" , 14 , 8 ) >>> name , age , grade = student1 # tuple unpacking >>> name 'Sam' >>> age 14 >>> grade 8 Tuple unpacking can also be used to return/accept multiple values from a function. def return2 (): return 'a' , 'b' >>> return2 () ( 'a' , 'b' ) >>> val1 , val2 = return2 () >>> val1 'a' >>> val2 'b' Use * for extra values. The * does not need to be the last parameter. ``` {.python} a, b, *rest = range(1, 5) a, b, rest (1, 2, [3, 4])","title":"Tuple unpacking"},{"location":"Python/Weakref/","text":"Weakref A weak reference to an object is not enough to keep the object alive: when the only remaining references to a referent are weak references, garbage collection is free to destroy the referent and reuse its memory for something else. However, until the object is actually destroyed the weak reference may return the object even if there are no strong references to it. A primary use for weak references is to implement caches or mappings holding large objects, where it's desired that a large object not be kept alive solely because it appears in a cache or mapping. WeakKeyDictionary, WeakValueDictionary, WeakSet, and finalize (which use weak references internally) instead of creating and handling your own weak ref.ref instances by hand.","title":"Weakref"},{"location":"Python/Weakref/#weakref","text":"A weak reference to an object is not enough to keep the object alive: when the only remaining references to a referent are weak references, garbage collection is free to destroy the referent and reuse its memory for something else. However, until the object is actually destroyed the weak reference may return the object even if there are no strong references to it. A primary use for weak references is to implement caches or mappings holding large objects, where it's desired that a large object not be kept alive solely because it appears in a cache or mapping. WeakKeyDictionary, WeakValueDictionary, WeakSet, and finalize (which use weak references internally) instead of creating and handling your own weak ref.ref instances by hand.","title":"Weakref"},{"location":"Python/django/","text":"Forms Formsets Create a simple model with few fields. # models.py class Employee ( models . Model ): name = models . CharField ( max_length = 40 ) is_manager = models . BooleanField ( default = False ) email = models . CharField ( max_length = 100 ) def __str__ ( self ): return self . name After creating a model, lets create a model form for that model. Also create the formset for that model and form as shown below. # forms.py from django import forms from .models import * from django.forms import modelformset_factory class EmployeeForm ( forms . ModelForm ): email = forms . EmailField ( disabled = True ) # disable field class Meta : model = Employee fields = [ 'email' , 'name' , 'is_manager' ] EmployeeFormSet = modelformset_factory ( Employee , form = EmployeeForm , max_num = 0 ) Use the formset created in the view. # views.py from django.shortcuts import render from .forms import * from django.views import View from .models import * def index ( request ): context = {} if request . method == 'GET' : formset = EmployeeFormSet () return render ( request , 'tryformsets/index.html' , { 'formset' : formset }) if request . method == 'POST' : # formset = EmployeeFormSet(request.POST or None, request.FILES or None) formset = EmployeeFormSet ( request . POST ) if formset . is_valid (): formset . save () formset = EmployeeFormSet () return render ( request , 'tryformsets/index.html' , { 'formset' : formset }) Then create the template and use the formset. Make sure to include the {{formset.management_form}} else it gives error. For more information check the link <form method= \"POST\" action= \".\" > {% csrf_token %} {{ formset.management_form }} <table class= \"table\" > <thead> <tr> <th> Name </th> <th> Item Name </th> <th> Item Price </th> </tr> </thead> {% for form in formset %} <tbody> {{ form.id }} <tr> <td> {{ form.name }} </td> <td> {{ form.email }} </td> <td> {{ form.is_manager }} </td> </tr> {% endfor %} </tbody> </table> <button type= \"submit\" > Submit </button> </form> Working with DB Bulk Update # Coverage is a model which we need to update in bulk. # Here we are trying to update the rows with pk = 33, 34 and 35 with different values. >>> objs = [] >>> objs . append ( Coverage . objects . get ( pk = 33 )) >>> >>> objs . append ( Coverage . objects . get ( pk = 34 )) >>> objs . append ( Coverage . objects . get ( pk = 35 )) >>> >>> objs [ 1 ] . coverage_needed = True >>> >>> objs [ 1 ] . coverage_needed True >>> >>> objs [ 2 ] . coverage_needed False >>> >>> objs [ 0 ] . unavailable = True >>> >>> objs [ 2 ] . supply_called = True >>> >>> Coverage . objects . bulk_update ( objs , [ 'coverage_needed' , 'supply_called' , 'unavailable' ]) Links Forms DataTable Editable Django Forms - HTML Django Forms - Rendering each form element manually for better styling.","title":"Django"},{"location":"Python/django/#forms","text":"","title":"Forms"},{"location":"Python/django/#formsets","text":"Create a simple model with few fields. # models.py class Employee ( models . Model ): name = models . CharField ( max_length = 40 ) is_manager = models . BooleanField ( default = False ) email = models . CharField ( max_length = 100 ) def __str__ ( self ): return self . name After creating a model, lets create a model form for that model. Also create the formset for that model and form as shown below. # forms.py from django import forms from .models import * from django.forms import modelformset_factory class EmployeeForm ( forms . ModelForm ): email = forms . EmailField ( disabled = True ) # disable field class Meta : model = Employee fields = [ 'email' , 'name' , 'is_manager' ] EmployeeFormSet = modelformset_factory ( Employee , form = EmployeeForm , max_num = 0 ) Use the formset created in the view. # views.py from django.shortcuts import render from .forms import * from django.views import View from .models import * def index ( request ): context = {} if request . method == 'GET' : formset = EmployeeFormSet () return render ( request , 'tryformsets/index.html' , { 'formset' : formset }) if request . method == 'POST' : # formset = EmployeeFormSet(request.POST or None, request.FILES or None) formset = EmployeeFormSet ( request . POST ) if formset . is_valid (): formset . save () formset = EmployeeFormSet () return render ( request , 'tryformsets/index.html' , { 'formset' : formset }) Then create the template and use the formset. Make sure to include the {{formset.management_form}} else it gives error. For more information check the link <form method= \"POST\" action= \".\" > {% csrf_token %} {{ formset.management_form }} <table class= \"table\" > <thead> <tr> <th> Name </th> <th> Item Name </th> <th> Item Price </th> </tr> </thead> {% for form in formset %} <tbody> {{ form.id }} <tr> <td> {{ form.name }} </td> <td> {{ form.email }} </td> <td> {{ form.is_manager }} </td> </tr> {% endfor %} </tbody> </table> <button type= \"submit\" > Submit </button> </form>","title":"Formsets"},{"location":"Python/django/#working-with-db","text":"","title":"Working with DB"},{"location":"Python/django/#bulk-update","text":"# Coverage is a model which we need to update in bulk. # Here we are trying to update the rows with pk = 33, 34 and 35 with different values. >>> objs = [] >>> objs . append ( Coverage . objects . get ( pk = 33 )) >>> >>> objs . append ( Coverage . objects . get ( pk = 34 )) >>> objs . append ( Coverage . objects . get ( pk = 35 )) >>> >>> objs [ 1 ] . coverage_needed = True >>> >>> objs [ 1 ] . coverage_needed True >>> >>> objs [ 2 ] . coverage_needed False >>> >>> objs [ 0 ] . unavailable = True >>> >>> objs [ 2 ] . supply_called = True >>> >>> Coverage . objects . bulk_update ( objs , [ 'coverage_needed' , 'supply_called' , 'unavailable' ])","title":"Bulk Update"},{"location":"Python/django/#links","text":"","title":"Links"},{"location":"Python/django/#forms_1","text":"DataTable Editable Django Forms - HTML Django Forms - Rendering each form element manually for better styling.","title":"Forms"},{"location":"Python/foundations/","text":"Installation Python at C:\\ Intro to Python Shell Intro to Python IDLE -> Writing Python Files Pycharm VS Code Computer Science Types of computer languages Interpretted Compiled Python Language Basics Numbers # Int a = 10 print ( type ( a )) # Float b = 10.1 print ( type ( b )) # Math operations 5 % 2 # Modulus 5 ** 2 # Power Strings c = \"Hello\" # You can use single or double quotes c = 'Samantha \\' s' # Escaping quotes # Multiline string c = \"\"\" Hello There New line \"\"\" # Slicing Strings str = \"Hello\" str [ 0 : 2 ] # He # Calculating the length of a string len ( str ) #5 Indeces of Strings +---+---+---+---+---+---+ | P | y | t | h | o | n | +---+---+---+---+---+---+ 0 1 2 3 4 5 6 ( L to R ) - 6 - 5 - 4 - 3 - 2 - 1 ( R to L ) String Operations There are lots of string methods which do operations on the string variables. Some of the common methods are described below. You can find more information on the string methods here a = 'hello' a . capitalize () # 'Hello' --> capitalize the first character. a = 'hello' a . endswith ( 'llo' ) # True a = 'Today is Monday' a . split ( ' ' ) # ['Today', 'is', 'Monday'] --> Splits the string on delimeter. Conditionals - If - Nested If - Pass statement Range Object range ( start , stop , increment ) For Loop Loop on range Loop on list Nested loop Break and Continue else clause on loop enumerate zip() While Loop Data Structures Lists # Creating a list squares = [ 1 , 4 , 9 , 16 , 25 ] # indexing returns the item squares [ 0 ] # 1 # slicing returns a new list squares [ - 3 :] # [9, 16, 25] # you can concatenate lists a = [ 1 , 2 , 3 ] b = [ 4 , 5 , 6 ] c = a + b print ( c ) # [1, 2, 3, 4, 5, 6] # append to the end of the list a . append ( 4 ) print ( a ) # [1, 2, 3, 4] # Replacing some letters letters = [ 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' ] letters [ 2 : 5 ] = [ 'C' , 'D' , 'E' ] print ( letters ) # ['a', 'b', 'C', 'D', 'E', 'f', 'g'] # Checking the length of the list a = [ 1 , 2 , 4 ] len ( a ) # 3 # Deleting based on index del a [ 0 ] print ( a ) # [2, 4] # Other list methods # - pop # - count List Comprehension # Creating a list of squares [ x ** 2 for x in range ( 10 )] # Creating a list of tuples [( x , y ) for x in [ 1 , 2 , 3 ] for y in [ 3 , 1 , 4 ] if x != y ] # Nested functions in list [ str ( round ( pi , i )) for i in range ( 1 , 6 )] Dictionaries create dict add to dict delete from dict dictionary comprehension Tuples Tuples are immutable. t = 12345 , 54321 , 'hello!' t = ( 12345 , 54321 , 'hello!' ) Sets A set is an unordered collection with no duplicate elements. Set objects also support mathematical operations like union, intersection, difference, and symmetric difference. basket = { 'apple' , 'orange' , 'apple' , 'pear' , 'orange' , 'banana' }","title":"Foundations"},{"location":"Python/foundations/#installation","text":"Python at C:\\ Intro to Python Shell Intro to Python IDLE -> Writing Python Files Pycharm VS Code","title":"Installation"},{"location":"Python/foundations/#computer-science","text":"","title":"Computer Science"},{"location":"Python/foundations/#types-of-computer-languages","text":"Interpretted Compiled","title":"Types of computer languages"},{"location":"Python/foundations/#python-language-basics","text":"Numbers # Int a = 10 print ( type ( a )) # Float b = 10.1 print ( type ( b )) # Math operations 5 % 2 # Modulus 5 ** 2 # Power Strings c = \"Hello\" # You can use single or double quotes c = 'Samantha \\' s' # Escaping quotes # Multiline string c = \"\"\" Hello There New line \"\"\" # Slicing Strings str = \"Hello\" str [ 0 : 2 ] # He # Calculating the length of a string len ( str ) #5 Indeces of Strings +---+---+---+---+---+---+ | P | y | t | h | o | n | +---+---+---+---+---+---+ 0 1 2 3 4 5 6 ( L to R ) - 6 - 5 - 4 - 3 - 2 - 1 ( R to L ) String Operations There are lots of string methods which do operations on the string variables. Some of the common methods are described below. You can find more information on the string methods here a = 'hello' a . capitalize () # 'Hello' --> capitalize the first character. a = 'hello' a . endswith ( 'llo' ) # True a = 'Today is Monday' a . split ( ' ' ) # ['Today', 'is', 'Monday'] --> Splits the string on delimeter. Conditionals - If - Nested If - Pass statement Range Object range ( start , stop , increment ) For Loop Loop on range Loop on list Nested loop Break and Continue else clause on loop enumerate zip() While Loop","title":"Python Language Basics"},{"location":"Python/foundations/#data-structures","text":"Lists # Creating a list squares = [ 1 , 4 , 9 , 16 , 25 ] # indexing returns the item squares [ 0 ] # 1 # slicing returns a new list squares [ - 3 :] # [9, 16, 25] # you can concatenate lists a = [ 1 , 2 , 3 ] b = [ 4 , 5 , 6 ] c = a + b print ( c ) # [1, 2, 3, 4, 5, 6] # append to the end of the list a . append ( 4 ) print ( a ) # [1, 2, 3, 4] # Replacing some letters letters = [ 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' ] letters [ 2 : 5 ] = [ 'C' , 'D' , 'E' ] print ( letters ) # ['a', 'b', 'C', 'D', 'E', 'f', 'g'] # Checking the length of the list a = [ 1 , 2 , 4 ] len ( a ) # 3 # Deleting based on index del a [ 0 ] print ( a ) # [2, 4] # Other list methods # - pop # - count List Comprehension # Creating a list of squares [ x ** 2 for x in range ( 10 )] # Creating a list of tuples [( x , y ) for x in [ 1 , 2 , 3 ] for y in [ 3 , 1 , 4 ] if x != y ] # Nested functions in list [ str ( round ( pi , i )) for i in range ( 1 , 6 )] Dictionaries create dict add to dict delete from dict dictionary comprehension Tuples Tuples are immutable. t = 12345 , 54321 , 'hello!' t = ( 12345 , 54321 , 'hello!' ) Sets A set is an unordered collection with no duplicate elements. Set objects also support mathematical operations like union, intersection, difference, and symmetric difference. basket = { 'apple' , 'orange' , 'apple' , 'pear' , 'orange' , 'banana' }","title":"Data Structures"},{"location":"Python/metaprogramming/","text":"Metaprogramming This is based on David Beazley tutorial on Metaprogramming on Youtube. I have tried to go through his tutorial and tried to understand and pick up things which he has used in his code. Prerequisites In this section I have documented the things which I found out when I was searching for things which were mentioned in the talk directly or indirectly. Reduce This is provided by functools reduce ( lambda x , y : x * y , [ 1 , 2 , 3 , 4 , 5 ]) #120 reduce ( lambda x , y : max ( x , y ), [ 1 , 2 , 3 , 4 , 5 ]) #5 Property Decorator The property decorator creates sort of an alias for that function but access as a property and not a function. class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname @property def full_name ( self ): return f \" { self . fname } { self . lname } \" sam_nelson = Person ( \"Sam\" , \"Nelson\" ) print ( sam_nelson . full_name ) # can access as a property print ( sam_nelson . __dict__ ) # as you see full_name is not there in the dict Sam Nelson { 'fname' : 'Sam' , 'lname' : 'Nelson' } Instead of property decorator you can also use @cached_property decorator which caches the result and does not execute the function again and again. lru~cache~ This is provided by functools and cache the results of function, very useful, check it out online. If the argument was provided before to that function the result will be returned by cache else will be executed. Makes sense where the calculation is complex but the return value is very less in size e.g. calculating if prime or not (difficult but return is just true or false), fibonacchi (difficult to calc 100th fib value but result is just 1 int value.) partial This is provided by functools. Take an example below where we have some top level function such as add which does some stuff. Now we can create a closure over it and create another function which always adds 1 i.e. add_one function. def add ( a , b ): return a + b def add_one ( a ): return add ( a , 1 ) print ( add_one ( 10 )) 11 partial from functools simplifies this behaviour by always passing in 1 for the first argument and the rest user has to provide. from functools import partial def add ( a , b ): print ( \"Default = \" , a , \"user supplied = \" , b ) return a + b add_one = partial ( add , 1 ) print ( \"Result\" , add_one ( 4 )) Default = 1 user supplied = 4 Result 5 You can also provide value of b by default instead of a as in the above code, you need to provide it as a keyword argument as shown below. from functools import partial def add ( a , b ): print ( a , b ) # Now b is always 1 and a is supplied by the caller. return a + b add_one = partial ( add , b = 1 ) # Notice the change add_one ( 4 ) 4 1 Class Variable vs Instance Variable class Spam : a = 1 # class variable def __init__ ( self , b ): self . b = b # instance variable Spam . a Out [ 3 ]: 1 Spam ( 10 ) . b Out [ 4 ]: 10 The Talk Decorators A simple decorator Decorators are used to wrap a function and execute some new fucntionality which is defined in the decorator as shown in the example below. The key idea is that you can change the implementation of the decorator independently of the code you are using it on. # A simple decorator which prints the name of the func from functools import wraps def mydeco ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): print ( func . __qualname__ ) # extra stuff : printing func name return func ( * args , ** kwargs ) return wrapper @mydeco def add ( x , y ): return x + y print ( add ( 20 , 20 )) add 40 The decorator can be create to take in optinal arguments as well, so that differnt functions can use the same decorator in a different way. from functools import wraps def mydeco ( prefix = '' ): def decorate ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): print ( prefix + func . __qualname__ ) # extra stuff : printing func name return func ( * args , ** kwargs ) return wrapper return decorate @mydeco ( prefix = \"**\" ) def add ( x , y ): return x + y print ( add ( 20 , 20 )) ** add 40 In the above case the decorator has to be always called via () e.g. mydeco() the function style and not just @mydeco . There is a hack which David mentions (which I did not understand fully) from functools import wraps , partial def mydeco ( func = None , * , prefix = '' ): if func is None : return partial ( mydeco , prefix = prefix ) @wraps ( func ) def wrapper ( * args , ** kwargs ): print ( prefix + func . __qualname__ ) # extra stuff : printing func name return func ( * args , ** kwargs ) return wrapper @mydeco def add ( x , y ): return x + y @mydeco ( prefix = \"**\" ) def mult ( x , y ): return x * y print ( add ( 20 , 20 )) print ( mult ( 20 , 20 )) add 40 ** mult 400 Class Decorator Moving on a little bit more in depth, lets say you have a class and there are many functions in a class. How are you supposed to decorate all methods of that class. Putting decorator on each function sort of repetitive, thus we have a class decorator to simplify that. When you create a class you can access the attribues of a module, class or instance by using the vars function. class Person : def __init__ ( self , fn , ln ): self . fn = fn self . ln = ln def full_name ( self ): return f \" { self . fn } { self . ln } \" >>> vars ( Person ) mappingproxy ({ '__module__' : '__main__' , '__init__' : < function Person . __init__ at 0x041FA100 > , 'full_name' : < function Person . full_name at 0x041FA070 > , '__dict__' : < attribute '__dict__' of 'Person' objects > , '__weakref__' : < attribute '__weakref__' of 'Person' objects > , '__doc__' : None }) You can then further check if the attribute is callable or not using the callable keyword as shown below. >>> callable ( vars ( Person )[ 'full_name' ]) True >>> callable ( vars ( Person )[ '__doc__' ]) False Before jumping in to the actual decorator python has another function setattr which lets you set the attribute on an object. e.g. set a new attribute (which could be a function, variable, doc etc...) on a class. (Remember vars gives you the attributes and setattr lets you set the attributes.) Syntax is : setattr(object, name, value) Once you have identified the callable you can then wrap it with a decorator. from functools import partial , wraps def mydeco ( func = None , * , prefix = '' ): if func is None : return partial ( mydeco , prefix = prefix ) @wraps ( func ) def wrapper ( * args , ** kwargs ): print ( prefix + func . __qualname__ ) # extra stuff : printing func name return func ( * args , ** kwargs ) return wrapper # creating class deco which sets mydeco to each callable of that class def classdeco ( cls ): for key , val in vars ( cls ) . items (): if callable ( val ): setattr ( cls , key , mydeco ( val )) # instead of val wrap it into mydeco and set the callable return cls @classdeco # setting decorator on class class Person : def __init__ ( self , fn , ln ): self . fn = fn self . ln = ln def full_name ( self ): return f \" { self . fn } { self . ln } \" >>> sam = Person ( \"Sam\" , \"Nelson\" ) Person . __init__ # comes from the decorator >>> sam . full_name () Person . full_name # comes from the decorator 'Sam Nelson' The above method will only set the decorator on the instance methods as the class methods or static methods are not identified as callable as shown below. class Person : def __init__ ( self , fn , ln ): self . fn = fn self . ln = ln def full_name ( self ): return f \" { self . fn } { self . ln } \" @classmethod def class_details ( cls ): pass >>> callable ( vars ( Person )[ 'class_details' ]) False Lets create a simple example of class decorator which modifies the __getattribute__ (which does the attribute lookup). def snooper ( cls ): orggetattribute = cls . __getattribute__ def __getattribute__ ( self , name ): print ( 'Get :' , name ) return orggetattribute ( self , name ) cls . __getattribute__ = __getattribute__ return cls @snooper class Person : def __init__ ( self , fn , ln ): self . fn = fn self . ln = ln def full_name ( self ): return f \" { self . fn } { self . ln } \" @classmethod def class_details ( cls ): pass sam = Person ( \"S\" , \"N\" ) # More output than what's displayed below. >>> sam . fn Get : fn 'S' Metaclasses Before we dive into metaclasses, lets understand how the classes are made in Python. Consider the code below, type of a is int and type of int is type thus type must be a class whoes instances are classes. a = 10 type ( a ) < class ' int '> type ( int ) < class ' type '> Lets further verify this by using a custom class class A : pass >>> type ( A ) < class ' type '> Also we can use the type function to create a class. There are few steps involved. def x ( self ): # creating a simple function print ( 10 ) # creating a class dict which attaches a name to that function. clsdict = { 'x' : x } # Creating the class, now Spam is a class Spam = type ( 'Spam' , ( object ,), clsdict ) >>> Spam < class ' __main__ . Spam '> The idea behind metaclass is to use something other than type , so whatever you put in class X(metaclass=...) is going to be used to create the class. e.g. # Instead of this (which is the default behaviour, even if you do not put in the type) class X ( type ): pass # We want class Y ( metaclass = our_own_implementation_of_type ): pass Lets see the structure to create a simple metaclass. - Inherit from type - Redefine __new__ or __init__ In below example mytype is a simple metaclass. (which does not do anything though) class mytype ( type ): def __new__ ( cls , name , bases , clsdict ): clsobj = super () . __new__ ( cls , name , bases , clsdict ) return clsobj class Spam ( metaclass = mytype ): pass Lets see a simple example where a metaclass will prevent multiple inheritance in python. class mytype ( type ): def __new__ ( cls , name , bases , clsdict ): if len ( bases ) > 1 : raise TypeError ( \"Multiple Inheritance is not allowed\" ) return super () . __new__ ( cls , name , bases , clsdict ) class Base ( metaclass = mytype ): pass class A ( Base ): pass class B ( Base ): pass class C ( A , B ): pass When we run this we get error as expected. Traceback ( most recent call last ) : File \"C:/Users/RS/AppData/Roaming/JetBrains/PyCharmCE2020.1/scratches/scratch.py\" , line 17 , in <module> class C ( A, B ) : File \"C:/Users/RS/AppData/Roaming/JetBrains/PyCharmCE2020.1/scratches/scratch.py\" , line 4 , in __new__ raise TypeError ( \"Multiple Inheritance is not allowed\" ) TypeError: Multiple Inheritance is not allowed So the idea behind metaclass is that set it on the base class and it will propogate throughout wherever the inheritance hierarchy goes. (could be in your whole framework) Key points - Simple decorator - decorates or wraps 1 function or multiple (if applied) - Class decorator - decorates or wraps all methods (instance) of that class - Metaclass - decorates or wraps throughout the whole class hierarchy So metaclasses get information about class definitions at the time of definition, so they can - inspect this data - modify this data However they have bad reputation in python community. Difference b/w class decorater and metaclass is that class decorator is called after the class is created/formed and metaclass is called before. (Subtle difference thus use case may vary depending on scenario) <<<<<<<< ---- Min 39.20 ---- >>>>>>>>","title":"Metaprogramming"},{"location":"Python/metaprogramming/#metaprogramming","text":"This is based on David Beazley tutorial on Metaprogramming on Youtube. I have tried to go through his tutorial and tried to understand and pick up things which he has used in his code.","title":"Metaprogramming"},{"location":"Python/metaprogramming/#prerequisites","text":"In this section I have documented the things which I found out when I was searching for things which were mentioned in the talk directly or indirectly. Reduce This is provided by functools reduce ( lambda x , y : x * y , [ 1 , 2 , 3 , 4 , 5 ]) #120 reduce ( lambda x , y : max ( x , y ), [ 1 , 2 , 3 , 4 , 5 ]) #5 Property Decorator The property decorator creates sort of an alias for that function but access as a property and not a function. class Person : def __init__ ( self , fname , lname ): self . fname = fname self . lname = lname @property def full_name ( self ): return f \" { self . fname } { self . lname } \" sam_nelson = Person ( \"Sam\" , \"Nelson\" ) print ( sam_nelson . full_name ) # can access as a property print ( sam_nelson . __dict__ ) # as you see full_name is not there in the dict Sam Nelson { 'fname' : 'Sam' , 'lname' : 'Nelson' } Instead of property decorator you can also use @cached_property decorator which caches the result and does not execute the function again and again. lru~cache~ This is provided by functools and cache the results of function, very useful, check it out online. If the argument was provided before to that function the result will be returned by cache else will be executed. Makes sense where the calculation is complex but the return value is very less in size e.g. calculating if prime or not (difficult but return is just true or false), fibonacchi (difficult to calc 100th fib value but result is just 1 int value.) partial This is provided by functools. Take an example below where we have some top level function such as add which does some stuff. Now we can create a closure over it and create another function which always adds 1 i.e. add_one function. def add ( a , b ): return a + b def add_one ( a ): return add ( a , 1 ) print ( add_one ( 10 )) 11 partial from functools simplifies this behaviour by always passing in 1 for the first argument and the rest user has to provide. from functools import partial def add ( a , b ): print ( \"Default = \" , a , \"user supplied = \" , b ) return a + b add_one = partial ( add , 1 ) print ( \"Result\" , add_one ( 4 )) Default = 1 user supplied = 4 Result 5 You can also provide value of b by default instead of a as in the above code, you need to provide it as a keyword argument as shown below. from functools import partial def add ( a , b ): print ( a , b ) # Now b is always 1 and a is supplied by the caller. return a + b add_one = partial ( add , b = 1 ) # Notice the change add_one ( 4 ) 4 1 Class Variable vs Instance Variable class Spam : a = 1 # class variable def __init__ ( self , b ): self . b = b # instance variable Spam . a Out [ 3 ]: 1 Spam ( 10 ) . b Out [ 4 ]: 10","title":"Prerequisites"},{"location":"Python/metaprogramming/#the-talk","text":"","title":"The Talk"},{"location":"Python/metaprogramming/#decorators","text":"A simple decorator Decorators are used to wrap a function and execute some new fucntionality which is defined in the decorator as shown in the example below. The key idea is that you can change the implementation of the decorator independently of the code you are using it on. # A simple decorator which prints the name of the func from functools import wraps def mydeco ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): print ( func . __qualname__ ) # extra stuff : printing func name return func ( * args , ** kwargs ) return wrapper @mydeco def add ( x , y ): return x + y print ( add ( 20 , 20 )) add 40 The decorator can be create to take in optinal arguments as well, so that differnt functions can use the same decorator in a different way. from functools import wraps def mydeco ( prefix = '' ): def decorate ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): print ( prefix + func . __qualname__ ) # extra stuff : printing func name return func ( * args , ** kwargs ) return wrapper return decorate @mydeco ( prefix = \"**\" ) def add ( x , y ): return x + y print ( add ( 20 , 20 )) ** add 40 In the above case the decorator has to be always called via () e.g. mydeco() the function style and not just @mydeco . There is a hack which David mentions (which I did not understand fully) from functools import wraps , partial def mydeco ( func = None , * , prefix = '' ): if func is None : return partial ( mydeco , prefix = prefix ) @wraps ( func ) def wrapper ( * args , ** kwargs ): print ( prefix + func . __qualname__ ) # extra stuff : printing func name return func ( * args , ** kwargs ) return wrapper @mydeco def add ( x , y ): return x + y @mydeco ( prefix = \"**\" ) def mult ( x , y ): return x * y print ( add ( 20 , 20 )) print ( mult ( 20 , 20 )) add 40 ** mult 400 Class Decorator Moving on a little bit more in depth, lets say you have a class and there are many functions in a class. How are you supposed to decorate all methods of that class. Putting decorator on each function sort of repetitive, thus we have a class decorator to simplify that. When you create a class you can access the attribues of a module, class or instance by using the vars function. class Person : def __init__ ( self , fn , ln ): self . fn = fn self . ln = ln def full_name ( self ): return f \" { self . fn } { self . ln } \" >>> vars ( Person ) mappingproxy ({ '__module__' : '__main__' , '__init__' : < function Person . __init__ at 0x041FA100 > , 'full_name' : < function Person . full_name at 0x041FA070 > , '__dict__' : < attribute '__dict__' of 'Person' objects > , '__weakref__' : < attribute '__weakref__' of 'Person' objects > , '__doc__' : None }) You can then further check if the attribute is callable or not using the callable keyword as shown below. >>> callable ( vars ( Person )[ 'full_name' ]) True >>> callable ( vars ( Person )[ '__doc__' ]) False Before jumping in to the actual decorator python has another function setattr which lets you set the attribute on an object. e.g. set a new attribute (which could be a function, variable, doc etc...) on a class. (Remember vars gives you the attributes and setattr lets you set the attributes.) Syntax is : setattr(object, name, value) Once you have identified the callable you can then wrap it with a decorator. from functools import partial , wraps def mydeco ( func = None , * , prefix = '' ): if func is None : return partial ( mydeco , prefix = prefix ) @wraps ( func ) def wrapper ( * args , ** kwargs ): print ( prefix + func . __qualname__ ) # extra stuff : printing func name return func ( * args , ** kwargs ) return wrapper # creating class deco which sets mydeco to each callable of that class def classdeco ( cls ): for key , val in vars ( cls ) . items (): if callable ( val ): setattr ( cls , key , mydeco ( val )) # instead of val wrap it into mydeco and set the callable return cls @classdeco # setting decorator on class class Person : def __init__ ( self , fn , ln ): self . fn = fn self . ln = ln def full_name ( self ): return f \" { self . fn } { self . ln } \" >>> sam = Person ( \"Sam\" , \"Nelson\" ) Person . __init__ # comes from the decorator >>> sam . full_name () Person . full_name # comes from the decorator 'Sam Nelson' The above method will only set the decorator on the instance methods as the class methods or static methods are not identified as callable as shown below. class Person : def __init__ ( self , fn , ln ): self . fn = fn self . ln = ln def full_name ( self ): return f \" { self . fn } { self . ln } \" @classmethod def class_details ( cls ): pass >>> callable ( vars ( Person )[ 'class_details' ]) False Lets create a simple example of class decorator which modifies the __getattribute__ (which does the attribute lookup). def snooper ( cls ): orggetattribute = cls . __getattribute__ def __getattribute__ ( self , name ): print ( 'Get :' , name ) return orggetattribute ( self , name ) cls . __getattribute__ = __getattribute__ return cls @snooper class Person : def __init__ ( self , fn , ln ): self . fn = fn self . ln = ln def full_name ( self ): return f \" { self . fn } { self . ln } \" @classmethod def class_details ( cls ): pass sam = Person ( \"S\" , \"N\" ) # More output than what's displayed below. >>> sam . fn Get : fn 'S'","title":"Decorators"},{"location":"Python/metaprogramming/#metaclasses","text":"Before we dive into metaclasses, lets understand how the classes are made in Python. Consider the code below, type of a is int and type of int is type thus type must be a class whoes instances are classes. a = 10 type ( a ) < class ' int '> type ( int ) < class ' type '> Lets further verify this by using a custom class class A : pass >>> type ( A ) < class ' type '> Also we can use the type function to create a class. There are few steps involved. def x ( self ): # creating a simple function print ( 10 ) # creating a class dict which attaches a name to that function. clsdict = { 'x' : x } # Creating the class, now Spam is a class Spam = type ( 'Spam' , ( object ,), clsdict ) >>> Spam < class ' __main__ . Spam '> The idea behind metaclass is to use something other than type , so whatever you put in class X(metaclass=...) is going to be used to create the class. e.g. # Instead of this (which is the default behaviour, even if you do not put in the type) class X ( type ): pass # We want class Y ( metaclass = our_own_implementation_of_type ): pass Lets see the structure to create a simple metaclass. - Inherit from type - Redefine __new__ or __init__ In below example mytype is a simple metaclass. (which does not do anything though) class mytype ( type ): def __new__ ( cls , name , bases , clsdict ): clsobj = super () . __new__ ( cls , name , bases , clsdict ) return clsobj class Spam ( metaclass = mytype ): pass Lets see a simple example where a metaclass will prevent multiple inheritance in python. class mytype ( type ): def __new__ ( cls , name , bases , clsdict ): if len ( bases ) > 1 : raise TypeError ( \"Multiple Inheritance is not allowed\" ) return super () . __new__ ( cls , name , bases , clsdict ) class Base ( metaclass = mytype ): pass class A ( Base ): pass class B ( Base ): pass class C ( A , B ): pass When we run this we get error as expected. Traceback ( most recent call last ) : File \"C:/Users/RS/AppData/Roaming/JetBrains/PyCharmCE2020.1/scratches/scratch.py\" , line 17 , in <module> class C ( A, B ) : File \"C:/Users/RS/AppData/Roaming/JetBrains/PyCharmCE2020.1/scratches/scratch.py\" , line 4 , in __new__ raise TypeError ( \"Multiple Inheritance is not allowed\" ) TypeError: Multiple Inheritance is not allowed So the idea behind metaclass is that set it on the base class and it will propogate throughout wherever the inheritance hierarchy goes. (could be in your whole framework) Key points - Simple decorator - decorates or wraps 1 function or multiple (if applied) - Class decorator - decorates or wraps all methods (instance) of that class - Metaclass - decorates or wraps throughout the whole class hierarchy So metaclasses get information about class definitions at the time of definition, so they can - inspect this data - modify this data However they have bad reputation in python community. Difference b/w class decorater and metaclass is that class decorator is called after the class is created/formed and metaclass is called before. (Subtle difference thus use case may vary depending on scenario) <<<<<<<< ---- Min 39.20 ---- >>>>>>>>","title":"Metaclasses"}]}